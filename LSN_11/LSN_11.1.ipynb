{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tommaso Peritore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import plotly.graph_objs as go\n",
    "from plotly.subplots import make_subplots\n",
    "from matplotlib.gridspec import GridSpec\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "#from tensorflow.keras import backend as K\n",
    "#from tensorflow.keras.utils import get_custom_objects\n",
    "\n",
    "import itertools\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In this notebook our task will be to perform machine learning regression on noisy data with a Neural Network (NN).\n",
    "\n",
    "We will explore how the ability to fit depends on the structure of the NN. The goal is also to build intuition about why prediction is difficult."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.1\n",
    "\n",
    "In order to make practice with NN, explore how does the previous linear regression depend on the number of epochs, $N_{\\mathrm{epochs}}$, the number of data points $N_{\\mathrm{train}}$ and on the noise $\\sigma$. Try to improve the previous result operating on these parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will try to improve the fitting of the following function:\n",
    "$$\n",
    "f(x)=2x+1\n",
    "$$\n",
    "We want to explore how the linear regression depends on the number of epochs, the number of data points and the noise $\\sigma$. Thus we are interested in seeing how these parameters affect the ability of the NN to fit the data.\n",
    "\n",
    "To explore the effect of all parameters we will define a set of values for each and then loop over all possible combinations of these values. Below I am showing the values for the $3$ parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sigma =  [0.1, 0.2, 0.5, 1]\n",
      "N_epochs =  [10, 15, 30, 50]\n",
      "N_train =  [500, 1000, 1500, 2000]\n"
     ]
    }
   ],
   "source": [
    "# Parameters to vary\n",
    "_sigma = [0.1, 0.2, 0.5, 1] # <<< noise\n",
    "_n_epochs = [10, 15, 30, 50] # <<< epochs\n",
    "_n_train = [500, 1000, 1500, 2000] # <<< train\n",
    "\n",
    "N_valid = 50 # <<< test\n",
    "\n",
    "# target parameters of f(x) = m*x + q\n",
    "m = 2 # slope\n",
    "q = 1 # intersect\n",
    "\n",
    "print('Sigma = ', _sigma)\n",
    "print('N_epochs = ', _n_epochs)\n",
    "print('N_train = ', _n_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameters to explore were chosen with these criteria in mind:\n",
    "- The values of sigma were chosen to span from very low noise to very high noise so as to explore the ability of the NN model to fit the data in the presence of progressively more noise.\n",
    "- The number of epochs was chosen to span from below to above the value given in the original code, to see if $30$ epochs was already enough to saturate the training or if more epochs would improve the fit, or if less epochs would be enough.\n",
    "- Finally, the number of training data was varied significantly to see if the model would perform better after having been trained on more data.\n",
    "\n",
    "Below I am going to define a function to prepare the data, define the model, compile it and finally train it. This function, `run_model` will be used to fill a `loss` array with the values of the loss function for each combination of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function for the NN model\n",
    "def run_model (sigma, n_epochs, train, valid):\n",
    "\t# generate training inputs\n",
    "\tnp.random.seed(0)\n",
    "\tx_train = np.random.uniform(-1, 1, train)\n",
    "\tx_valid = np.random.uniform(-1, 1, valid)\n",
    "\tx_valid.sort()\n",
    "\t# y_target = m * x_valid + q # ideal (target) linear function\n",
    "\n",
    "\t# actual measures from which we want to guess regression parameters\n",
    "\ty_train = np.random.normal(m * x_train + q, sigma) \n",
    "\ty_valid = np.random.normal(m * x_valid + q, sigma)\n",
    "\n",
    "\t# compose the NN model\n",
    "\tmodel = tf.keras.Sequential()\n",
    "\tmodel.add(Dense(1, input_shape = (1,)))\n",
    "\n",
    "\t# compile the model choosing optimizer, loss and metrics objects\n",
    "\tmodel.compile(optimizer = 'sgd', loss = 'mse', metrics = ['mse'])\n",
    "\n",
    "\t# train the model\n",
    "\tmodel.fit(x = x_train, y = y_train, batch_size = 32, epochs = n_epochs,\n",
    "                    shuffle = True, validation_data = (x_valid, y_valid))\n",
    "\tscore = model.evaluate(x_valid, y_valid, batch_size = 32, verbose = 0)\n",
    "\t\n",
    "\treturn score[0]\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.9663 - mse: 0.9663 - val_loss: 0.7162 - val_mse: 0.7162\n",
      "Epoch 2/10\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.5750 - mse: 0.5750 - val_loss: 0.4390 - val_mse: 0.4390\n",
      "Epoch 3/10\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3578 - mse: 0.3578 - val_loss: 0.2823 - val_mse: 0.2823\n",
      "Epoch 4/10\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.2339 - mse: 0.2339 - val_loss: 0.1895 - val_mse: 0.1895\n",
      "Epoch 5/10\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.1599 - mse: 0.1599 - val_loss: 0.1335 - val_mse: 0.1335\n",
      "Epoch 6/10\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.1149 - mse: 0.1149 - val_loss: 0.0985 - val_mse: 0.0985\n",
      "Epoch 7/10\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0863 - mse: 0.0863 - val_loss: 0.0755 - val_mse: 0.0755\n",
      "Epoch 8/10\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0672 - mse: 0.0672 - val_loss: 0.0593 - val_mse: 0.0593\n",
      "Epoch 9/10\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0536 - mse: 0.0536 - val_loss: 0.0478 - val_mse: 0.0478\n",
      "Epoch 10/10\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0438 - mse: 0.0438 - val_loss: 0.0395 - val_mse: 0.0395\n",
      "Epoch 1/10\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.7965 - mse: 0.7965 - val_loss: 0.4362 - val_mse: 0.4362\n",
      "Epoch 2/10\n",
      "32/32 [==============================] - 0s 849us/step - loss: 0.3059 - mse: 0.3059 - val_loss: 0.1942 - val_mse: 0.1942\n",
      "Epoch 3/10\n",
      "32/32 [==============================] - 0s 810us/step - loss: 0.1432 - mse: 0.1432 - val_loss: 0.1049 - val_mse: 0.1049\n",
      "Epoch 4/10\n",
      "32/32 [==============================] - 0s 774us/step - loss: 0.0792 - mse: 0.0792 - val_loss: 0.0670 - val_mse: 0.0670\n",
      "Epoch 5/10\n",
      "32/32 [==============================] - 0s 783us/step - loss: 0.0507 - mse: 0.0507 - val_loss: 0.0463 - val_mse: 0.0463\n",
      "Epoch 6/10\n",
      "32/32 [==============================] - 0s 780us/step - loss: 0.0350 - mse: 0.0350 - val_loss: 0.0337 - val_mse: 0.0337\n",
      "Epoch 7/10\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0256 - mse: 0.0256 - val_loss: 0.0257 - val_mse: 0.0257\n",
      "Epoch 8/10\n",
      "32/32 [==============================] - 0s 783us/step - loss: 0.0198 - mse: 0.0198 - val_loss: 0.0204 - val_mse: 0.0204\n",
      "Epoch 9/10\n",
      "32/32 [==============================] - 0s 790us/step - loss: 0.0162 - mse: 0.0162 - val_loss: 0.0168 - val_mse: 0.0168\n",
      "Epoch 10/10\n",
      "32/32 [==============================] - 0s 762us/step - loss: 0.0138 - mse: 0.0138 - val_loss: 0.0143 - val_mse: 0.0143\n",
      "Epoch 1/10\n",
      "47/47 [==============================] - 0s 2ms/step - loss: 0.6672 - mse: 0.6672 - val_loss: 0.3655 - val_mse: 0.3655\n",
      "Epoch 2/10\n",
      "47/47 [==============================] - 0s 683us/step - loss: 0.1821 - mse: 0.1821 - val_loss: 0.1220 - val_mse: 0.1220\n",
      "Epoch 3/10\n",
      "47/47 [==============================] - 0s 639us/step - loss: 0.0739 - mse: 0.0739 - val_loss: 0.0553 - val_mse: 0.0553\n",
      "Epoch 4/10\n",
      "47/47 [==============================] - 0s 615us/step - loss: 0.0391 - mse: 0.0391 - val_loss: 0.0312 - val_mse: 0.0312\n",
      "Epoch 5/10\n",
      "47/47 [==============================] - 0s 601us/step - loss: 0.0243 - mse: 0.0243 - val_loss: 0.0207 - val_mse: 0.0207\n",
      "Epoch 6/10\n",
      "47/47 [==============================] - 0s 596us/step - loss: 0.0171 - mse: 0.0171 - val_loss: 0.0155 - val_mse: 0.0155\n",
      "Epoch 7/10\n",
      "47/47 [==============================] - 0s 646us/step - loss: 0.0134 - mse: 0.0134 - val_loss: 0.0128 - val_mse: 0.0128\n",
      "Epoch 8/10\n",
      "47/47 [==============================] - 0s 643us/step - loss: 0.0115 - mse: 0.0115 - val_loss: 0.0114 - val_mse: 0.0114\n",
      "Epoch 9/10\n",
      "47/47 [==============================] - 0s 617us/step - loss: 0.0105 - mse: 0.0105 - val_loss: 0.0107 - val_mse: 0.0107\n",
      "Epoch 10/10\n",
      "47/47 [==============================] - 0s 625us/step - loss: 0.0100 - mse: 0.0100 - val_loss: 0.0103 - val_mse: 0.0103\n",
      "Epoch 1/10\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.5444 - mse: 0.5444 - val_loss: 0.1571 - val_mse: 0.1571\n",
      "Epoch 2/10\n",
      "63/63 [==============================] - 0s 565us/step - loss: 0.1050 - mse: 0.1050 - val_loss: 0.0520 - val_mse: 0.0520\n",
      "Epoch 3/10\n",
      "63/63 [==============================] - 0s 534us/step - loss: 0.0391 - mse: 0.0391 - val_loss: 0.0268 - val_mse: 0.0268\n",
      "Epoch 4/10\n",
      "63/63 [==============================] - 0s 528us/step - loss: 0.0210 - mse: 0.0210 - val_loss: 0.0171 - val_mse: 0.0171\n",
      "Epoch 5/10\n",
      "63/63 [==============================] - 0s 933us/step - loss: 0.0143 - mse: 0.0143 - val_loss: 0.0130 - val_mse: 0.0130\n",
      "Epoch 6/10\n",
      "63/63 [==============================] - 0s 555us/step - loss: 0.0115 - mse: 0.0115 - val_loss: 0.0111 - val_mse: 0.0111\n",
      "Epoch 7/10\n",
      "63/63 [==============================] - 0s 543us/step - loss: 0.0104 - mse: 0.0104 - val_loss: 0.0103 - val_mse: 0.0103\n",
      "Epoch 8/10\n",
      "63/63 [==============================] - 0s 539us/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.0099 - val_mse: 0.0099\n",
      "Epoch 9/10\n",
      "63/63 [==============================] - 0s 523us/step - loss: 0.0097 - mse: 0.0097 - val_loss: 0.0097 - val_mse: 0.0097\n",
      "Epoch 10/10\n",
      "63/63 [==============================] - 0s 540us/step - loss: 0.0097 - mse: 0.0097 - val_loss: 0.0096 - val_mse: 0.0096\n",
      "Epoch 1/15\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.1204 - mse: 4.1204 - val_loss: 3.1783 - val_mse: 3.1783\n",
      "Epoch 2/15\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 3.1190 - mse: 3.1190 - val_loss: 2.4439 - val_mse: 2.4439\n",
      "Epoch 3/15\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 2.4070 - mse: 2.4070 - val_loss: 1.9095 - val_mse: 1.9095\n",
      "Epoch 4/15\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.8840 - mse: 1.8840 - val_loss: 1.5122 - val_mse: 1.5122\n",
      "Epoch 5/15\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.4919 - mse: 1.4919 - val_loss: 1.2037 - val_mse: 1.2037\n",
      "Epoch 6/15\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1874 - mse: 1.1874 - val_loss: 0.9624 - val_mse: 0.9624\n",
      "Epoch 7/15\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.9489 - mse: 0.9489 - val_loss: 0.7708 - val_mse: 0.7708\n",
      "Epoch 8/15\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.7597 - mse: 0.7597 - val_loss: 0.6196 - val_mse: 0.6196\n",
      "Epoch 9/15\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.6109 - mse: 0.6109 - val_loss: 0.4979 - val_mse: 0.4979\n",
      "Epoch 10/15\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4910 - mse: 0.4910 - val_loss: 0.4010 - val_mse: 0.4010\n",
      "Epoch 11/15\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3958 - mse: 0.3958 - val_loss: 0.3239 - val_mse: 0.3239\n",
      "Epoch 12/15\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3200 - mse: 0.3200 - val_loss: 0.2622 - val_mse: 0.2622\n",
      "Epoch 13/15\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.2593 - mse: 0.2593 - val_loss: 0.2120 - val_mse: 0.2120\n",
      "Epoch 14/15\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.2100 - mse: 0.2100 - val_loss: 0.1722 - val_mse: 0.1722\n",
      "Epoch 15/15\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.1708 - mse: 0.1708 - val_loss: 0.1402 - val_mse: 0.1402\n",
      "Epoch 1/15\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.7312 - mse: 0.7312 - val_loss: 0.3810 - val_mse: 0.3810\n",
      "Epoch 2/15\n",
      "32/32 [==============================] - 0s 777us/step - loss: 0.2630 - mse: 0.2630 - val_loss: 0.1571 - val_mse: 0.1571\n",
      "Epoch 3/15\n",
      "32/32 [==============================] - 0s 763us/step - loss: 0.1152 - mse: 0.1152 - val_loss: 0.0817 - val_mse: 0.0817\n",
      "Epoch 4/15\n",
      "32/32 [==============================] - 0s 741us/step - loss: 0.0620 - mse: 0.0620 - val_loss: 0.0509 - val_mse: 0.0509\n",
      "Epoch 5/15\n",
      "32/32 [==============================] - 0s 747us/step - loss: 0.0391 - mse: 0.0391 - val_loss: 0.0356 - val_mse: 0.0356\n",
      "Epoch 6/15\n",
      "32/32 [==============================] - 0s 751us/step - loss: 0.0274 - mse: 0.0274 - val_loss: 0.0264 - val_mse: 0.0264\n",
      "Epoch 7/15\n",
      "32/32 [==============================] - 0s 760us/step - loss: 0.0206 - mse: 0.0206 - val_loss: 0.0208 - val_mse: 0.0208\n",
      "Epoch 8/15\n",
      "32/32 [==============================] - 0s 747us/step - loss: 0.0166 - mse: 0.0166 - val_loss: 0.0170 - val_mse: 0.0170\n",
      "Epoch 9/15\n",
      "32/32 [==============================] - 0s 750us/step - loss: 0.0141 - mse: 0.0141 - val_loss: 0.0144 - val_mse: 0.0144\n",
      "Epoch 10/15\n",
      "32/32 [==============================] - 0s 748us/step - loss: 0.0124 - mse: 0.0124 - val_loss: 0.0127 - val_mse: 0.0127\n",
      "Epoch 11/15\n",
      "32/32 [==============================] - 0s 773us/step - loss: 0.0114 - mse: 0.0114 - val_loss: 0.0115 - val_mse: 0.0115\n",
      "Epoch 12/15\n",
      "32/32 [==============================] - 0s 736us/step - loss: 0.0107 - mse: 0.0107 - val_loss: 0.0107 - val_mse: 0.0107\n",
      "Epoch 13/15\n",
      "32/32 [==============================] - 0s 734us/step - loss: 0.0103 - mse: 0.0103 - val_loss: 0.0100 - val_mse: 0.0100\n",
      "Epoch 14/15\n",
      "32/32 [==============================] - 0s 747us/step - loss: 0.0100 - mse: 0.0100 - val_loss: 0.0096 - val_mse: 0.0096\n",
      "Epoch 15/15\n",
      "32/32 [==============================] - 0s 749us/step - loss: 0.0098 - mse: 0.0098 - val_loss: 0.0094 - val_mse: 0.0094\n",
      "Epoch 1/15\n",
      "47/47 [==============================] - 0s 2ms/step - loss: 1.6789 - mse: 1.6789 - val_loss: 1.1449 - val_mse: 1.1449\n",
      "Epoch 2/15\n",
      "47/47 [==============================] - 0s 646us/step - loss: 0.7057 - mse: 0.7057 - val_loss: 0.4982 - val_mse: 0.4982\n",
      "Epoch 3/15\n",
      "47/47 [==============================] - 0s 612us/step - loss: 0.3458 - mse: 0.3458 - val_loss: 0.2434 - val_mse: 0.2434\n",
      "Epoch 4/15\n",
      "47/47 [==============================] - 0s 616us/step - loss: 0.1805 - mse: 0.1805 - val_loss: 0.1277 - val_mse: 0.1277\n",
      "Epoch 5/15\n",
      "47/47 [==============================] - 0s 589us/step - loss: 0.0978 - mse: 0.0978 - val_loss: 0.0707 - val_mse: 0.0707\n",
      "Epoch 6/15\n",
      "47/47 [==============================] - 0s 609us/step - loss: 0.0553 - mse: 0.0553 - val_loss: 0.0418 - val_mse: 0.0418\n",
      "Epoch 7/15\n",
      "47/47 [==============================] - 0s 605us/step - loss: 0.0333 - mse: 0.0333 - val_loss: 0.0267 - val_mse: 0.0267\n",
      "Epoch 8/15\n",
      "47/47 [==============================] - 0s 2ms/step - loss: 0.0218 - mse: 0.0218 - val_loss: 0.0188 - val_mse: 0.0188\n",
      "Epoch 9/15\n",
      "47/47 [==============================] - 0s 608us/step - loss: 0.0159 - mse: 0.0159 - val_loss: 0.0146 - val_mse: 0.0146\n",
      "Epoch 10/15\n",
      "47/47 [==============================] - 0s 636us/step - loss: 0.0128 - mse: 0.0128 - val_loss: 0.0124 - val_mse: 0.0124\n",
      "Epoch 11/15\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.0112 - mse: 0.0112 - val_loss: 0.0112 - val_mse: 0.0112\n",
      "Epoch 12/15\n",
      "47/47 [==============================] - 0s 843us/step - loss: 0.0104 - mse: 0.0104 - val_loss: 0.0105 - val_mse: 0.0105\n",
      "Epoch 13/15\n",
      "47/47 [==============================] - 0s 651us/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.0102 - val_mse: 0.0102\n",
      "Epoch 14/15\n",
      "47/47 [==============================] - 0s 626us/step - loss: 0.0097 - mse: 0.0097 - val_loss: 0.0100 - val_mse: 0.0100\n",
      "Epoch 15/15\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0099 - val_mse: 0.0099\n",
      "Epoch 1/15\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.6721 - mse: 1.6721 - val_loss: 0.7734 - val_mse: 0.7734\n",
      "Epoch 2/15\n",
      "63/63 [==============================] - 0s 553us/step - loss: 0.5682 - mse: 0.5682 - val_loss: 0.3231 - val_mse: 0.3231\n",
      "Epoch 3/15\n",
      "63/63 [==============================] - 0s 527us/step - loss: 0.2315 - mse: 0.2315 - val_loss: 0.1435 - val_mse: 0.1435\n",
      "Epoch 4/15\n",
      "63/63 [==============================] - 0s 529us/step - loss: 0.1013 - mse: 0.1013 - val_loss: 0.0667 - val_mse: 0.0667\n",
      "Epoch 5/15\n",
      "63/63 [==============================] - 0s 545us/step - loss: 0.0475 - mse: 0.0475 - val_loss: 0.0341 - val_mse: 0.0341\n",
      "Epoch 6/15\n",
      "63/63 [==============================] - 0s 525us/step - loss: 0.0254 - mse: 0.0254 - val_loss: 0.0202 - val_mse: 0.0202\n",
      "Epoch 7/15\n",
      "63/63 [==============================] - 0s 531us/step - loss: 0.0161 - mse: 0.0161 - val_loss: 0.0142 - val_mse: 0.0142\n",
      "Epoch 8/15\n",
      "63/63 [==============================] - 0s 577us/step - loss: 0.0123 - mse: 0.0123 - val_loss: 0.0116 - val_mse: 0.0116\n",
      "Epoch 9/15\n",
      "63/63 [==============================] - 0s 508us/step - loss: 0.0107 - mse: 0.0107 - val_loss: 0.0105 - val_mse: 0.0105\n",
      "Epoch 10/15\n",
      "63/63 [==============================] - 0s 519us/step - loss: 0.0101 - mse: 0.0101 - val_loss: 0.0100 - val_mse: 0.0100\n",
      "Epoch 11/15\n",
      "63/63 [==============================] - 0s 519us/step - loss: 0.0098 - mse: 0.0098 - val_loss: 0.0098 - val_mse: 0.0098\n",
      "Epoch 12/15\n",
      "63/63 [==============================] - 0s 518us/step - loss: 0.0097 - mse: 0.0097 - val_loss: 0.0096 - val_mse: 0.0096\n",
      "Epoch 13/15\n",
      "63/63 [==============================] - 0s 514us/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0096 - val_mse: 0.0096\n",
      "Epoch 14/15\n",
      "63/63 [==============================] - 0s 518us/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0096 - val_mse: 0.0096\n",
      "Epoch 15/15\n",
      "63/63 [==============================] - 0s 506us/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0095 - val_mse: 0.0095\n",
      "Epoch 1/30\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.1448 - mse: 4.1448 - val_loss: 3.2035 - val_mse: 3.2035\n",
      "Epoch 2/30\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 3.1435 - mse: 3.1435 - val_loss: 2.4655 - val_mse: 2.4655\n",
      "Epoch 3/30\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 2.4277 - mse: 2.4277 - val_loss: 1.9300 - val_mse: 1.9300\n",
      "Epoch 4/30\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.9029 - mse: 1.9029 - val_loss: 1.5234 - val_mse: 1.5234\n",
      "Epoch 5/30\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.5032 - mse: 1.5032 - val_loss: 1.2112 - val_mse: 1.2112\n",
      "Epoch 6/30\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1945 - mse: 1.1945 - val_loss: 0.9683 - val_mse: 0.9683\n",
      "Epoch 7/30\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.9549 - mse: 0.9549 - val_loss: 0.7776 - val_mse: 0.7776\n",
      "Epoch 8/30\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.7665 - mse: 0.7665 - val_loss: 0.6233 - val_mse: 0.6233\n",
      "Epoch 9/30\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.6144 - mse: 0.6144 - val_loss: 0.5014 - val_mse: 0.5014\n",
      "Epoch 10/30\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.4947 - mse: 0.4947 - val_loss: 0.4040 - val_mse: 0.4040\n",
      "Epoch 11/30\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3988 - mse: 0.3988 - val_loss: 0.3265 - val_mse: 0.3265\n",
      "Epoch 12/30\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3226 - mse: 0.3226 - val_loss: 0.2636 - val_mse: 0.2636\n",
      "Epoch 13/30\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.2608 - mse: 0.2608 - val_loss: 0.2138 - val_mse: 0.2138\n",
      "Epoch 14/30\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.2119 - mse: 0.2119 - val_loss: 0.1735 - val_mse: 0.1735\n",
      "Epoch 15/30\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.1722 - mse: 0.1722 - val_loss: 0.1413 - val_mse: 0.1413\n",
      "Epoch 16/30\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.1403 - mse: 0.1403 - val_loss: 0.1152 - val_mse: 0.1152\n",
      "Epoch 17/30\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.1147 - mse: 0.1147 - val_loss: 0.0945 - val_mse: 0.0945\n",
      "Epoch 18/30\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0941 - mse: 0.0941 - val_loss: 0.0777 - val_mse: 0.0777\n",
      "Epoch 19/30\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0775 - mse: 0.0775 - val_loss: 0.0644 - val_mse: 0.0644\n",
      "Epoch 20/30\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0643 - mse: 0.0643 - val_loss: 0.0537 - val_mse: 0.0537\n",
      "Epoch 21/30\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0536 - mse: 0.0536 - val_loss: 0.0451 - val_mse: 0.0451\n",
      "Epoch 22/30\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0450 - mse: 0.0450 - val_loss: 0.0382 - val_mse: 0.0382\n",
      "Epoch 23/30\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0381 - mse: 0.0381 - val_loss: 0.0327 - val_mse: 0.0327\n",
      "Epoch 24/30\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0325 - mse: 0.0325 - val_loss: 0.0283 - val_mse: 0.0283\n",
      "Epoch 25/30\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0281 - mse: 0.0281 - val_loss: 0.0248 - val_mse: 0.0248\n",
      "Epoch 26/30\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0245 - mse: 0.0245 - val_loss: 0.0219 - val_mse: 0.0219\n",
      "Epoch 27/30\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0216 - mse: 0.0216 - val_loss: 0.0197 - val_mse: 0.0197\n",
      "Epoch 28/30\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0193 - mse: 0.0193 - val_loss: 0.0179 - val_mse: 0.0179\n",
      "Epoch 29/30\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0175 - mse: 0.0175 - val_loss: 0.0165 - val_mse: 0.0165\n",
      "Epoch 30/30\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0160 - mse: 0.0160 - val_loss: 0.0154 - val_mse: 0.0154\n",
      "Epoch 1/30\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2.3798 - mse: 2.3798 - val_loss: 1.7723 - val_mse: 1.7723\n",
      "Epoch 2/30\n",
      "32/32 [==============================] - 0s 789us/step - loss: 1.3469 - mse: 1.3469 - val_loss: 1.0736 - val_mse: 1.0736\n",
      "Epoch 3/30\n",
      "32/32 [==============================] - 0s 772us/step - loss: 0.8192 - mse: 0.8192 - val_loss: 0.6859 - val_mse: 0.6859\n",
      "Epoch 4/30\n",
      "32/32 [==============================] - 0s 746us/step - loss: 0.5199 - mse: 0.5199 - val_loss: 0.4474 - val_mse: 0.4474\n",
      "Epoch 5/30\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.3360 - mse: 0.3360 - val_loss: 0.2955 - val_mse: 0.2955\n",
      "Epoch 6/30\n",
      "32/32 [==============================] - 0s 749us/step - loss: 0.2194 - mse: 0.2194 - val_loss: 0.1976 - val_mse: 0.1976\n",
      "Epoch 7/30\n",
      "32/32 [==============================] - 0s 742us/step - loss: 0.1451 - mse: 0.1451 - val_loss: 0.1340 - val_mse: 0.1340\n",
      "Epoch 8/30\n",
      "32/32 [==============================] - 0s 755us/step - loss: 0.0975 - mse: 0.0975 - val_loss: 0.0920 - val_mse: 0.0920\n",
      "Epoch 9/30\n",
      "32/32 [==============================] - 0s 744us/step - loss: 0.0667 - mse: 0.0667 - val_loss: 0.0644 - val_mse: 0.0644\n",
      "Epoch 10/30\n",
      "32/32 [==============================] - 0s 746us/step - loss: 0.0466 - mse: 0.0466 - val_loss: 0.0458 - val_mse: 0.0458\n",
      "Epoch 11/30\n",
      "32/32 [==============================] - 0s 738us/step - loss: 0.0335 - mse: 0.0335 - val_loss: 0.0338 - val_mse: 0.0338\n",
      "Epoch 12/30\n",
      "32/32 [==============================] - 0s 764us/step - loss: 0.0250 - mse: 0.0250 - val_loss: 0.0258 - val_mse: 0.0258\n",
      "Epoch 13/30\n",
      "32/32 [==============================] - 0s 729us/step - loss: 0.0196 - mse: 0.0196 - val_loss: 0.0204 - val_mse: 0.0204\n",
      "Epoch 14/30\n",
      "32/32 [==============================] - 0s 726us/step - loss: 0.0160 - mse: 0.0160 - val_loss: 0.0166 - val_mse: 0.0166\n",
      "Epoch 15/30\n",
      "32/32 [==============================] - 0s 738us/step - loss: 0.0136 - mse: 0.0136 - val_loss: 0.0142 - val_mse: 0.0142\n",
      "Epoch 16/30\n",
      "32/32 [==============================] - 0s 757us/step - loss: 0.0122 - mse: 0.0122 - val_loss: 0.0124 - val_mse: 0.0124\n",
      "Epoch 17/30\n",
      "32/32 [==============================] - 0s 739us/step - loss: 0.0112 - mse: 0.0112 - val_loss: 0.0113 - val_mse: 0.0113\n",
      "Epoch 18/30\n",
      "32/32 [==============================] - 0s 740us/step - loss: 0.0106 - mse: 0.0106 - val_loss: 0.0105 - val_mse: 0.0105\n",
      "Epoch 19/30\n",
      "32/32 [==============================] - 0s 740us/step - loss: 0.0102 - mse: 0.0102 - val_loss: 0.0100 - val_mse: 0.0100\n",
      "Epoch 20/30\n",
      "32/32 [==============================] - 0s 764us/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.0095 - val_mse: 0.0095\n",
      "Epoch 21/30\n",
      "32/32 [==============================] - 0s 739us/step - loss: 0.0098 - mse: 0.0098 - val_loss: 0.0093 - val_mse: 0.0093\n",
      "Epoch 22/30\n",
      "32/32 [==============================] - 0s 737us/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0091 - val_mse: 0.0091\n",
      "Epoch 23/30\n",
      "32/32 [==============================] - 0s 732us/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0089 - val_mse: 0.0089\n",
      "Epoch 24/30\n",
      "32/32 [==============================] - 0s 774us/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0088 - val_mse: 0.0088\n",
      "Epoch 25/30\n",
      "32/32 [==============================] - 0s 756us/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0087 - val_mse: 0.0087\n",
      "Epoch 26/30\n",
      "32/32 [==============================] - 0s 724us/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0087 - val_mse: 0.0087\n",
      "Epoch 27/30\n",
      "32/32 [==============================] - 0s 735us/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0087 - val_mse: 0.0087\n",
      "Epoch 28/30\n",
      "32/32 [==============================] - 0s 745us/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0086 - val_mse: 0.0086\n",
      "Epoch 29/30\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0086 - val_mse: 0.0086\n",
      "Epoch 30/30\n",
      "32/32 [==============================] - 0s 741us/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0085 - val_mse: 0.0085\n",
      "Epoch 1/30\n",
      "47/47 [==============================] - 0s 2ms/step - loss: 3.5051 - mse: 3.5051 - val_loss: 2.4512 - val_mse: 2.4512\n",
      "Epoch 2/30\n",
      "47/47 [==============================] - 0s 646us/step - loss: 1.6567 - mse: 1.6567 - val_loss: 1.1534 - val_mse: 1.1534\n",
      "Epoch 3/30\n",
      "47/47 [==============================] - 0s 604us/step - loss: 0.8402 - mse: 0.8402 - val_loss: 0.5762 - val_mse: 0.5762\n",
      "Epoch 4/30\n",
      "47/47 [==============================] - 0s 612us/step - loss: 0.4373 - mse: 0.4373 - val_loss: 0.2984 - val_mse: 0.2984\n",
      "Epoch 5/30\n",
      "47/47 [==============================] - 0s 614us/step - loss: 0.2312 - mse: 0.2312 - val_loss: 0.1593 - val_mse: 0.1593\n",
      "Epoch 6/30\n",
      "47/47 [==============================] - 0s 589us/step - loss: 0.1246 - mse: 0.1246 - val_loss: 0.0879 - val_mse: 0.0879\n",
      "Epoch 7/30\n",
      "47/47 [==============================] - 0s 622us/step - loss: 0.0693 - mse: 0.0693 - val_loss: 0.0509 - val_mse: 0.0509\n",
      "Epoch 8/30\n",
      "47/47 [==============================] - 0s 609us/step - loss: 0.0406 - mse: 0.0406 - val_loss: 0.0315 - val_mse: 0.0315\n",
      "Epoch 9/30\n",
      "47/47 [==============================] - 0s 616us/step - loss: 0.0256 - mse: 0.0256 - val_loss: 0.0214 - val_mse: 0.0214\n",
      "Epoch 10/30\n",
      "47/47 [==============================] - 0s 618us/step - loss: 0.0179 - mse: 0.0179 - val_loss: 0.0160 - val_mse: 0.0160\n",
      "Epoch 11/30\n",
      "47/47 [==============================] - 0s 604us/step - loss: 0.0138 - mse: 0.0138 - val_loss: 0.0132 - val_mse: 0.0132\n",
      "Epoch 12/30\n",
      "47/47 [==============================] - 0s 602us/step - loss: 0.0117 - mse: 0.0117 - val_loss: 0.0116 - val_mse: 0.0116\n",
      "Epoch 13/30\n",
      "47/47 [==============================] - 0s 599us/step - loss: 0.0106 - mse: 0.0106 - val_loss: 0.0108 - val_mse: 0.0108\n",
      "Epoch 14/30\n",
      "47/47 [==============================] - 0s 601us/step - loss: 0.0101 - mse: 0.0101 - val_loss: 0.0103 - val_mse: 0.0103\n",
      "Epoch 15/30\n",
      "47/47 [==============================] - 0s 592us/step - loss: 0.0098 - mse: 0.0098 - val_loss: 0.0101 - val_mse: 0.0101\n",
      "Epoch 16/30\n",
      "47/47 [==============================] - 0s 611us/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0099 - val_mse: 0.0099\n",
      "Epoch 17/30\n",
      "47/47 [==============================] - 0s 619us/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0098 - val_mse: 0.0098\n",
      "Epoch 18/30\n",
      "47/47 [==============================] - 0s 630us/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0098 - val_mse: 0.0098\n",
      "Epoch 19/30\n",
      "47/47 [==============================] - 0s 641us/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0097 - val_mse: 0.0097\n",
      "Epoch 20/30\n",
      "47/47 [==============================] - 0s 613us/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0097 - val_mse: 0.0097\n",
      "Epoch 21/30\n",
      "47/47 [==============================] - 0s 612us/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0097 - val_mse: 0.0097\n",
      "Epoch 22/30\n",
      "47/47 [==============================] - 0s 628us/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0097 - val_mse: 0.0097\n",
      "Epoch 23/30\n",
      "47/47 [==============================] - 0s 579us/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0097 - val_mse: 0.0097\n",
      "Epoch 24/30\n",
      "47/47 [==============================] - 0s 2ms/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0097 - val_mse: 0.0097\n",
      "Epoch 25/30\n",
      "47/47 [==============================] - 0s 623us/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0097 - val_mse: 0.0097\n",
      "Epoch 26/30\n",
      "47/47 [==============================] - 0s 616us/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0097 - val_mse: 0.0097\n",
      "Epoch 27/30\n",
      "47/47 [==============================] - 0s 602us/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0097 - val_mse: 0.0097\n",
      "Epoch 28/30\n",
      "47/47 [==============================] - 0s 589us/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0097 - val_mse: 0.0097\n",
      "Epoch 29/30\n",
      "47/47 [==============================] - 0s 609us/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0097 - val_mse: 0.0097\n",
      "Epoch 30/30\n",
      "47/47 [==============================] - 0s 602us/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0097 - val_mse: 0.0097\n",
      "Epoch 1/30\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 3.5484 - mse: 3.5484 - val_loss: 1.8417 - val_mse: 1.8417\n",
      "Epoch 2/30\n",
      "63/63 [==============================] - 0s 562us/step - loss: 1.3454 - mse: 1.3454 - val_loss: 0.7765 - val_mse: 0.7765\n",
      "Epoch 3/30\n",
      "63/63 [==============================] - 0s 536us/step - loss: 0.5530 - mse: 0.5530 - val_loss: 0.3345 - val_mse: 0.3345\n",
      "Epoch 4/30\n",
      "63/63 [==============================] - 0s 523us/step - loss: 0.2340 - mse: 0.2340 - val_loss: 0.1467 - val_mse: 0.1467\n",
      "Epoch 5/30\n",
      "63/63 [==============================] - 0s 525us/step - loss: 0.1026 - mse: 0.1026 - val_loss: 0.0678 - val_mse: 0.0678\n",
      "Epoch 6/30\n",
      "63/63 [==============================] - 0s 513us/step - loss: 0.0482 - mse: 0.0482 - val_loss: 0.0343 - val_mse: 0.0343\n",
      "Epoch 7/30\n",
      "63/63 [==============================] - 0s 516us/step - loss: 0.0256 - mse: 0.0256 - val_loss: 0.0203 - val_mse: 0.0203\n",
      "Epoch 8/30\n",
      "63/63 [==============================] - 0s 523us/step - loss: 0.0162 - mse: 0.0162 - val_loss: 0.0143 - val_mse: 0.0143\n",
      "Epoch 9/30\n",
      "63/63 [==============================] - 0s 516us/step - loss: 0.0123 - mse: 0.0123 - val_loss: 0.0117 - val_mse: 0.0117\n",
      "Epoch 10/30\n",
      "63/63 [==============================] - 0s 514us/step - loss: 0.0107 - mse: 0.0107 - val_loss: 0.0105 - val_mse: 0.0105\n",
      "Epoch 11/30\n",
      "63/63 [==============================] - 0s 511us/step - loss: 0.0101 - mse: 0.0101 - val_loss: 0.0100 - val_mse: 0.0100\n",
      "Epoch 12/30\n",
      "63/63 [==============================] - 0s 514us/step - loss: 0.0098 - mse: 0.0098 - val_loss: 0.0098 - val_mse: 0.0098\n",
      "Epoch 13/30\n",
      "63/63 [==============================] - 0s 509us/step - loss: 0.0097 - mse: 0.0097 - val_loss: 0.0097 - val_mse: 0.0097\n",
      "Epoch 14/30\n",
      "63/63 [==============================] - 0s 504us/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0096 - val_mse: 0.0096\n",
      "Epoch 15/30\n",
      "63/63 [==============================] - 0s 523us/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0096 - val_mse: 0.0096\n",
      "Epoch 16/30\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0095 - val_mse: 0.0095\n",
      "Epoch 17/30\n",
      "63/63 [==============================] - 0s 520us/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0095 - val_mse: 0.0095\n",
      "Epoch 18/30\n",
      "63/63 [==============================] - 0s 521us/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0095 - val_mse: 0.0095\n",
      "Epoch 19/30\n",
      "63/63 [==============================] - 0s 509us/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0095 - val_mse: 0.0095\n",
      "Epoch 20/30\n",
      "63/63 [==============================] - 0s 508us/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0095 - val_mse: 0.0095\n",
      "Epoch 21/30\n",
      "63/63 [==============================] - 0s 508us/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0095 - val_mse: 0.0095\n",
      "Epoch 22/30\n",
      "63/63 [==============================] - 0s 521us/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0095 - val_mse: 0.0095\n",
      "Epoch 23/30\n",
      "63/63 [==============================] - 0s 525us/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0095 - val_mse: 0.0095\n",
      "Epoch 24/30\n",
      "63/63 [==============================] - 0s 504us/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0095 - val_mse: 0.0095\n",
      "Epoch 25/30\n",
      "63/63 [==============================] - 0s 523us/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0095 - val_mse: 0.0095\n",
      "Epoch 26/30\n",
      "63/63 [==============================] - 0s 506us/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0095 - val_mse: 0.0095\n",
      "Epoch 27/30\n",
      "63/63 [==============================] - 0s 500us/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0095 - val_mse: 0.0095\n",
      "Epoch 28/30\n",
      "63/63 [==============================] - 0s 518us/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0095 - val_mse: 0.0095\n",
      "Epoch 29/30\n",
      "63/63 [==============================] - 0s 518us/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0095 - val_mse: 0.0095\n",
      "Epoch 30/30\n",
      "63/63 [==============================] - 0s 516us/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0095 - val_mse: 0.0095\n",
      "Epoch 1/50\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1.3875 - mse: 1.3875 - val_loss: 1.0286 - val_mse: 1.0286\n",
      "Epoch 2/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.9168 - mse: 0.9168 - val_loss: 0.6985 - val_mse: 0.6985\n",
      "Epoch 3/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.6352 - mse: 0.6352 - val_loss: 0.4918 - val_mse: 0.4918\n",
      "Epoch 4/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.4551 - mse: 0.4551 - val_loss: 0.3618 - val_mse: 0.3618\n",
      "Epoch 5/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3393 - mse: 0.3393 - val_loss: 0.2744 - val_mse: 0.2744\n",
      "Epoch 6/50\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.2598 - mse: 0.2598 - val_loss: 0.2123 - val_mse: 0.2123\n",
      "Epoch 7/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2027 - mse: 0.2027 - val_loss: 0.1670 - val_mse: 0.1670\n",
      "Epoch 8/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.1605 - mse: 0.1605 - val_loss: 0.1334 - val_mse: 0.1334\n",
      "Epoch 9/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.1288 - mse: 0.1288 - val_loss: 0.1077 - val_mse: 0.1077\n",
      "Epoch 10/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.1044 - mse: 0.1044 - val_loss: 0.0876 - val_mse: 0.0876\n",
      "Epoch 11/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0852 - mse: 0.0852 - val_loss: 0.0717 - val_mse: 0.0717\n",
      "Epoch 12/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0700 - mse: 0.0700 - val_loss: 0.0592 - val_mse: 0.0592\n",
      "Epoch 13/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0580 - mse: 0.0580 - val_loss: 0.0494 - val_mse: 0.0494\n",
      "Epoch 14/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0485 - mse: 0.0485 - val_loss: 0.0416 - val_mse: 0.0416\n",
      "Epoch 15/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0409 - mse: 0.0409 - val_loss: 0.0353 - val_mse: 0.0353\n",
      "Epoch 16/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0347 - mse: 0.0347 - val_loss: 0.0304 - val_mse: 0.0304\n",
      "Epoch 17/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0298 - mse: 0.0298 - val_loss: 0.0264 - val_mse: 0.0264\n",
      "Epoch 18/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0259 - mse: 0.0259 - val_loss: 0.0232 - val_mse: 0.0232\n",
      "Epoch 19/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0228 - mse: 0.0228 - val_loss: 0.0206 - val_mse: 0.0206\n",
      "Epoch 20/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0202 - mse: 0.0202 - val_loss: 0.0186 - val_mse: 0.0186\n",
      "Epoch 21/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0182 - mse: 0.0182 - val_loss: 0.0171 - val_mse: 0.0171\n",
      "Epoch 22/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0165 - mse: 0.0165 - val_loss: 0.0158 - val_mse: 0.0158\n",
      "Epoch 23/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0152 - mse: 0.0152 - val_loss: 0.0148 - val_mse: 0.0148\n",
      "Epoch 24/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0142 - mse: 0.0142 - val_loss: 0.0140 - val_mse: 0.0140\n",
      "Epoch 25/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0133 - mse: 0.0133 - val_loss: 0.0134 - val_mse: 0.0134\n",
      "Epoch 26/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0126 - mse: 0.0126 - val_loss: 0.0129 - val_mse: 0.0129\n",
      "Epoch 27/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0121 - mse: 0.0121 - val_loss: 0.0125 - val_mse: 0.0125\n",
      "Epoch 28/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0116 - mse: 0.0116 - val_loss: 0.0122 - val_mse: 0.0122\n",
      "Epoch 29/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0113 - mse: 0.0113 - val_loss: 0.0119 - val_mse: 0.0119\n",
      "Epoch 30/50\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.0110 - mse: 0.0110 - val_loss: 0.0117 - val_mse: 0.0117\n",
      "Epoch 31/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0107 - mse: 0.0107 - val_loss: 0.0116 - val_mse: 0.0116\n",
      "Epoch 32/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0105 - mse: 0.0105 - val_loss: 0.0115 - val_mse: 0.0115\n",
      "Epoch 33/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0104 - mse: 0.0104 - val_loss: 0.0114 - val_mse: 0.0114\n",
      "Epoch 34/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0103 - mse: 0.0103 - val_loss: 0.0113 - val_mse: 0.0113\n",
      "Epoch 35/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0102 - mse: 0.0102 - val_loss: 0.0113 - val_mse: 0.0113\n",
      "Epoch 36/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0101 - mse: 0.0101 - val_loss: 0.0112 - val_mse: 0.0112\n",
      "Epoch 37/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0100 - mse: 0.0100 - val_loss: 0.0111 - val_mse: 0.0111\n",
      "Epoch 38/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0100 - mse: 0.0100 - val_loss: 0.0111 - val_mse: 0.0111\n",
      "Epoch 39/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0100 - mse: 0.0100 - val_loss: 0.0111 - val_mse: 0.0111\n",
      "Epoch 40/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.0111 - val_mse: 0.0111\n",
      "Epoch 41/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.0111 - val_mse: 0.0111\n",
      "Epoch 42/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.0111 - val_mse: 0.0111\n",
      "Epoch 43/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.0111 - val_mse: 0.0111\n",
      "Epoch 44/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0098 - mse: 0.0098 - val_loss: 0.0111 - val_mse: 0.0111\n",
      "Epoch 45/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0098 - mse: 0.0098 - val_loss: 0.0111 - val_mse: 0.0111\n",
      "Epoch 46/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0098 - mse: 0.0098 - val_loss: 0.0111 - val_mse: 0.0111\n",
      "Epoch 47/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0098 - mse: 0.0098 - val_loss: 0.0111 - val_mse: 0.0111\n",
      "Epoch 48/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0098 - mse: 0.0098 - val_loss: 0.0111 - val_mse: 0.0111\n",
      "Epoch 49/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0098 - mse: 0.0098 - val_loss: 0.0112 - val_mse: 0.0112\n",
      "Epoch 50/50\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.0098 - mse: 0.0098 - val_loss: 0.0112 - val_mse: 0.0112\n",
      "Epoch 1/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.6332 - mse: 1.6332 - val_loss: 1.1400 - val_mse: 1.1400\n",
      "Epoch 2/50\n",
      "32/32 [==============================] - 0s 854us/step - loss: 0.8553 - mse: 0.8553 - val_loss: 0.6623 - val_mse: 0.6623\n",
      "Epoch 3/50\n",
      "32/32 [==============================] - 0s 956us/step - loss: 0.5021 - mse: 0.5021 - val_loss: 0.4103 - val_mse: 0.4103\n",
      "Epoch 4/50\n",
      "32/32 [==============================] - 0s 806us/step - loss: 0.3096 - mse: 0.3096 - val_loss: 0.2675 - val_mse: 0.2675\n",
      "Epoch 5/50\n",
      "32/32 [==============================] - 0s 897us/step - loss: 0.1997 - mse: 0.1997 - val_loss: 0.1773 - val_mse: 0.1773\n",
      "Epoch 6/50\n",
      "32/32 [==============================] - 0s 805us/step - loss: 0.1310 - mse: 0.1310 - val_loss: 0.1193 - val_mse: 0.1193\n",
      "Epoch 7/50\n",
      "32/32 [==============================] - 0s 792us/step - loss: 0.0873 - mse: 0.0873 - val_loss: 0.0820 - val_mse: 0.0820\n",
      "Epoch 8/50\n",
      "32/32 [==============================] - 0s 742us/step - loss: 0.0596 - mse: 0.0596 - val_loss: 0.0578 - val_mse: 0.0578\n",
      "Epoch 9/50\n",
      "32/32 [==============================] - 0s 792us/step - loss: 0.0420 - mse: 0.0420 - val_loss: 0.0416 - val_mse: 0.0416\n",
      "Epoch 10/50\n",
      "32/32 [==============================] - 0s 731us/step - loss: 0.0304 - mse: 0.0304 - val_loss: 0.0309 - val_mse: 0.0309\n",
      "Epoch 11/50\n",
      "32/32 [==============================] - 0s 721us/step - loss: 0.0230 - mse: 0.0230 - val_loss: 0.0237 - val_mse: 0.0237\n",
      "Epoch 12/50\n",
      "32/32 [==============================] - 0s 720us/step - loss: 0.0182 - mse: 0.0182 - val_loss: 0.0190 - val_mse: 0.0190\n",
      "Epoch 13/50\n",
      "32/32 [==============================] - 0s 749us/step - loss: 0.0151 - mse: 0.0151 - val_loss: 0.0157 - val_mse: 0.0157\n",
      "Epoch 14/50\n",
      "32/32 [==============================] - 0s 734us/step - loss: 0.0132 - mse: 0.0132 - val_loss: 0.0135 - val_mse: 0.0135\n",
      "Epoch 15/50\n",
      "32/32 [==============================] - 0s 736us/step - loss: 0.0118 - mse: 0.0118 - val_loss: 0.0121 - val_mse: 0.0121\n",
      "Epoch 16/50\n",
      "32/32 [==============================] - 0s 758us/step - loss: 0.0110 - mse: 0.0110 - val_loss: 0.0111 - val_mse: 0.0111\n",
      "Epoch 17/50\n",
      "32/32 [==============================] - 0s 779us/step - loss: 0.0105 - mse: 0.0105 - val_loss: 0.0104 - val_mse: 0.0104\n",
      "Epoch 18/50\n",
      "32/32 [==============================] - 0s 739us/step - loss: 0.0101 - mse: 0.0101 - val_loss: 0.0099 - val_mse: 0.0099\n",
      "Epoch 19/50\n",
      "32/32 [==============================] - 0s 771us/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.0095 - val_mse: 0.0095\n",
      "Epoch 20/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0097 - mse: 0.0097 - val_loss: 0.0093 - val_mse: 0.0093\n",
      "Epoch 21/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0091 - val_mse: 0.0091\n",
      "Epoch 22/50\n",
      "32/32 [==============================] - 0s 753us/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0090 - val_mse: 0.0090\n",
      "Epoch 23/50\n",
      "32/32 [==============================] - 0s 771us/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0088 - val_mse: 0.0088\n",
      "Epoch 24/50\n",
      "32/32 [==============================] - 0s 789us/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0087 - val_mse: 0.0087\n",
      "Epoch 25/50\n",
      "32/32 [==============================] - 0s 810us/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0086 - val_mse: 0.0086\n",
      "Epoch 26/50\n",
      "32/32 [==============================] - 0s 781us/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0086 - val_mse: 0.0086\n",
      "Epoch 27/50\n",
      "32/32 [==============================] - 0s 766us/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0085 - val_mse: 0.0085\n",
      "Epoch 28/50\n",
      "32/32 [==============================] - 0s 753us/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0085 - val_mse: 0.0085\n",
      "Epoch 29/50\n",
      "32/32 [==============================] - 0s 733us/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0085 - val_mse: 0.0085\n",
      "Epoch 30/50\n",
      "32/32 [==============================] - 0s 749us/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0084 - val_mse: 0.0084\n",
      "Epoch 31/50\n",
      "32/32 [==============================] - 0s 774us/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0085 - val_mse: 0.0085\n",
      "Epoch 32/50\n",
      "32/32 [==============================] - 0s 734us/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0085 - val_mse: 0.0085\n",
      "Epoch 33/50\n",
      "32/32 [==============================] - 0s 756us/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0085 - val_mse: 0.0085\n",
      "Epoch 34/50\n",
      "32/32 [==============================] - 0s 772us/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0084 - val_mse: 0.0084\n",
      "Epoch 35/50\n",
      "32/32 [==============================] - 0s 798us/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0085 - val_mse: 0.0085\n",
      "Epoch 36/50\n",
      "32/32 [==============================] - 0s 804us/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0084 - val_mse: 0.0084\n",
      "Epoch 37/50\n",
      "32/32 [==============================] - 0s 830us/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0084 - val_mse: 0.0084\n",
      "Epoch 38/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0084 - val_mse: 0.0084\n",
      "Epoch 39/50\n",
      "32/32 [==============================] - 0s 758us/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0084 - val_mse: 0.0084\n",
      "Epoch 40/50\n",
      "32/32 [==============================] - 0s 758us/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0084 - val_mse: 0.0084\n",
      "Epoch 41/50\n",
      "32/32 [==============================] - 0s 739us/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0084 - val_mse: 0.0084\n",
      "Epoch 42/50\n",
      "32/32 [==============================] - 0s 757us/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0084 - val_mse: 0.0084\n",
      "Epoch 43/50\n",
      "32/32 [==============================] - 0s 745us/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0084 - val_mse: 0.0084\n",
      "Epoch 44/50\n",
      "32/32 [==============================] - 0s 729us/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0084 - val_mse: 0.0084\n",
      "Epoch 45/50\n",
      "32/32 [==============================] - 0s 747us/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0084 - val_mse: 0.0084\n",
      "Epoch 46/50\n",
      "32/32 [==============================] - 0s 758us/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0085 - val_mse: 0.0085\n",
      "Epoch 47/50\n",
      "32/32 [==============================] - 0s 749us/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0084 - val_mse: 0.0084\n",
      "Epoch 48/50\n",
      "32/32 [==============================] - 0s 745us/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0084 - val_mse: 0.0084\n",
      "Epoch 49/50\n",
      "32/32 [==============================] - 0s 757us/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0084 - val_mse: 0.0084\n",
      "Epoch 50/50\n",
      "32/32 [==============================] - 0s 752us/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0084 - val_mse: 0.0084\n",
      "Epoch 1/50\n",
      "47/47 [==============================] - 0s 2ms/step - loss: 1.9724 - mse: 1.9724 - val_loss: 1.3605 - val_mse: 1.3605\n",
      "Epoch 2/50\n",
      "47/47 [==============================] - 0s 644us/step - loss: 0.8592 - mse: 0.8592 - val_loss: 0.6045 - val_mse: 0.6045\n",
      "Epoch 3/50\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.4256 - mse: 0.4256 - val_loss: 0.2981 - val_mse: 0.2981\n",
      "Epoch 4/50\n",
      "47/47 [==============================] - 0s 649us/step - loss: 0.2219 - mse: 0.2219 - val_loss: 0.1557 - val_mse: 0.1557\n",
      "Epoch 5/50\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.1193 - mse: 0.1193 - val_loss: 0.0850 - val_mse: 0.0850\n",
      "Epoch 6/50\n",
      "47/47 [==============================] - 0s 839us/step - loss: 0.0664 - mse: 0.0664 - val_loss: 0.0491 - val_mse: 0.0491\n",
      "Epoch 7/50\n",
      "47/47 [==============================] - 0s 619us/step - loss: 0.0390 - mse: 0.0390 - val_loss: 0.0306 - val_mse: 0.0306\n",
      "Epoch 8/50\n",
      "47/47 [==============================] - 0s 618us/step - loss: 0.0248 - mse: 0.0248 - val_loss: 0.0209 - val_mse: 0.0209\n",
      "Epoch 9/50\n",
      "47/47 [==============================] - 0s 610us/step - loss: 0.0175 - mse: 0.0175 - val_loss: 0.0158 - val_mse: 0.0158\n",
      "Epoch 10/50\n",
      "47/47 [==============================] - 0s 599us/step - loss: 0.0136 - mse: 0.0136 - val_loss: 0.0130 - val_mse: 0.0130\n",
      "Epoch 11/50\n",
      "47/47 [==============================] - 0s 609us/step - loss: 0.0116 - mse: 0.0116 - val_loss: 0.0115 - val_mse: 0.0115\n",
      "Epoch 12/50\n",
      "47/47 [==============================] - 0s 612us/step - loss: 0.0106 - mse: 0.0106 - val_loss: 0.0107 - val_mse: 0.0107\n",
      "Epoch 13/50\n",
      "47/47 [==============================] - 0s 595us/step - loss: 0.0100 - mse: 0.0100 - val_loss: 0.0103 - val_mse: 0.0103\n",
      "Epoch 14/50\n",
      "47/47 [==============================] - 0s 580us/step - loss: 0.0098 - mse: 0.0098 - val_loss: 0.0100 - val_mse: 0.0100\n",
      "Epoch 15/50\n",
      "47/47 [==============================] - 0s 609us/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0099 - val_mse: 0.0099\n",
      "Epoch 16/50\n",
      "47/47 [==============================] - 0s 599us/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0098 - val_mse: 0.0098\n",
      "Epoch 17/50\n",
      "47/47 [==============================] - 0s 615us/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0098 - val_mse: 0.0098\n",
      "Epoch 18/50\n",
      "47/47 [==============================] - 0s 598us/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0097 - val_mse: 0.0097\n",
      "Epoch 19/50\n",
      "47/47 [==============================] - 0s 589us/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0097 - val_mse: 0.0097\n",
      "Epoch 20/50\n",
      "47/47 [==============================] - 0s 597us/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0097 - val_mse: 0.0097\n",
      "Epoch 21/50\n",
      "47/47 [==============================] - 0s 611us/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0097 - val_mse: 0.0097\n",
      "Epoch 22/50\n",
      "47/47 [==============================] - 0s 591us/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0097 - val_mse: 0.0097\n",
      "Epoch 23/50\n",
      "47/47 [==============================] - 0s 593us/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0097 - val_mse: 0.0097\n",
      "Epoch 24/50\n",
      "47/47 [==============================] - 0s 2ms/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0097 - val_mse: 0.0097\n",
      "Epoch 25/50\n",
      "47/47 [==============================] - 0s 590us/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0097 - val_mse: 0.0097\n",
      "Epoch 26/50\n",
      "47/47 [==============================] - 0s 596us/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0097 - val_mse: 0.0097\n",
      "Epoch 27/50\n",
      "47/47 [==============================] - 0s 601us/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0097 - val_mse: 0.0097\n",
      "Epoch 28/50\n",
      "47/47 [==============================] - 0s 587us/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0097 - val_mse: 0.0097\n",
      "Epoch 29/50\n",
      "47/47 [==============================] - 0s 577us/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0097 - val_mse: 0.0097\n",
      "Epoch 30/50\n",
      "47/47 [==============================] - 0s 593us/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0097 - val_mse: 0.0097\n",
      "Epoch 31/50\n",
      "47/47 [==============================] - 0s 601us/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0097 - val_mse: 0.0097\n",
      "Epoch 32/50\n",
      "47/47 [==============================] - 0s 595us/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0097 - val_mse: 0.0097\n",
      "Epoch 33/50\n",
      "47/47 [==============================] - 0s 591us/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0097 - val_mse: 0.0097\n",
      "Epoch 34/50\n",
      "47/47 [==============================] - 0s 593us/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0097 - val_mse: 0.0097\n",
      "Epoch 35/50\n",
      "47/47 [==============================] - 0s 598us/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0097 - val_mse: 0.0097\n",
      "Epoch 36/50\n",
      "47/47 [==============================] - 0s 604us/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0097 - val_mse: 0.0097\n",
      "Epoch 37/50\n",
      "47/47 [==============================] - 0s 604us/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0097 - val_mse: 0.0097\n",
      "Epoch 38/50\n",
      "47/47 [==============================] - 0s 591us/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0097 - val_mse: 0.0097\n",
      "Epoch 39/50\n",
      "47/47 [==============================] - 0s 594us/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0097 - val_mse: 0.0097\n",
      "Epoch 40/50\n",
      "47/47 [==============================] - 0s 2ms/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0097 - val_mse: 0.0097\n",
      "Epoch 41/50\n",
      "47/47 [==============================] - 0s 594us/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0097 - val_mse: 0.0097\n",
      "Epoch 42/50\n",
      "47/47 [==============================] - 0s 600us/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0097 - val_mse: 0.0097\n",
      "Epoch 43/50\n",
      "47/47 [==============================] - 0s 615us/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0096 - val_mse: 0.0096\n",
      "Epoch 44/50\n",
      "47/47 [==============================] - 0s 605us/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0097 - val_mse: 0.0097\n",
      "Epoch 45/50\n",
      "47/47 [==============================] - 0s 594us/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0097 - val_mse: 0.0097\n",
      "Epoch 46/50\n",
      "47/47 [==============================] - 0s 593us/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0096 - val_mse: 0.0096\n",
      "Epoch 47/50\n",
      "47/47 [==============================] - 0s 601us/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0096 - val_mse: 0.0096\n",
      "Epoch 48/50\n",
      "47/47 [==============================] - 0s 609us/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0097 - val_mse: 0.0097\n",
      "Epoch 49/50\n",
      "47/47 [==============================] - 0s 593us/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0097 - val_mse: 0.0097\n",
      "Epoch 50/50\n",
      "47/47 [==============================] - 0s 594us/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0097 - val_mse: 0.0097\n",
      "Epoch 1/50\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.2666 - mse: 1.2666 - val_loss: 0.5465 - val_mse: 0.5465\n",
      "Epoch 2/50\n",
      "63/63 [==============================] - 0s 537us/step - loss: 0.4010 - mse: 0.4010 - val_loss: 0.2235 - val_mse: 0.2235\n",
      "Epoch 3/50\n",
      "63/63 [==============================] - 0s 525us/step - loss: 0.1615 - mse: 0.1615 - val_loss: 0.1006 - val_mse: 0.1006\n",
      "Epoch 4/50\n",
      "63/63 [==============================] - 0s 509us/step - loss: 0.0718 - mse: 0.0718 - val_loss: 0.0484 - val_mse: 0.0484\n",
      "Epoch 5/50\n",
      "63/63 [==============================] - 0s 510us/step - loss: 0.0353 - mse: 0.0353 - val_loss: 0.0263 - val_mse: 0.0263\n",
      "Epoch 6/50\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0203 - mse: 0.0203 - val_loss: 0.0169 - val_mse: 0.0169\n",
      "Epoch 7/50\n",
      "63/63 [==============================] - 0s 532us/step - loss: 0.0140 - mse: 0.0140 - val_loss: 0.0128 - val_mse: 0.0128\n",
      "Epoch 8/50\n",
      "63/63 [==============================] - 0s 525us/step - loss: 0.0114 - mse: 0.0114 - val_loss: 0.0110 - val_mse: 0.0110\n",
      "Epoch 9/50\n",
      "63/63 [==============================] - 0s 522us/step - loss: 0.0104 - mse: 0.0104 - val_loss: 0.0102 - val_mse: 0.0102\n",
      "Epoch 10/50\n",
      "63/63 [==============================] - 0s 526us/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.0099 - val_mse: 0.0099\n",
      "Epoch 11/50\n",
      "63/63 [==============================] - 0s 525us/step - loss: 0.0097 - mse: 0.0097 - val_loss: 0.0097 - val_mse: 0.0097\n",
      "Epoch 12/50\n",
      "63/63 [==============================] - 0s 529us/step - loss: 0.0097 - mse: 0.0097 - val_loss: 0.0096 - val_mse: 0.0096\n",
      "Epoch 13/50\n",
      "63/63 [==============================] - 0s 533us/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0096 - val_mse: 0.0096\n",
      "Epoch 14/50\n",
      "63/63 [==============================] - 0s 526us/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0095 - val_mse: 0.0095\n",
      "Epoch 15/50\n",
      "63/63 [==============================] - 0s 524us/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0095 - val_mse: 0.0095\n",
      "Epoch 16/50\n",
      "63/63 [==============================] - 0s 535us/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0095 - val_mse: 0.0095\n",
      "Epoch 17/50\n",
      "63/63 [==============================] - 0s 524us/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0095 - val_mse: 0.0095\n",
      "Epoch 18/50\n",
      "63/63 [==============================] - 0s 513us/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0095 - val_mse: 0.0095\n",
      "Epoch 19/50\n",
      "63/63 [==============================] - 0s 531us/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0095 - val_mse: 0.0095\n",
      "Epoch 20/50\n",
      "63/63 [==============================] - 0s 518us/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0095 - val_mse: 0.0095\n",
      "Epoch 21/50\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0095 - val_mse: 0.0095\n",
      "Epoch 22/50\n",
      "63/63 [==============================] - 0s 526us/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0095 - val_mse: 0.0095\n",
      "Epoch 23/50\n",
      "63/63 [==============================] - 0s 517us/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0095 - val_mse: 0.0095\n",
      "Epoch 24/50\n",
      "63/63 [==============================] - 0s 520us/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0095 - val_mse: 0.0095\n",
      "Epoch 25/50\n",
      "63/63 [==============================] - 0s 531us/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0095 - val_mse: 0.0095\n",
      "Epoch 26/50\n",
      "63/63 [==============================] - 0s 516us/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0095 - val_mse: 0.0095\n",
      "Epoch 27/50\n",
      "63/63 [==============================] - 0s 521us/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0095 - val_mse: 0.0095\n",
      "Epoch 28/50\n",
      "63/63 [==============================] - 0s 520us/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0095 - val_mse: 0.0095\n",
      "Epoch 29/50\n",
      "63/63 [==============================] - 0s 527us/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0095 - val_mse: 0.0095\n",
      "Epoch 30/50\n",
      "63/63 [==============================] - 0s 524us/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0095 - val_mse: 0.0095\n",
      "Epoch 31/50\n",
      "63/63 [==============================] - 0s 492us/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0095 - val_mse: 0.0095\n",
      "Epoch 32/50\n",
      "63/63 [==============================] - 0s 507us/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0095 - val_mse: 0.0095\n",
      "Epoch 33/50\n",
      "63/63 [==============================] - 0s 531us/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0095 - val_mse: 0.0095\n",
      "Epoch 34/50\n",
      "63/63 [==============================] - 0s 513us/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0095 - val_mse: 0.0095\n",
      "Epoch 35/50\n",
      "63/63 [==============================] - 0s 524us/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0095 - val_mse: 0.0095\n",
      "Epoch 36/50\n",
      "63/63 [==============================] - 0s 515us/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0095 - val_mse: 0.0095\n",
      "Epoch 37/50\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0095 - val_mse: 0.0095\n",
      "Epoch 38/50\n",
      "63/63 [==============================] - 0s 524us/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0095 - val_mse: 0.0095\n",
      "Epoch 39/50\n",
      "63/63 [==============================] - 0s 524us/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0095 - val_mse: 0.0095\n",
      "Epoch 40/50\n",
      "63/63 [==============================] - 0s 536us/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0095 - val_mse: 0.0095\n",
      "Epoch 41/50\n",
      "63/63 [==============================] - 0s 534us/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0095 - val_mse: 0.0095\n",
      "Epoch 42/50\n",
      "63/63 [==============================] - 0s 525us/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0095 - val_mse: 0.0095\n",
      "Epoch 43/50\n",
      "63/63 [==============================] - 0s 533us/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0095 - val_mse: 0.0095\n",
      "Epoch 44/50\n",
      "63/63 [==============================] - 0s 519us/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0095 - val_mse: 0.0095\n",
      "Epoch 45/50\n",
      "63/63 [==============================] - 0s 577us/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0095 - val_mse: 0.0095\n",
      "Epoch 46/50\n",
      "63/63 [==============================] - 0s 525us/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0095 - val_mse: 0.0095\n",
      "Epoch 47/50\n",
      "63/63 [==============================] - 0s 510us/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0095 - val_mse: 0.0095\n",
      "Epoch 48/50\n",
      "63/63 [==============================] - 0s 513us/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0095 - val_mse: 0.0095\n",
      "Epoch 49/50\n",
      "63/63 [==============================] - 0s 533us/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0095 - val_mse: 0.0095\n",
      "Epoch 50/50\n",
      "63/63 [==============================] - 0s 514us/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0095 - val_mse: 0.0095\n",
      "Epoch 1/10\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.4462 - mse: 4.4462 - val_loss: 3.4947 - val_mse: 3.4947\n",
      "Epoch 2/10\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 3.3938 - mse: 3.3938 - val_loss: 2.6984 - val_mse: 2.6984\n",
      "Epoch 3/10\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 2.6327 - mse: 2.6327 - val_loss: 2.1195 - val_mse: 2.1195\n",
      "Epoch 4/10\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2.0735 - mse: 2.0735 - val_loss: 1.6799 - val_mse: 1.6799\n",
      "Epoch 5/10\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.6472 - mse: 1.6472 - val_loss: 1.3440 - val_mse: 1.3440\n",
      "Epoch 6/10\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.3188 - mse: 1.3188 - val_loss: 1.0784 - val_mse: 1.0784\n",
      "Epoch 7/10\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0601 - mse: 1.0601 - val_loss: 0.8674 - val_mse: 0.8674\n",
      "Epoch 8/10\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.8540 - mse: 0.8540 - val_loss: 0.7031 - val_mse: 0.7031\n",
      "Epoch 9/10\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.6934 - mse: 0.6934 - val_loss: 0.5710 - val_mse: 0.5710\n",
      "Epoch 10/10\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.5640 - mse: 0.5640 - val_loss: 0.4653 - val_mse: 0.4653\n",
      "Epoch 1/10\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.2245 - mse: 1.2245 - val_loss: 0.8020 - val_mse: 0.8020\n",
      "Epoch 2/10\n",
      "32/32 [==============================] - 0s 801us/step - loss: 0.5946 - mse: 0.5946 - val_loss: 0.4491 - val_mse: 0.4491\n",
      "Epoch 3/10\n",
      "32/32 [==============================] - 0s 784us/step - loss: 0.3384 - mse: 0.3384 - val_loss: 0.2891 - val_mse: 0.2891\n",
      "Epoch 4/10\n",
      "32/32 [==============================] - 0s 765us/step - loss: 0.2176 - mse: 0.2176 - val_loss: 0.1989 - val_mse: 0.1989\n",
      "Epoch 5/10\n",
      "32/32 [==============================] - 0s 725us/step - loss: 0.1493 - mse: 0.1493 - val_loss: 0.1446 - val_mse: 0.1446\n",
      "Epoch 6/10\n",
      "32/32 [==============================] - 0s 759us/step - loss: 0.1090 - mse: 0.1090 - val_loss: 0.1091 - val_mse: 0.1091\n",
      "Epoch 7/10\n",
      "32/32 [==============================] - 0s 755us/step - loss: 0.0836 - mse: 0.0836 - val_loss: 0.0855 - val_mse: 0.0855\n",
      "Epoch 8/10\n",
      "32/32 [==============================] - 0s 768us/step - loss: 0.0674 - mse: 0.0674 - val_loss: 0.0700 - val_mse: 0.0700\n",
      "Epoch 9/10\n",
      "32/32 [==============================] - 0s 761us/step - loss: 0.0571 - mse: 0.0571 - val_loss: 0.0591 - val_mse: 0.0591\n",
      "Epoch 10/10\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.0502 - mse: 0.0502 - val_loss: 0.0515 - val_mse: 0.0515\n",
      "Epoch 1/10\n",
      "47/47 [==============================] - 0s 2ms/step - loss: 1.6896 - mse: 1.6896 - val_loss: 1.1651 - val_mse: 1.1651\n",
      "Epoch 2/10\n",
      "47/47 [==============================] - 0s 621us/step - loss: 0.7266 - mse: 0.7266 - val_loss: 0.5269 - val_mse: 0.5269\n",
      "Epoch 3/10\n",
      "47/47 [==============================] - 0s 589us/step - loss: 0.3696 - mse: 0.3696 - val_loss: 0.2741 - val_mse: 0.2741\n",
      "Epoch 4/10\n",
      "47/47 [==============================] - 0s 574us/step - loss: 0.2064 - mse: 0.2064 - val_loss: 0.1586 - val_mse: 0.1586\n",
      "Epoch 5/10\n",
      "47/47 [==============================] - 0s 575us/step - loss: 0.1249 - mse: 0.1249 - val_loss: 0.1016 - val_mse: 0.1016\n",
      "Epoch 6/10\n",
      "47/47 [==============================] - 0s 592us/step - loss: 0.0830 - mse: 0.0830 - val_loss: 0.0722 - val_mse: 0.0722\n",
      "Epoch 7/10\n",
      "47/47 [==============================] - 0s 579us/step - loss: 0.0613 - mse: 0.0613 - val_loss: 0.0570 - val_mse: 0.0570\n",
      "Epoch 8/10\n",
      "47/47 [==============================] - 0s 575us/step - loss: 0.0500 - mse: 0.0500 - val_loss: 0.0487 - val_mse: 0.0487\n",
      "Epoch 9/10\n",
      "47/47 [==============================] - 0s 579us/step - loss: 0.0441 - mse: 0.0441 - val_loss: 0.0443 - val_mse: 0.0443\n",
      "Epoch 10/10\n",
      "47/47 [==============================] - 0s 586us/step - loss: 0.0411 - mse: 0.0411 - val_loss: 0.0419 - val_mse: 0.0419\n",
      "Epoch 1/10\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.5687 - mse: 1.5687 - val_loss: 0.7393 - val_mse: 0.7393\n",
      "Epoch 2/10\n",
      "63/63 [==============================] - 0s 546us/step - loss: 0.5427 - mse: 0.5427 - val_loss: 0.3257 - val_mse: 0.3257\n",
      "Epoch 3/10\n",
      "63/63 [==============================] - 0s 530us/step - loss: 0.2371 - mse: 0.2371 - val_loss: 0.1618 - val_mse: 0.1618\n",
      "Epoch 4/10\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.1201 - mse: 0.1201 - val_loss: 0.0920 - val_mse: 0.0920\n",
      "Epoch 5/10\n",
      "63/63 [==============================] - 0s 531us/step - loss: 0.0723 - mse: 0.0723 - val_loss: 0.0621 - val_mse: 0.0621\n",
      "Epoch 6/10\n",
      "63/63 [==============================] - 0s 525us/step - loss: 0.0525 - mse: 0.0525 - val_loss: 0.0488 - val_mse: 0.0488\n",
      "Epoch 7/10\n",
      "63/63 [==============================] - 0s 522us/step - loss: 0.0443 - mse: 0.0443 - val_loss: 0.0431 - val_mse: 0.0431\n",
      "Epoch 8/10\n",
      "63/63 [==============================] - 0s 527us/step - loss: 0.0409 - mse: 0.0409 - val_loss: 0.0405 - val_mse: 0.0405\n",
      "Epoch 9/10\n",
      "63/63 [==============================] - 0s 516us/step - loss: 0.0394 - mse: 0.0394 - val_loss: 0.0393 - val_mse: 0.0393\n",
      "Epoch 10/10\n",
      "63/63 [==============================] - 0s 508us/step - loss: 0.0388 - mse: 0.0388 - val_loss: 0.0387 - val_mse: 0.0387\n",
      "Epoch 1/15\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.8609 - mse: 0.8609 - val_loss: 0.7164 - val_mse: 0.7164\n",
      "Epoch 2/15\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.4978 - mse: 0.4978 - val_loss: 0.4394 - val_mse: 0.4394\n",
      "Epoch 3/15\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3026 - mse: 0.3026 - val_loss: 0.2847 - val_mse: 0.2847\n",
      "Epoch 4/15\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.1962 - mse: 0.1962 - val_loss: 0.1958 - val_mse: 0.1958\n",
      "Epoch 5/15\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.1368 - mse: 0.1368 - val_loss: 0.1432 - val_mse: 0.1432\n",
      "Epoch 6/15\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.1028 - mse: 0.1028 - val_loss: 0.1115 - val_mse: 0.1115\n",
      "Epoch 7/15\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0827 - mse: 0.0827 - val_loss: 0.0914 - val_mse: 0.0914\n",
      "Epoch 8/15\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0702 - mse: 0.0702 - val_loss: 0.0777 - val_mse: 0.0777\n",
      "Epoch 9/15\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0620 - mse: 0.0620 - val_loss: 0.0687 - val_mse: 0.0687\n",
      "Epoch 10/15\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0564 - mse: 0.0564 - val_loss: 0.0623 - val_mse: 0.0623\n",
      "Epoch 11/15\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0524 - mse: 0.0524 - val_loss: 0.0576 - val_mse: 0.0576\n",
      "Epoch 12/15\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.0495 - mse: 0.0495 - val_loss: 0.0543 - val_mse: 0.0543\n",
      "Epoch 13/15\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0473 - mse: 0.0473 - val_loss: 0.0518 - val_mse: 0.0518\n",
      "Epoch 14/15\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0456 - mse: 0.0456 - val_loss: 0.0499 - val_mse: 0.0499\n",
      "Epoch 15/15\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0443 - mse: 0.0443 - val_loss: 0.0486 - val_mse: 0.0486\n",
      "Epoch 1/15\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 4.1267 - mse: 4.1267 - val_loss: 3.2676 - val_mse: 3.2676\n",
      "Epoch 2/15\n",
      "32/32 [==============================] - 0s 810us/step - loss: 2.4859 - mse: 2.4859 - val_loss: 2.0810 - val_mse: 2.0810\n",
      "Epoch 3/15\n",
      "32/32 [==============================] - 0s 758us/step - loss: 1.5725 - mse: 1.5725 - val_loss: 1.3648 - val_mse: 1.3648\n",
      "Epoch 4/15\n",
      "32/32 [==============================] - 0s 750us/step - loss: 1.0205 - mse: 1.0205 - val_loss: 0.9073 - val_mse: 0.9073\n",
      "Epoch 5/15\n",
      "32/32 [==============================] - 0s 750us/step - loss: 0.6710 - mse: 0.6710 - val_loss: 0.6106 - val_mse: 0.6106\n",
      "Epoch 6/15\n",
      "32/32 [==============================] - 0s 744us/step - loss: 0.4471 - mse: 0.4471 - val_loss: 0.4152 - val_mse: 0.4152\n",
      "Epoch 7/15\n",
      "32/32 [==============================] - 0s 752us/step - loss: 0.3021 - mse: 0.3021 - val_loss: 0.2871 - val_mse: 0.2871\n",
      "Epoch 8/15\n",
      "32/32 [==============================] - 0s 742us/step - loss: 0.2086 - mse: 0.2086 - val_loss: 0.2037 - val_mse: 0.2037\n",
      "Epoch 9/15\n",
      "32/32 [==============================] - 0s 748us/step - loss: 0.1481 - mse: 0.1481 - val_loss: 0.1478 - val_mse: 0.1478\n",
      "Epoch 10/15\n",
      "32/32 [==============================] - 0s 771us/step - loss: 0.1090 - mse: 0.1090 - val_loss: 0.1112 - val_mse: 0.1112\n",
      "Epoch 11/15\n",
      "32/32 [==============================] - 0s 768us/step - loss: 0.0840 - mse: 0.0840 - val_loss: 0.0866 - val_mse: 0.0866\n",
      "Epoch 12/15\n",
      "32/32 [==============================] - 0s 747us/step - loss: 0.0674 - mse: 0.0674 - val_loss: 0.0705 - val_mse: 0.0705\n",
      "Epoch 13/15\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0571 - mse: 0.0571 - val_loss: 0.0590 - val_mse: 0.0590\n",
      "Epoch 14/15\n",
      "32/32 [==============================] - 0s 739us/step - loss: 0.0502 - mse: 0.0502 - val_loss: 0.0514 - val_mse: 0.0514\n",
      "Epoch 15/15\n",
      "32/32 [==============================] - 0s 737us/step - loss: 0.0458 - mse: 0.0458 - val_loss: 0.0465 - val_mse: 0.0465\n",
      "Epoch 1/15\n",
      "47/47 [==============================] - 0s 2ms/step - loss: 2.4931 - mse: 2.4931 - val_loss: 1.7513 - val_mse: 1.7513\n",
      "Epoch 2/15\n",
      "47/47 [==============================] - 0s 648us/step - loss: 1.1412 - mse: 1.1412 - val_loss: 0.8137 - val_mse: 0.8137\n",
      "Epoch 3/15\n",
      "47/47 [==============================] - 0s 587us/step - loss: 0.5856 - mse: 0.5856 - val_loss: 0.4229 - val_mse: 0.4229\n",
      "Epoch 4/15\n",
      "47/47 [==============================] - 0s 589us/step - loss: 0.3188 - mse: 0.3188 - val_loss: 0.2343 - val_mse: 0.2343\n",
      "Epoch 5/15\n",
      "47/47 [==============================] - 0s 610us/step - loss: 0.1831 - mse: 0.1831 - val_loss: 0.1413 - val_mse: 0.1413\n",
      "Epoch 6/15\n",
      "47/47 [==============================] - 0s 606us/step - loss: 0.1132 - mse: 0.1132 - val_loss: 0.0932 - val_mse: 0.0932\n",
      "Epoch 7/15\n",
      "47/47 [==============================] - 0s 594us/step - loss: 0.0770 - mse: 0.0770 - val_loss: 0.0681 - val_mse: 0.0681\n",
      "Epoch 8/15\n",
      "47/47 [==============================] - 0s 586us/step - loss: 0.0582 - mse: 0.0582 - val_loss: 0.0547 - val_mse: 0.0547\n",
      "Epoch 9/15\n",
      "47/47 [==============================] - 0s 611us/step - loss: 0.0484 - mse: 0.0484 - val_loss: 0.0476 - val_mse: 0.0476\n",
      "Epoch 10/15\n",
      "47/47 [==============================] - 0s 599us/step - loss: 0.0433 - mse: 0.0433 - val_loss: 0.0437 - val_mse: 0.0437\n",
      "Epoch 11/15\n",
      "47/47 [==============================] - 0s 596us/step - loss: 0.0407 - mse: 0.0407 - val_loss: 0.0416 - val_mse: 0.0416\n",
      "Epoch 12/15\n",
      "47/47 [==============================] - 0s 2ms/step - loss: 0.0393 - mse: 0.0393 - val_loss: 0.0403 - val_mse: 0.0403\n",
      "Epoch 13/15\n",
      "47/47 [==============================] - 0s 596us/step - loss: 0.0386 - mse: 0.0386 - val_loss: 0.0396 - val_mse: 0.0396\n",
      "Epoch 14/15\n",
      "47/47 [==============================] - 0s 624us/step - loss: 0.0382 - mse: 0.0382 - val_loss: 0.0393 - val_mse: 0.0393\n",
      "Epoch 15/15\n",
      "47/47 [==============================] - 0s 602us/step - loss: 0.0380 - mse: 0.0380 - val_loss: 0.0391 - val_mse: 0.0391\n",
      "Epoch 1/15\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.5781 - mse: 0.5781 - val_loss: 0.1923 - val_mse: 0.1923\n",
      "Epoch 2/15\n",
      "63/63 [==============================] - 0s 543us/step - loss: 0.1360 - mse: 0.1360 - val_loss: 0.0849 - val_mse: 0.0849\n",
      "Epoch 3/15\n",
      "63/63 [==============================] - 0s 528us/step - loss: 0.0691 - mse: 0.0691 - val_loss: 0.0579 - val_mse: 0.0579\n",
      "Epoch 4/15\n",
      "63/63 [==============================] - 0s 531us/step - loss: 0.0504 - mse: 0.0504 - val_loss: 0.0471 - val_mse: 0.0471\n",
      "Epoch 5/15\n",
      "63/63 [==============================] - 0s 534us/step - loss: 0.0433 - mse: 0.0433 - val_loss: 0.0423 - val_mse: 0.0423\n",
      "Epoch 6/15\n",
      "63/63 [==============================] - 0s 532us/step - loss: 0.0404 - mse: 0.0404 - val_loss: 0.0402 - val_mse: 0.0402\n",
      "Epoch 7/15\n",
      "63/63 [==============================] - 0s 526us/step - loss: 0.0393 - mse: 0.0393 - val_loss: 0.0391 - val_mse: 0.0391\n",
      "Epoch 8/15\n",
      "63/63 [==============================] - 0s 514us/step - loss: 0.0387 - mse: 0.0387 - val_loss: 0.0386 - val_mse: 0.0386\n",
      "Epoch 9/15\n",
      "63/63 [==============================] - 0s 527us/step - loss: 0.0385 - mse: 0.0385 - val_loss: 0.0383 - val_mse: 0.0383\n",
      "Epoch 10/15\n",
      "63/63 [==============================] - 0s 525us/step - loss: 0.0385 - mse: 0.0385 - val_loss: 0.0382 - val_mse: 0.0382\n",
      "Epoch 11/15\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0384 - mse: 0.0384 - val_loss: 0.0382 - val_mse: 0.0382\n",
      "Epoch 12/15\n",
      "63/63 [==============================] - 0s 518us/step - loss: 0.0384 - mse: 0.0384 - val_loss: 0.0381 - val_mse: 0.0381\n",
      "Epoch 13/15\n",
      "63/63 [==============================] - 0s 534us/step - loss: 0.0384 - mse: 0.0384 - val_loss: 0.0381 - val_mse: 0.0381\n",
      "Epoch 14/15\n",
      "63/63 [==============================] - 0s 519us/step - loss: 0.0384 - mse: 0.0384 - val_loss: 0.0380 - val_mse: 0.0380\n",
      "Epoch 15/15\n",
      "63/63 [==============================] - 0s 505us/step - loss: 0.0384 - mse: 0.0384 - val_loss: 0.0380 - val_mse: 0.0380\n",
      "Epoch 1/30\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.0918 - mse: 4.0918 - val_loss: 3.2150 - val_mse: 3.2150\n",
      "Epoch 2/30\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 3.1105 - mse: 3.1105 - val_loss: 2.4755 - val_mse: 2.4755\n",
      "Epoch 3/30\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 2.4070 - mse: 2.4070 - val_loss: 1.9341 - val_mse: 1.9341\n",
      "Epoch 4/30\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.8885 - mse: 1.8885 - val_loss: 1.5311 - val_mse: 1.5311\n",
      "Epoch 5/30\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.4997 - mse: 1.4997 - val_loss: 1.2217 - val_mse: 1.2217\n",
      "Epoch 6/30\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1984 - mse: 1.1984 - val_loss: 0.9810 - val_mse: 0.9810\n",
      "Epoch 7/30\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.9633 - mse: 0.9633 - val_loss: 0.7918 - val_mse: 0.7918\n",
      "Epoch 8/30\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.7791 - mse: 0.7791 - val_loss: 0.6416 - val_mse: 0.6416\n",
      "Epoch 9/30\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.6321 - mse: 0.6321 - val_loss: 0.5218 - val_mse: 0.5218\n",
      "Epoch 10/30\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.5152 - mse: 0.5152 - val_loss: 0.4258 - val_mse: 0.4258\n",
      "Epoch 11/30\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.4210 - mse: 0.4210 - val_loss: 0.3491 - val_mse: 0.3491\n",
      "Epoch 12/30\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.3460 - mse: 0.3460 - val_loss: 0.2883 - val_mse: 0.2883\n",
      "Epoch 13/30\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.2860 - mse: 0.2860 - val_loss: 0.2383 - val_mse: 0.2383\n",
      "Epoch 14/30\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.2368 - mse: 0.2368 - val_loss: 0.1991 - val_mse: 0.1991\n",
      "Epoch 15/30\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.1979 - mse: 0.1979 - val_loss: 0.1679 - val_mse: 0.1679\n",
      "Epoch 16/30\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.1669 - mse: 0.1669 - val_loss: 0.1428 - val_mse: 0.1428\n",
      "Epoch 17/30\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.1419 - mse: 0.1419 - val_loss: 0.1228 - val_mse: 0.1228\n",
      "Epoch 18/30\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.1215 - mse: 0.1215 - val_loss: 0.1066 - val_mse: 0.1066\n",
      "Epoch 19/30\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.1053 - mse: 0.1053 - val_loss: 0.0938 - val_mse: 0.0938\n",
      "Epoch 20/30\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0923 - mse: 0.0923 - val_loss: 0.0834 - val_mse: 0.0834\n",
      "Epoch 21/30\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0818 - mse: 0.0818 - val_loss: 0.0752 - val_mse: 0.0752\n",
      "Epoch 22/30\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0734 - mse: 0.0734 - val_loss: 0.0687 - val_mse: 0.0687\n",
      "Epoch 23/30\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0667 - mse: 0.0667 - val_loss: 0.0635 - val_mse: 0.0635\n",
      "Epoch 24/30\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0612 - mse: 0.0612 - val_loss: 0.0595 - val_mse: 0.0595\n",
      "Epoch 25/30\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0569 - mse: 0.0569 - val_loss: 0.0561 - val_mse: 0.0561\n",
      "Epoch 26/30\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0534 - mse: 0.0534 - val_loss: 0.0537 - val_mse: 0.0537\n",
      "Epoch 27/30\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0506 - mse: 0.0506 - val_loss: 0.0516 - val_mse: 0.0516\n",
      "Epoch 28/30\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0484 - mse: 0.0484 - val_loss: 0.0500 - val_mse: 0.0500\n",
      "Epoch 29/30\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0466 - mse: 0.0466 - val_loss: 0.0488 - val_mse: 0.0488\n",
      "Epoch 30/30\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0451 - mse: 0.0451 - val_loss: 0.0479 - val_mse: 0.0479\n",
      "Epoch 1/30\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.0936 - mse: 1.0936 - val_loss: 0.6932 - val_mse: 0.6932\n",
      "Epoch 2/30\n",
      "32/32 [==============================] - 0s 791us/step - loss: 0.5116 - mse: 0.5116 - val_loss: 0.3752 - val_mse: 0.3752\n",
      "Epoch 3/30\n",
      "32/32 [==============================] - 0s 759us/step - loss: 0.2836 - mse: 0.2836 - val_loss: 0.2365 - val_mse: 0.2365\n",
      "Epoch 4/30\n",
      "32/32 [==============================] - 0s 745us/step - loss: 0.1795 - mse: 0.1795 - val_loss: 0.1654 - val_mse: 0.1654\n",
      "Epoch 5/30\n",
      "32/32 [==============================] - 0s 735us/step - loss: 0.1257 - mse: 0.1257 - val_loss: 0.1216 - val_mse: 0.1216\n",
      "Epoch 6/30\n",
      "32/32 [==============================] - 0s 739us/step - loss: 0.0930 - mse: 0.0930 - val_loss: 0.0936 - val_mse: 0.0936\n",
      "Epoch 7/30\n",
      "32/32 [==============================] - 0s 739us/step - loss: 0.0733 - mse: 0.0733 - val_loss: 0.0749 - val_mse: 0.0749\n",
      "Epoch 8/30\n",
      "32/32 [==============================] - 0s 747us/step - loss: 0.0605 - mse: 0.0605 - val_loss: 0.0628 - val_mse: 0.0628\n",
      "Epoch 9/30\n",
      "32/32 [==============================] - 0s 755us/step - loss: 0.0526 - mse: 0.0526 - val_loss: 0.0543 - val_mse: 0.0543\n",
      "Epoch 10/30\n",
      "32/32 [==============================] - 0s 747us/step - loss: 0.0474 - mse: 0.0474 - val_loss: 0.0483 - val_mse: 0.0483\n",
      "Epoch 11/30\n",
      "32/32 [==============================] - 0s 737us/step - loss: 0.0440 - mse: 0.0440 - val_loss: 0.0442 - val_mse: 0.0442\n",
      "Epoch 12/30\n",
      "32/32 [==============================] - 0s 759us/step - loss: 0.0418 - mse: 0.0418 - val_loss: 0.0414 - val_mse: 0.0414\n",
      "Epoch 13/30\n",
      "32/32 [==============================] - 0s 762us/step - loss: 0.0404 - mse: 0.0404 - val_loss: 0.0393 - val_mse: 0.0393\n",
      "Epoch 14/30\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0395 - mse: 0.0395 - val_loss: 0.0380 - val_mse: 0.0380\n",
      "Epoch 15/30\n",
      "32/32 [==============================] - 0s 760us/step - loss: 0.0389 - mse: 0.0389 - val_loss: 0.0370 - val_mse: 0.0370\n",
      "Epoch 16/30\n",
      "32/32 [==============================] - 0s 743us/step - loss: 0.0385 - mse: 0.0385 - val_loss: 0.0361 - val_mse: 0.0361\n",
      "Epoch 17/30\n",
      "32/32 [==============================] - 0s 725us/step - loss: 0.0383 - mse: 0.0383 - val_loss: 0.0354 - val_mse: 0.0354\n",
      "Epoch 18/30\n",
      "32/32 [==============================] - 0s 757us/step - loss: 0.0381 - mse: 0.0381 - val_loss: 0.0349 - val_mse: 0.0349\n",
      "Epoch 19/30\n",
      "32/32 [==============================] - 0s 753us/step - loss: 0.0380 - mse: 0.0380 - val_loss: 0.0348 - val_mse: 0.0348\n",
      "Epoch 20/30\n",
      "32/32 [==============================] - 0s 746us/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0345 - val_mse: 0.0345\n",
      "Epoch 21/30\n",
      "32/32 [==============================] - 0s 741us/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0343 - val_mse: 0.0343\n",
      "Epoch 22/30\n",
      "32/32 [==============================] - 0s 769us/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0342 - val_mse: 0.0342\n",
      "Epoch 23/30\n",
      "32/32 [==============================] - 0s 768us/step - loss: 0.0378 - mse: 0.0378 - val_loss: 0.0340 - val_mse: 0.0340\n",
      "Epoch 24/30\n",
      "32/32 [==============================] - 0s 734us/step - loss: 0.0378 - mse: 0.0378 - val_loss: 0.0338 - val_mse: 0.0338\n",
      "Epoch 25/30\n",
      "32/32 [==============================] - 0s 748us/step - loss: 0.0378 - mse: 0.0378 - val_loss: 0.0340 - val_mse: 0.0340\n",
      "Epoch 26/30\n",
      "32/32 [==============================] - 0s 753us/step - loss: 0.0378 - mse: 0.0378 - val_loss: 0.0339 - val_mse: 0.0339\n",
      "Epoch 27/30\n",
      "32/32 [==============================] - 0s 741us/step - loss: 0.0378 - mse: 0.0378 - val_loss: 0.0337 - val_mse: 0.0337\n",
      "Epoch 28/30\n",
      "32/32 [==============================] - 0s 742us/step - loss: 0.0378 - mse: 0.0378 - val_loss: 0.0339 - val_mse: 0.0339\n",
      "Epoch 29/30\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0378 - mse: 0.0378 - val_loss: 0.0338 - val_mse: 0.0338\n",
      "Epoch 30/30\n",
      "32/32 [==============================] - 0s 809us/step - loss: 0.0378 - mse: 0.0378 - val_loss: 0.0337 - val_mse: 0.0337\n",
      "Epoch 1/30\n",
      "47/47 [==============================] - 0s 2ms/step - loss: 1.8726 - mse: 1.8726 - val_loss: 1.2964 - val_mse: 1.2964\n",
      "Epoch 2/30\n",
      "47/47 [==============================] - 0s 708us/step - loss: 0.8208 - mse: 0.8208 - val_loss: 0.5909 - val_mse: 0.5909\n",
      "Epoch 3/30\n",
      "47/47 [==============================] - 0s 622us/step - loss: 0.4188 - mse: 0.4188 - val_loss: 0.3065 - val_mse: 0.3065\n",
      "Epoch 4/30\n",
      "47/47 [==============================] - 0s 601us/step - loss: 0.2323 - mse: 0.2323 - val_loss: 0.1760 - val_mse: 0.1760\n",
      "Epoch 5/30\n",
      "47/47 [==============================] - 0s 604us/step - loss: 0.1385 - mse: 0.1385 - val_loss: 0.1109 - val_mse: 0.1109\n",
      "Epoch 6/30\n",
      "47/47 [==============================] - 0s 602us/step - loss: 0.0901 - mse: 0.0901 - val_loss: 0.0775 - val_mse: 0.0775\n",
      "Epoch 7/30\n",
      "47/47 [==============================] - 0s 609us/step - loss: 0.0650 - mse: 0.0650 - val_loss: 0.0596 - val_mse: 0.0596\n",
      "Epoch 8/30\n",
      "47/47 [==============================] - 0s 600us/step - loss: 0.0519 - mse: 0.0519 - val_loss: 0.0502 - val_mse: 0.0502\n",
      "Epoch 9/30\n",
      "47/47 [==============================] - 0s 604us/step - loss: 0.0452 - mse: 0.0452 - val_loss: 0.0451 - val_mse: 0.0451\n",
      "Epoch 10/30\n",
      "47/47 [==============================] - 0s 597us/step - loss: 0.0416 - mse: 0.0416 - val_loss: 0.0423 - val_mse: 0.0423\n",
      "Epoch 11/30\n",
      "47/47 [==============================] - 0s 619us/step - loss: 0.0398 - mse: 0.0398 - val_loss: 0.0408 - val_mse: 0.0408\n",
      "Epoch 12/30\n",
      "47/47 [==============================] - 0s 596us/step - loss: 0.0388 - mse: 0.0388 - val_loss: 0.0399 - val_mse: 0.0399\n",
      "Epoch 13/30\n",
      "47/47 [==============================] - 0s 2ms/step - loss: 0.0383 - mse: 0.0383 - val_loss: 0.0395 - val_mse: 0.0395\n",
      "Epoch 14/30\n",
      "47/47 [==============================] - 0s 588us/step - loss: 0.0381 - mse: 0.0381 - val_loss: 0.0391 - val_mse: 0.0391\n",
      "Epoch 15/30\n",
      "47/47 [==============================] - 0s 595us/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0389 - val_mse: 0.0389\n",
      "Epoch 16/30\n",
      "47/47 [==============================] - 0s 606us/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0389 - val_mse: 0.0389\n",
      "Epoch 17/30\n",
      "47/47 [==============================] - 0s 591us/step - loss: 0.0378 - mse: 0.0378 - val_loss: 0.0387 - val_mse: 0.0387\n",
      "Epoch 18/30\n",
      "47/47 [==============================] - 0s 601us/step - loss: 0.0378 - mse: 0.0378 - val_loss: 0.0387 - val_mse: 0.0387\n",
      "Epoch 19/30\n",
      "47/47 [==============================] - 0s 624us/step - loss: 0.0378 - mse: 0.0378 - val_loss: 0.0388 - val_mse: 0.0388\n",
      "Epoch 20/30\n",
      "47/47 [==============================] - 0s 603us/step - loss: 0.0378 - mse: 0.0378 - val_loss: 0.0387 - val_mse: 0.0387\n",
      "Epoch 21/30\n",
      "47/47 [==============================] - 0s 606us/step - loss: 0.0378 - mse: 0.0378 - val_loss: 0.0387 - val_mse: 0.0387\n",
      "Epoch 22/30\n",
      "47/47 [==============================] - 0s 613us/step - loss: 0.0378 - mse: 0.0378 - val_loss: 0.0387 - val_mse: 0.0387\n",
      "Epoch 23/30\n",
      "47/47 [==============================] - 0s 593us/step - loss: 0.0378 - mse: 0.0378 - val_loss: 0.0387 - val_mse: 0.0387\n",
      "Epoch 24/30\n",
      "47/47 [==============================] - 0s 594us/step - loss: 0.0378 - mse: 0.0378 - val_loss: 0.0386 - val_mse: 0.0386\n",
      "Epoch 25/30\n",
      "47/47 [==============================] - 0s 614us/step - loss: 0.0378 - mse: 0.0378 - val_loss: 0.0386 - val_mse: 0.0386\n",
      "Epoch 26/30\n",
      "47/47 [==============================] - 0s 614us/step - loss: 0.0378 - mse: 0.0378 - val_loss: 0.0386 - val_mse: 0.0386\n",
      "Epoch 27/30\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.0378 - mse: 0.0378 - val_loss: 0.0386 - val_mse: 0.0386\n",
      "Epoch 28/30\n",
      "47/47 [==============================] - 0s 615us/step - loss: 0.0378 - mse: 0.0378 - val_loss: 0.0385 - val_mse: 0.0385\n",
      "Epoch 29/30\n",
      "47/47 [==============================] - 0s 611us/step - loss: 0.0378 - mse: 0.0378 - val_loss: 0.0387 - val_mse: 0.0387\n",
      "Epoch 30/30\n",
      "47/47 [==============================] - 0s 593us/step - loss: 0.0378 - mse: 0.0378 - val_loss: 0.0386 - val_mse: 0.0386\n",
      "Epoch 1/30\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.5370 - mse: 0.5370 - val_loss: 0.1726 - val_mse: 0.1726\n",
      "Epoch 2/30\n",
      "63/63 [==============================] - 0s 536us/step - loss: 0.1192 - mse: 0.1192 - val_loss: 0.0746 - val_mse: 0.0746\n",
      "Epoch 3/30\n",
      "63/63 [==============================] - 0s 503us/step - loss: 0.0618 - mse: 0.0618 - val_loss: 0.0534 - val_mse: 0.0534\n",
      "Epoch 4/30\n",
      "63/63 [==============================] - 0s 510us/step - loss: 0.0474 - mse: 0.0474 - val_loss: 0.0452 - val_mse: 0.0452\n",
      "Epoch 5/30\n",
      "63/63 [==============================] - 0s 520us/step - loss: 0.0421 - mse: 0.0421 - val_loss: 0.0415 - val_mse: 0.0415\n",
      "Epoch 6/30\n",
      "63/63 [==============================] - 0s 488us/step - loss: 0.0400 - mse: 0.0400 - val_loss: 0.0398 - val_mse: 0.0398\n",
      "Epoch 7/30\n",
      "63/63 [==============================] - 0s 500us/step - loss: 0.0390 - mse: 0.0390 - val_loss: 0.0390 - val_mse: 0.0390\n",
      "Epoch 8/30\n",
      "63/63 [==============================] - 0s 513us/step - loss: 0.0387 - mse: 0.0387 - val_loss: 0.0385 - val_mse: 0.0385\n",
      "Epoch 9/30\n",
      "63/63 [==============================] - 0s 497us/step - loss: 0.0385 - mse: 0.0385 - val_loss: 0.0383 - val_mse: 0.0383\n",
      "Epoch 10/30\n",
      "63/63 [==============================] - 0s 498us/step - loss: 0.0385 - mse: 0.0385 - val_loss: 0.0382 - val_mse: 0.0382\n",
      "Epoch 11/30\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0384 - mse: 0.0384 - val_loss: 0.0381 - val_mse: 0.0381\n",
      "Epoch 12/30\n",
      "63/63 [==============================] - 0s 511us/step - loss: 0.0384 - mse: 0.0384 - val_loss: 0.0381 - val_mse: 0.0381\n",
      "Epoch 13/30\n",
      "63/63 [==============================] - 0s 505us/step - loss: 0.0384 - mse: 0.0384 - val_loss: 0.0381 - val_mse: 0.0381\n",
      "Epoch 14/30\n",
      "63/63 [==============================] - 0s 496us/step - loss: 0.0384 - mse: 0.0384 - val_loss: 0.0381 - val_mse: 0.0381\n",
      "Epoch 15/30\n",
      "63/63 [==============================] - 0s 504us/step - loss: 0.0384 - mse: 0.0384 - val_loss: 0.0380 - val_mse: 0.0380\n",
      "Epoch 16/30\n",
      "63/63 [==============================] - 0s 507us/step - loss: 0.0384 - mse: 0.0384 - val_loss: 0.0380 - val_mse: 0.0380\n",
      "Epoch 17/30\n",
      "63/63 [==============================] - 0s 495us/step - loss: 0.0384 - mse: 0.0384 - val_loss: 0.0380 - val_mse: 0.0380\n",
      "Epoch 18/30\n",
      "63/63 [==============================] - 0s 499us/step - loss: 0.0384 - mse: 0.0384 - val_loss: 0.0380 - val_mse: 0.0380\n",
      "Epoch 19/30\n",
      "63/63 [==============================] - 0s 511us/step - loss: 0.0384 - mse: 0.0384 - val_loss: 0.0380 - val_mse: 0.0380\n",
      "Epoch 20/30\n",
      "63/63 [==============================] - 0s 501us/step - loss: 0.0384 - mse: 0.0384 - val_loss: 0.0380 - val_mse: 0.0380\n",
      "Epoch 21/30\n",
      "63/63 [==============================] - 0s 507us/step - loss: 0.0384 - mse: 0.0384 - val_loss: 0.0380 - val_mse: 0.0380\n",
      "Epoch 22/30\n",
      "63/63 [==============================] - 0s 508us/step - loss: 0.0384 - mse: 0.0384 - val_loss: 0.0380 - val_mse: 0.0380\n",
      "Epoch 23/30\n",
      "63/63 [==============================] - 0s 499us/step - loss: 0.0384 - mse: 0.0384 - val_loss: 0.0380 - val_mse: 0.0380\n",
      "Epoch 24/30\n",
      "63/63 [==============================] - 0s 499us/step - loss: 0.0384 - mse: 0.0384 - val_loss: 0.0380 - val_mse: 0.0380\n",
      "Epoch 25/30\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0384 - mse: 0.0384 - val_loss: 0.0380 - val_mse: 0.0380\n",
      "Epoch 26/30\n",
      "63/63 [==============================] - 0s 494us/step - loss: 0.0384 - mse: 0.0384 - val_loss: 0.0380 - val_mse: 0.0380\n",
      "Epoch 27/30\n",
      "63/63 [==============================] - 0s 509us/step - loss: 0.0384 - mse: 0.0384 - val_loss: 0.0380 - val_mse: 0.0380\n",
      "Epoch 28/30\n",
      "63/63 [==============================] - 0s 511us/step - loss: 0.0384 - mse: 0.0384 - val_loss: 0.0380 - val_mse: 0.0380\n",
      "Epoch 29/30\n",
      "63/63 [==============================] - 0s 488us/step - loss: 0.0384 - mse: 0.0384 - val_loss: 0.0380 - val_mse: 0.0380\n",
      "Epoch 30/30\n",
      "63/63 [==============================] - 0s 499us/step - loss: 0.0384 - mse: 0.0384 - val_loss: 0.0380 - val_mse: 0.0380\n",
      "Epoch 1/50\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2.4095 - mse: 2.4095 - val_loss: 1.8858 - val_mse: 1.8858\n",
      "Epoch 2/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.7541 - mse: 1.7541 - val_loss: 1.4040 - val_mse: 1.4040\n",
      "Epoch 3/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.3223 - mse: 1.3223 - val_loss: 1.0709 - val_mse: 1.0709\n",
      "Epoch 4/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0174 - mse: 1.0174 - val_loss: 0.8330 - val_mse: 0.8330\n",
      "Epoch 5/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.7975 - mse: 0.7975 - val_loss: 0.6580 - val_mse: 0.6580\n",
      "Epoch 6/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.6335 - mse: 0.6335 - val_loss: 0.5270 - val_mse: 0.5270\n",
      "Epoch 7/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.5097 - mse: 0.5097 - val_loss: 0.4259 - val_mse: 0.4259\n",
      "Epoch 8/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.4138 - mse: 0.4138 - val_loss: 0.3467 - val_mse: 0.3467\n",
      "Epoch 9/50\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.3382 - mse: 0.3382 - val_loss: 0.2851 - val_mse: 0.2851\n",
      "Epoch 10/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.2787 - mse: 0.2787 - val_loss: 0.2361 - val_mse: 0.2361\n",
      "Epoch 11/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.2314 - mse: 0.2314 - val_loss: 0.1971 - val_mse: 0.1971\n",
      "Epoch 12/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.1935 - mse: 0.1935 - val_loss: 0.1655 - val_mse: 0.1655\n",
      "Epoch 13/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.1627 - mse: 0.1627 - val_loss: 0.1409 - val_mse: 0.1409\n",
      "Epoch 14/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.1386 - mse: 0.1386 - val_loss: 0.1213 - val_mse: 0.1213\n",
      "Epoch 15/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.1192 - mse: 0.1192 - val_loss: 0.1055 - val_mse: 0.1055\n",
      "Epoch 16/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.1035 - mse: 0.1035 - val_loss: 0.0927 - val_mse: 0.0927\n",
      "Epoch 17/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0907 - mse: 0.0907 - val_loss: 0.0827 - val_mse: 0.0827\n",
      "Epoch 18/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0806 - mse: 0.0806 - val_loss: 0.0745 - val_mse: 0.0745\n",
      "Epoch 19/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0724 - mse: 0.0724 - val_loss: 0.0682 - val_mse: 0.0682\n",
      "Epoch 20/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0659 - mse: 0.0659 - val_loss: 0.0633 - val_mse: 0.0633\n",
      "Epoch 21/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0607 - mse: 0.0607 - val_loss: 0.0592 - val_mse: 0.0592\n",
      "Epoch 22/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0565 - mse: 0.0565 - val_loss: 0.0563 - val_mse: 0.0563\n",
      "Epoch 23/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0531 - mse: 0.0531 - val_loss: 0.0536 - val_mse: 0.0536\n",
      "Epoch 24/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0504 - mse: 0.0504 - val_loss: 0.0516 - val_mse: 0.0516\n",
      "Epoch 25/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0482 - mse: 0.0482 - val_loss: 0.0498 - val_mse: 0.0498\n",
      "Epoch 26/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0464 - mse: 0.0464 - val_loss: 0.0486 - val_mse: 0.0486\n",
      "Epoch 27/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0450 - mse: 0.0450 - val_loss: 0.0474 - val_mse: 0.0474\n",
      "Epoch 28/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0438 - mse: 0.0438 - val_loss: 0.0468 - val_mse: 0.0468\n",
      "Epoch 29/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0429 - mse: 0.0429 - val_loss: 0.0462 - val_mse: 0.0462\n",
      "Epoch 30/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0421 - mse: 0.0421 - val_loss: 0.0458 - val_mse: 0.0458\n",
      "Epoch 31/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0415 - mse: 0.0415 - val_loss: 0.0454 - val_mse: 0.0454\n",
      "Epoch 32/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0410 - mse: 0.0410 - val_loss: 0.0453 - val_mse: 0.0453\n",
      "Epoch 33/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0407 - mse: 0.0407 - val_loss: 0.0450 - val_mse: 0.0450\n",
      "Epoch 34/50\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.0404 - mse: 0.0404 - val_loss: 0.0448 - val_mse: 0.0448\n",
      "Epoch 35/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0401 - mse: 0.0401 - val_loss: 0.0447 - val_mse: 0.0447\n",
      "Epoch 36/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0399 - mse: 0.0399 - val_loss: 0.0446 - val_mse: 0.0446\n",
      "Epoch 37/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0398 - mse: 0.0398 - val_loss: 0.0445 - val_mse: 0.0445\n",
      "Epoch 38/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0396 - mse: 0.0396 - val_loss: 0.0445 - val_mse: 0.0445\n",
      "Epoch 39/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0396 - mse: 0.0396 - val_loss: 0.0444 - val_mse: 0.0444\n",
      "Epoch 40/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0395 - mse: 0.0395 - val_loss: 0.0445 - val_mse: 0.0445\n",
      "Epoch 41/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0394 - mse: 0.0394 - val_loss: 0.0446 - val_mse: 0.0446\n",
      "Epoch 42/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0393 - mse: 0.0393 - val_loss: 0.0447 - val_mse: 0.0447\n",
      "Epoch 43/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0393 - mse: 0.0393 - val_loss: 0.0445 - val_mse: 0.0445\n",
      "Epoch 44/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0393 - mse: 0.0393 - val_loss: 0.0446 - val_mse: 0.0446\n",
      "Epoch 45/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0393 - mse: 0.0393 - val_loss: 0.0446 - val_mse: 0.0446\n",
      "Epoch 46/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0392 - mse: 0.0392 - val_loss: 0.0446 - val_mse: 0.0446\n",
      "Epoch 47/50\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.0392 - mse: 0.0392 - val_loss: 0.0446 - val_mse: 0.0446\n",
      "Epoch 48/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0392 - mse: 0.0392 - val_loss: 0.0445 - val_mse: 0.0445\n",
      "Epoch 49/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0392 - mse: 0.0392 - val_loss: 0.0444 - val_mse: 0.0444\n",
      "Epoch 50/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0392 - mse: 0.0392 - val_loss: 0.0446 - val_mse: 0.0446\n",
      "Epoch 1/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2.7216 - mse: 2.7216 - val_loss: 2.0906 - val_mse: 2.0906\n",
      "Epoch 2/50\n",
      "32/32 [==============================] - 0s 791us/step - loss: 1.5804 - mse: 1.5804 - val_loss: 1.2921 - val_mse: 1.2921\n",
      "Epoch 3/50\n",
      "32/32 [==============================] - 0s 789us/step - loss: 0.9731 - mse: 0.9731 - val_loss: 0.8427 - val_mse: 0.8427\n",
      "Epoch 4/50\n",
      "32/32 [==============================] - 0s 745us/step - loss: 0.6279 - mse: 0.6279 - val_loss: 0.5648 - val_mse: 0.5648\n",
      "Epoch 5/50\n",
      "32/32 [==============================] - 0s 755us/step - loss: 0.4158 - mse: 0.4158 - val_loss: 0.3870 - val_mse: 0.3870\n",
      "Epoch 6/50\n",
      "32/32 [==============================] - 0s 749us/step - loss: 0.2824 - mse: 0.2824 - val_loss: 0.2704 - val_mse: 0.2704\n",
      "Epoch 7/50\n",
      "32/32 [==============================] - 0s 784us/step - loss: 0.1964 - mse: 0.1964 - val_loss: 0.1927 - val_mse: 0.1927\n",
      "Epoch 8/50\n",
      "32/32 [==============================] - 0s 746us/step - loss: 0.1403 - mse: 0.1403 - val_loss: 0.1408 - val_mse: 0.1408\n",
      "Epoch 9/50\n",
      "32/32 [==============================] - 0s 772us/step - loss: 0.1043 - mse: 0.1043 - val_loss: 0.1066 - val_mse: 0.1066\n",
      "Epoch 10/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0806 - mse: 0.0806 - val_loss: 0.0834 - val_mse: 0.0834\n",
      "Epoch 11/50\n",
      "32/32 [==============================] - 0s 862us/step - loss: 0.0655 - mse: 0.0655 - val_loss: 0.0683 - val_mse: 0.0683\n",
      "Epoch 12/50\n",
      "32/32 [==============================] - 0s 759us/step - loss: 0.0558 - mse: 0.0558 - val_loss: 0.0580 - val_mse: 0.0580\n",
      "Epoch 13/50\n",
      "32/32 [==============================] - 0s 741us/step - loss: 0.0496 - mse: 0.0496 - val_loss: 0.0509 - val_mse: 0.0509\n",
      "Epoch 14/50\n",
      "32/32 [==============================] - 0s 746us/step - loss: 0.0455 - mse: 0.0455 - val_loss: 0.0461 - val_mse: 0.0461\n",
      "Epoch 15/50\n",
      "32/32 [==============================] - 0s 706us/step - loss: 0.0428 - mse: 0.0428 - val_loss: 0.0429 - val_mse: 0.0429\n",
      "Epoch 16/50\n",
      "32/32 [==============================] - 0s 733us/step - loss: 0.0411 - mse: 0.0411 - val_loss: 0.0402 - val_mse: 0.0402\n",
      "Epoch 17/50\n",
      "32/32 [==============================] - 0s 735us/step - loss: 0.0399 - mse: 0.0399 - val_loss: 0.0386 - val_mse: 0.0386\n",
      "Epoch 18/50\n",
      "32/32 [==============================] - 0s 774us/step - loss: 0.0392 - mse: 0.0392 - val_loss: 0.0371 - val_mse: 0.0371\n",
      "Epoch 19/50\n",
      "32/32 [==============================] - 0s 761us/step - loss: 0.0387 - mse: 0.0387 - val_loss: 0.0361 - val_mse: 0.0361\n",
      "Epoch 20/50\n",
      "32/32 [==============================] - 0s 762us/step - loss: 0.0384 - mse: 0.0384 - val_loss: 0.0354 - val_mse: 0.0354\n",
      "Epoch 21/50\n",
      "32/32 [==============================] - 0s 843us/step - loss: 0.0382 - mse: 0.0382 - val_loss: 0.0350 - val_mse: 0.0350\n",
      "Epoch 22/50\n",
      "32/32 [==============================] - 0s 767us/step - loss: 0.0380 - mse: 0.0380 - val_loss: 0.0349 - val_mse: 0.0349\n",
      "Epoch 23/50\n",
      "32/32 [==============================] - 0s 753us/step - loss: 0.0380 - mse: 0.0380 - val_loss: 0.0346 - val_mse: 0.0346\n",
      "Epoch 24/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0344 - val_mse: 0.0344\n",
      "Epoch 25/50\n",
      "32/32 [==============================] - 0s 744us/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0341 - val_mse: 0.0341\n",
      "Epoch 26/50\n",
      "32/32 [==============================] - 0s 747us/step - loss: 0.0378 - mse: 0.0378 - val_loss: 0.0341 - val_mse: 0.0341\n",
      "Epoch 27/50\n",
      "32/32 [==============================] - 0s 750us/step - loss: 0.0378 - mse: 0.0378 - val_loss: 0.0337 - val_mse: 0.0337\n",
      "Epoch 28/50\n",
      "32/32 [==============================] - 0s 784us/step - loss: 0.0378 - mse: 0.0378 - val_loss: 0.0339 - val_mse: 0.0339\n",
      "Epoch 29/50\n",
      "32/32 [==============================] - 0s 748us/step - loss: 0.0378 - mse: 0.0378 - val_loss: 0.0339 - val_mse: 0.0339\n",
      "Epoch 30/50\n",
      "32/32 [==============================] - 0s 765us/step - loss: 0.0378 - mse: 0.0378 - val_loss: 0.0337 - val_mse: 0.0337\n",
      "Epoch 31/50\n",
      "32/32 [==============================] - 0s 747us/step - loss: 0.0378 - mse: 0.0378 - val_loss: 0.0337 - val_mse: 0.0337\n",
      "Epoch 32/50\n",
      "32/32 [==============================] - 0s 774us/step - loss: 0.0378 - mse: 0.0378 - val_loss: 0.0337 - val_mse: 0.0337\n",
      "Epoch 33/50\n",
      "32/32 [==============================] - 0s 758us/step - loss: 0.0378 - mse: 0.0378 - val_loss: 0.0337 - val_mse: 0.0337\n",
      "Epoch 34/50\n",
      "32/32 [==============================] - 0s 755us/step - loss: 0.0378 - mse: 0.0378 - val_loss: 0.0338 - val_mse: 0.0338\n",
      "Epoch 35/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0378 - mse: 0.0378 - val_loss: 0.0338 - val_mse: 0.0338\n",
      "Epoch 36/50\n",
      "32/32 [==============================] - 0s 994us/step - loss: 0.0378 - mse: 0.0378 - val_loss: 0.0338 - val_mse: 0.0338\n",
      "Epoch 37/50\n",
      "32/32 [==============================] - 0s 753us/step - loss: 0.0378 - mse: 0.0378 - val_loss: 0.0338 - val_mse: 0.0338\n",
      "Epoch 38/50\n",
      "32/32 [==============================] - 0s 759us/step - loss: 0.0378 - mse: 0.0378 - val_loss: 0.0338 - val_mse: 0.0338\n",
      "Epoch 39/50\n",
      "32/32 [==============================] - 0s 750us/step - loss: 0.0378 - mse: 0.0378 - val_loss: 0.0337 - val_mse: 0.0337\n",
      "Epoch 40/50\n",
      "32/32 [==============================] - 0s 747us/step - loss: 0.0378 - mse: 0.0378 - val_loss: 0.0338 - val_mse: 0.0338\n",
      "Epoch 41/50\n",
      "32/32 [==============================] - 0s 740us/step - loss: 0.0378 - mse: 0.0378 - val_loss: 0.0336 - val_mse: 0.0336\n",
      "Epoch 42/50\n",
      "32/32 [==============================] - 0s 754us/step - loss: 0.0378 - mse: 0.0378 - val_loss: 0.0336 - val_mse: 0.0336\n",
      "Epoch 43/50\n",
      "32/32 [==============================] - 0s 745us/step - loss: 0.0378 - mse: 0.0378 - val_loss: 0.0335 - val_mse: 0.0335\n",
      "Epoch 44/50\n",
      "32/32 [==============================] - 0s 773us/step - loss: 0.0378 - mse: 0.0378 - val_loss: 0.0337 - val_mse: 0.0337\n",
      "Epoch 45/50\n",
      "32/32 [==============================] - 0s 741us/step - loss: 0.0378 - mse: 0.0378 - val_loss: 0.0337 - val_mse: 0.0337\n",
      "Epoch 46/50\n",
      "32/32 [==============================] - 0s 782us/step - loss: 0.0378 - mse: 0.0378 - val_loss: 0.0336 - val_mse: 0.0336\n",
      "Epoch 47/50\n",
      "32/32 [==============================] - 0s 749us/step - loss: 0.0378 - mse: 0.0378 - val_loss: 0.0338 - val_mse: 0.0338\n",
      "Epoch 48/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0378 - mse: 0.0378 - val_loss: 0.0337 - val_mse: 0.0337\n",
      "Epoch 49/50\n",
      "32/32 [==============================] - 0s 730us/step - loss: 0.0378 - mse: 0.0378 - val_loss: 0.0337 - val_mse: 0.0337\n",
      "Epoch 50/50\n",
      "32/32 [==============================] - 0s 751us/step - loss: 0.0378 - mse: 0.0378 - val_loss: 0.0336 - val_mse: 0.0336\n",
      "Epoch 1/50\n",
      "47/47 [==============================] - 0s 2ms/step - loss: 3.7631 - mse: 3.7631 - val_loss: 2.6599 - val_mse: 2.6599\n",
      "Epoch 2/50\n",
      "47/47 [==============================] - 0s 626us/step - loss: 1.8001 - mse: 1.8001 - val_loss: 1.2698 - val_mse: 1.2698\n",
      "Epoch 3/50\n",
      "47/47 [==============================] - 0s 602us/step - loss: 0.9275 - mse: 0.9275 - val_loss: 0.6536 - val_mse: 0.6536\n",
      "Epoch 4/50\n",
      "47/47 [==============================] - 0s 606us/step - loss: 0.4962 - mse: 0.4962 - val_loss: 0.3541 - val_mse: 0.3541\n",
      "Epoch 5/50\n",
      "47/47 [==============================] - 0s 589us/step - loss: 0.2754 - mse: 0.2754 - val_loss: 0.2027 - val_mse: 0.2027\n",
      "Epoch 6/50\n",
      "47/47 [==============================] - 0s 580us/step - loss: 0.1613 - mse: 0.1613 - val_loss: 0.1260 - val_mse: 0.1260\n",
      "Epoch 7/50\n",
      "47/47 [==============================] - 0s 617us/step - loss: 0.1019 - mse: 0.1019 - val_loss: 0.0852 - val_mse: 0.0852\n",
      "Epoch 8/50\n",
      "47/47 [==============================] - 0s 596us/step - loss: 0.0711 - mse: 0.0711 - val_loss: 0.0638 - val_mse: 0.0638\n",
      "Epoch 9/50\n",
      "47/47 [==============================] - 0s 2ms/step - loss: 0.0551 - mse: 0.0551 - val_loss: 0.0524 - val_mse: 0.0524\n",
      "Epoch 10/50\n",
      "47/47 [==============================] - 0s 596us/step - loss: 0.0468 - mse: 0.0468 - val_loss: 0.0464 - val_mse: 0.0464\n",
      "Epoch 11/50\n",
      "47/47 [==============================] - 0s 584us/step - loss: 0.0425 - mse: 0.0425 - val_loss: 0.0431 - val_mse: 0.0431\n",
      "Epoch 12/50\n",
      "47/47 [==============================] - 0s 597us/step - loss: 0.0402 - mse: 0.0402 - val_loss: 0.0412 - val_mse: 0.0412\n",
      "Epoch 13/50\n",
      "47/47 [==============================] - 0s 592us/step - loss: 0.0391 - mse: 0.0391 - val_loss: 0.0401 - val_mse: 0.0401\n",
      "Epoch 14/50\n",
      "47/47 [==============================] - 0s 596us/step - loss: 0.0385 - mse: 0.0385 - val_loss: 0.0396 - val_mse: 0.0396\n",
      "Epoch 15/50\n",
      "47/47 [==============================] - 0s 600us/step - loss: 0.0382 - mse: 0.0382 - val_loss: 0.0393 - val_mse: 0.0393\n",
      "Epoch 16/50\n",
      "47/47 [==============================] - 0s 571us/step - loss: 0.0380 - mse: 0.0380 - val_loss: 0.0390 - val_mse: 0.0390\n",
      "Epoch 17/50\n",
      "47/47 [==============================] - 0s 584us/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0389 - val_mse: 0.0389\n",
      "Epoch 18/50\n",
      "47/47 [==============================] - 0s 589us/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0388 - val_mse: 0.0388\n",
      "Epoch 19/50\n",
      "47/47 [==============================] - 0s 608us/step - loss: 0.0378 - mse: 0.0378 - val_loss: 0.0388 - val_mse: 0.0388\n",
      "Epoch 20/50\n",
      "47/47 [==============================] - 0s 2ms/step - loss: 0.0378 - mse: 0.0378 - val_loss: 0.0388 - val_mse: 0.0388\n",
      "Epoch 21/50\n",
      "47/47 [==============================] - 0s 599us/step - loss: 0.0378 - mse: 0.0378 - val_loss: 0.0387 - val_mse: 0.0387\n",
      "Epoch 22/50\n",
      "47/47 [==============================] - 0s 580us/step - loss: 0.0378 - mse: 0.0378 - val_loss: 0.0386 - val_mse: 0.0386\n",
      "Epoch 23/50\n",
      "47/47 [==============================] - 0s 595us/step - loss: 0.0378 - mse: 0.0378 - val_loss: 0.0387 - val_mse: 0.0387\n",
      "Epoch 24/50\n",
      "47/47 [==============================] - 0s 622us/step - loss: 0.0378 - mse: 0.0378 - val_loss: 0.0387 - val_mse: 0.0387\n",
      "Epoch 25/50\n",
      "47/47 [==============================] - 0s 620us/step - loss: 0.0378 - mse: 0.0378 - val_loss: 0.0386 - val_mse: 0.0386\n",
      "Epoch 26/50\n",
      "47/47 [==============================] - 0s 592us/step - loss: 0.0378 - mse: 0.0378 - val_loss: 0.0387 - val_mse: 0.0387\n",
      "Epoch 27/50\n",
      "47/47 [==============================] - 0s 588us/step - loss: 0.0378 - mse: 0.0378 - val_loss: 0.0386 - val_mse: 0.0386\n",
      "Epoch 28/50\n",
      "47/47 [==============================] - 0s 620us/step - loss: 0.0378 - mse: 0.0378 - val_loss: 0.0386 - val_mse: 0.0386\n",
      "Epoch 29/50\n",
      "47/47 [==============================] - 0s 691us/step - loss: 0.0378 - mse: 0.0378 - val_loss: 0.0386 - val_mse: 0.0386\n",
      "Epoch 30/50\n",
      "47/47 [==============================] - 0s 644us/step - loss: 0.0378 - mse: 0.0378 - val_loss: 0.0386 - val_mse: 0.0386\n",
      "Epoch 31/50\n",
      "47/47 [==============================] - 0s 928us/step - loss: 0.0378 - mse: 0.0378 - val_loss: 0.0386 - val_mse: 0.0386\n",
      "Epoch 32/50\n",
      "47/47 [==============================] - 0s 627us/step - loss: 0.0378 - mse: 0.0378 - val_loss: 0.0387 - val_mse: 0.0387\n",
      "Epoch 33/50\n",
      "47/47 [==============================] - 0s 585us/step - loss: 0.0378 - mse: 0.0378 - val_loss: 0.0387 - val_mse: 0.0387\n",
      "Epoch 34/50\n",
      "47/47 [==============================] - 0s 589us/step - loss: 0.0378 - mse: 0.0378 - val_loss: 0.0387 - val_mse: 0.0387\n",
      "Epoch 35/50\n",
      "47/47 [==============================] - 0s 595us/step - loss: 0.0378 - mse: 0.0378 - val_loss: 0.0387 - val_mse: 0.0387\n",
      "Epoch 36/50\n",
      "47/47 [==============================] - 0s 686us/step - loss: 0.0378 - mse: 0.0378 - val_loss: 0.0387 - val_mse: 0.0387\n",
      "Epoch 37/50\n",
      "47/47 [==============================] - 0s 2ms/step - loss: 0.0378 - mse: 0.0378 - val_loss: 0.0387 - val_mse: 0.0387\n",
      "Epoch 38/50\n",
      "47/47 [==============================] - 0s 2ms/step - loss: 0.0378 - mse: 0.0378 - val_loss: 0.0386 - val_mse: 0.0386\n",
      "Epoch 39/50\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.0378 - mse: 0.0378 - val_loss: 0.0386 - val_mse: 0.0386\n",
      "Epoch 40/50\n",
      "47/47 [==============================] - 0s 888us/step - loss: 0.0378 - mse: 0.0378 - val_loss: 0.0386 - val_mse: 0.0386\n",
      "Epoch 41/50\n",
      "47/47 [==============================] - 0s 629us/step - loss: 0.0378 - mse: 0.0378 - val_loss: 0.0386 - val_mse: 0.0386\n",
      "Epoch 42/50\n",
      "47/47 [==============================] - 0s 607us/step - loss: 0.0378 - mse: 0.0378 - val_loss: 0.0386 - val_mse: 0.0386\n",
      "Epoch 43/50\n",
      "47/47 [==============================] - 0s 605us/step - loss: 0.0378 - mse: 0.0378 - val_loss: 0.0387 - val_mse: 0.0387\n",
      "Epoch 44/50\n",
      "47/47 [==============================] - 0s 2ms/step - loss: 0.0378 - mse: 0.0378 - val_loss: 0.0386 - val_mse: 0.0386\n",
      "Epoch 45/50\n",
      "47/47 [==============================] - 0s 674us/step - loss: 0.0378 - mse: 0.0378 - val_loss: 0.0387 - val_mse: 0.0387\n",
      "Epoch 46/50\n",
      "47/47 [==============================] - 0s 731us/step - loss: 0.0378 - mse: 0.0378 - val_loss: 0.0387 - val_mse: 0.0387\n",
      "Epoch 47/50\n",
      "47/47 [==============================] - 0s 608us/step - loss: 0.0378 - mse: 0.0378 - val_loss: 0.0386 - val_mse: 0.0386\n",
      "Epoch 48/50\n",
      "47/47 [==============================] - 0s 599us/step - loss: 0.0378 - mse: 0.0378 - val_loss: 0.0386 - val_mse: 0.0386\n",
      "Epoch 49/50\n",
      "47/47 [==============================] - 0s 606us/step - loss: 0.0378 - mse: 0.0378 - val_loss: 0.0386 - val_mse: 0.0386\n",
      "Epoch 50/50\n",
      "47/47 [==============================] - 0s 594us/step - loss: 0.0378 - mse: 0.0378 - val_loss: 0.0386 - val_mse: 0.0386\n",
      "Epoch 1/50\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 3.1363 - mse: 3.1363 - val_loss: 1.6325 - val_mse: 1.6325\n",
      "Epoch 2/50\n",
      "63/63 [==============================] - 0s 602us/step - loss: 1.1900 - mse: 1.1900 - val_loss: 0.7083 - val_mse: 0.7083\n",
      "Epoch 3/50\n",
      "63/63 [==============================] - 0s 536us/step - loss: 0.5054 - mse: 0.5054 - val_loss: 0.3243 - val_mse: 0.3243\n",
      "Epoch 4/50\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.2315 - mse: 0.2315 - val_loss: 0.1612 - val_mse: 0.1612\n",
      "Epoch 5/50\n",
      "63/63 [==============================] - 0s 520us/step - loss: 0.1185 - mse: 0.1185 - val_loss: 0.0913 - val_mse: 0.0913\n",
      "Epoch 6/50\n",
      "63/63 [==============================] - 0s 519us/step - loss: 0.0716 - mse: 0.0716 - val_loss: 0.0614 - val_mse: 0.0614\n",
      "Epoch 7/50\n",
      "63/63 [==============================] - 0s 576us/step - loss: 0.0521 - mse: 0.0521 - val_loss: 0.0485 - val_mse: 0.0485\n",
      "Epoch 8/50\n",
      "63/63 [==============================] - 0s 524us/step - loss: 0.0440 - mse: 0.0440 - val_loss: 0.0429 - val_mse: 0.0429\n",
      "Epoch 9/50\n",
      "63/63 [==============================] - 0s 526us/step - loss: 0.0407 - mse: 0.0407 - val_loss: 0.0404 - val_mse: 0.0404\n",
      "Epoch 10/50\n",
      "63/63 [==============================] - 0s 514us/step - loss: 0.0394 - mse: 0.0394 - val_loss: 0.0393 - val_mse: 0.0393\n",
      "Epoch 11/50\n",
      "63/63 [==============================] - 0s 510us/step - loss: 0.0388 - mse: 0.0388 - val_loss: 0.0387 - val_mse: 0.0387\n",
      "Epoch 12/50\n",
      "63/63 [==============================] - 0s 517us/step - loss: 0.0386 - mse: 0.0386 - val_loss: 0.0384 - val_mse: 0.0384\n",
      "Epoch 13/50\n",
      "63/63 [==============================] - 0s 505us/step - loss: 0.0385 - mse: 0.0385 - val_loss: 0.0383 - val_mse: 0.0383\n",
      "Epoch 14/50\n",
      "63/63 [==============================] - 0s 508us/step - loss: 0.0384 - mse: 0.0384 - val_loss: 0.0382 - val_mse: 0.0382\n",
      "Epoch 15/50\n",
      "63/63 [==============================] - 0s 641us/step - loss: 0.0384 - mse: 0.0384 - val_loss: 0.0381 - val_mse: 0.0381\n",
      "Epoch 16/50\n",
      "63/63 [==============================] - 0s 518us/step - loss: 0.0384 - mse: 0.0384 - val_loss: 0.0381 - val_mse: 0.0381\n",
      "Epoch 17/50\n",
      "63/63 [==============================] - 0s 512us/step - loss: 0.0384 - mse: 0.0384 - val_loss: 0.0381 - val_mse: 0.0381\n",
      "Epoch 18/50\n",
      "63/63 [==============================] - 0s 513us/step - loss: 0.0384 - mse: 0.0384 - val_loss: 0.0381 - val_mse: 0.0381\n",
      "Epoch 19/50\n",
      "63/63 [==============================] - 0s 513us/step - loss: 0.0384 - mse: 0.0384 - val_loss: 0.0380 - val_mse: 0.0380\n",
      "Epoch 20/50\n",
      "63/63 [==============================] - 0s 522us/step - loss: 0.0384 - mse: 0.0384 - val_loss: 0.0380 - val_mse: 0.0380\n",
      "Epoch 21/50\n",
      "63/63 [==============================] - 0s 504us/step - loss: 0.0384 - mse: 0.0384 - val_loss: 0.0380 - val_mse: 0.0380\n",
      "Epoch 22/50\n",
      "63/63 [==============================] - 0s 514us/step - loss: 0.0384 - mse: 0.0384 - val_loss: 0.0380 - val_mse: 0.0380\n",
      "Epoch 23/50\n",
      "63/63 [==============================] - 0s 515us/step - loss: 0.0384 - mse: 0.0384 - val_loss: 0.0380 - val_mse: 0.0380\n",
      "Epoch 24/50\n",
      "63/63 [==============================] - 0s 531us/step - loss: 0.0384 - mse: 0.0384 - val_loss: 0.0380 - val_mse: 0.0380\n",
      "Epoch 25/50\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0384 - mse: 0.0384 - val_loss: 0.0380 - val_mse: 0.0380\n",
      "Epoch 26/50\n",
      "63/63 [==============================] - 0s 713us/step - loss: 0.0384 - mse: 0.0384 - val_loss: 0.0380 - val_mse: 0.0380\n",
      "Epoch 27/50\n",
      "63/63 [==============================] - 0s 898us/step - loss: 0.0384 - mse: 0.0384 - val_loss: 0.0380 - val_mse: 0.0380\n",
      "Epoch 28/50\n",
      "63/63 [==============================] - 0s 551us/step - loss: 0.0384 - mse: 0.0384 - val_loss: 0.0380 - val_mse: 0.0380\n",
      "Epoch 29/50\n",
      "63/63 [==============================] - 0s 637us/step - loss: 0.0384 - mse: 0.0384 - val_loss: 0.0380 - val_mse: 0.0380\n",
      "Epoch 30/50\n",
      "63/63 [==============================] - 0s 513us/step - loss: 0.0384 - mse: 0.0384 - val_loss: 0.0380 - val_mse: 0.0380\n",
      "Epoch 31/50\n",
      "63/63 [==============================] - 0s 506us/step - loss: 0.0384 - mse: 0.0384 - val_loss: 0.0380 - val_mse: 0.0380\n",
      "Epoch 32/50\n",
      "63/63 [==============================] - 0s 515us/step - loss: 0.0384 - mse: 0.0384 - val_loss: 0.0380 - val_mse: 0.0380\n",
      "Epoch 33/50\n",
      "63/63 [==============================] - 0s 521us/step - loss: 0.0384 - mse: 0.0384 - val_loss: 0.0380 - val_mse: 0.0380\n",
      "Epoch 34/50\n",
      "63/63 [==============================] - 0s 515us/step - loss: 0.0384 - mse: 0.0384 - val_loss: 0.0380 - val_mse: 0.0380\n",
      "Epoch 35/50\n",
      "63/63 [==============================] - 0s 525us/step - loss: 0.0384 - mse: 0.0384 - val_loss: 0.0380 - val_mse: 0.0380\n",
      "Epoch 36/50\n",
      "63/63 [==============================] - 0s 509us/step - loss: 0.0384 - mse: 0.0384 - val_loss: 0.0380 - val_mse: 0.0380\n",
      "Epoch 37/50\n",
      "63/63 [==============================] - 0s 509us/step - loss: 0.0384 - mse: 0.0384 - val_loss: 0.0380 - val_mse: 0.0380\n",
      "Epoch 38/50\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0384 - mse: 0.0384 - val_loss: 0.0380 - val_mse: 0.0380\n",
      "Epoch 39/50\n",
      "63/63 [==============================] - 0s 517us/step - loss: 0.0384 - mse: 0.0384 - val_loss: 0.0380 - val_mse: 0.0380\n",
      "Epoch 40/50\n",
      "63/63 [==============================] - 0s 518us/step - loss: 0.0384 - mse: 0.0384 - val_loss: 0.0380 - val_mse: 0.0380\n",
      "Epoch 41/50\n",
      "63/63 [==============================] - 0s 517us/step - loss: 0.0384 - mse: 0.0384 - val_loss: 0.0380 - val_mse: 0.0380\n",
      "Epoch 42/50\n",
      "63/63 [==============================] - 0s 522us/step - loss: 0.0384 - mse: 0.0384 - val_loss: 0.0380 - val_mse: 0.0380\n",
      "Epoch 43/50\n",
      "63/63 [==============================] - 0s 508us/step - loss: 0.0384 - mse: 0.0384 - val_loss: 0.0380 - val_mse: 0.0380\n",
      "Epoch 44/50\n",
      "63/63 [==============================] - 0s 517us/step - loss: 0.0384 - mse: 0.0384 - val_loss: 0.0380 - val_mse: 0.0380\n",
      "Epoch 45/50\n",
      "63/63 [==============================] - 0s 524us/step - loss: 0.0384 - mse: 0.0384 - val_loss: 0.0380 - val_mse: 0.0380\n",
      "Epoch 46/50\n",
      "63/63 [==============================] - 0s 510us/step - loss: 0.0384 - mse: 0.0384 - val_loss: 0.0380 - val_mse: 0.0380\n",
      "Epoch 47/50\n",
      "63/63 [==============================] - 0s 519us/step - loss: 0.0384 - mse: 0.0384 - val_loss: 0.0380 - val_mse: 0.0380\n",
      "Epoch 48/50\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0384 - mse: 0.0384 - val_loss: 0.0380 - val_mse: 0.0380\n",
      "Epoch 49/50\n",
      "63/63 [==============================] - 0s 553us/step - loss: 0.0384 - mse: 0.0384 - val_loss: 0.0380 - val_mse: 0.0380\n",
      "Epoch 50/50\n",
      "63/63 [==============================] - 0s 582us/step - loss: 0.0384 - mse: 0.0384 - val_loss: 0.0380 - val_mse: 0.0380\n",
      "Epoch 1/10\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1.0253 - mse: 1.0253 - val_loss: 1.0983 - val_mse: 1.0983\n",
      "Epoch 2/10\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.6847 - mse: 0.6847 - val_loss: 0.7890 - val_mse: 0.7890\n",
      "Epoch 3/10\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.5022 - mse: 0.5022 - val_loss: 0.6067 - val_mse: 0.6067\n",
      "Epoch 4/10\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.4018 - mse: 0.4018 - val_loss: 0.4939 - val_mse: 0.4939\n",
      "Epoch 5/10\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3430 - mse: 0.3430 - val_loss: 0.4223 - val_mse: 0.4223\n",
      "Epoch 6/10\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3092 - mse: 0.3092 - val_loss: 0.3764 - val_mse: 0.3764\n",
      "Epoch 7/10\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.2891 - mse: 0.2891 - val_loss: 0.3471 - val_mse: 0.3471\n",
      "Epoch 8/10\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2766 - mse: 0.2766 - val_loss: 0.3274 - val_mse: 0.3274\n",
      "Epoch 9/10\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2682 - mse: 0.2682 - val_loss: 0.3128 - val_mse: 0.3128\n",
      "Epoch 10/10\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.2626 - mse: 0.2626 - val_loss: 0.3022 - val_mse: 0.3022\n",
      "Epoch 1/10\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.9543 - mse: 0.9543 - val_loss: 0.5486 - val_mse: 0.5486\n",
      "Epoch 2/10\n",
      "32/32 [==============================] - 0s 785us/step - loss: 0.4846 - mse: 0.4846 - val_loss: 0.3489 - val_mse: 0.3489\n",
      "Epoch 3/10\n",
      "32/32 [==============================] - 0s 781us/step - loss: 0.3349 - mse: 0.3349 - val_loss: 0.2878 - val_mse: 0.2878\n",
      "Epoch 4/10\n",
      "32/32 [==============================] - 0s 740us/step - loss: 0.2842 - mse: 0.2842 - val_loss: 0.2637 - val_mse: 0.2637\n",
      "Epoch 5/10\n",
      "32/32 [==============================] - 0s 748us/step - loss: 0.2632 - mse: 0.2632 - val_loss: 0.2493 - val_mse: 0.2493\n",
      "Epoch 6/10\n",
      "32/32 [==============================] - 0s 735us/step - loss: 0.2524 - mse: 0.2524 - val_loss: 0.2404 - val_mse: 0.2404\n",
      "Epoch 7/10\n",
      "32/32 [==============================] - 0s 734us/step - loss: 0.2460 - mse: 0.2460 - val_loss: 0.2337 - val_mse: 0.2337\n",
      "Epoch 8/10\n",
      "32/32 [==============================] - 0s 729us/step - loss: 0.2425 - mse: 0.2425 - val_loss: 0.2287 - val_mse: 0.2287\n",
      "Epoch 9/10\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2401 - mse: 0.2401 - val_loss: 0.2244 - val_mse: 0.2244\n",
      "Epoch 10/10\n",
      "32/32 [==============================] - 0s 809us/step - loss: 0.2388 - mse: 0.2388 - val_loss: 0.2213 - val_mse: 0.2213\n",
      "Epoch 1/10\n",
      "47/47 [==============================] - 0s 2ms/step - loss: 2.9187 - mse: 2.9187 - val_loss: 2.1210 - val_mse: 2.1210\n",
      "Epoch 2/10\n",
      "47/47 [==============================] - 0s 631us/step - loss: 1.4613 - mse: 1.4613 - val_loss: 1.1210 - val_mse: 1.1210\n",
      "Epoch 3/10\n",
      "47/47 [==============================] - 0s 597us/step - loss: 0.8481 - mse: 0.8481 - val_loss: 0.6852 - val_mse: 0.6852\n",
      "Epoch 4/10\n",
      "47/47 [==============================] - 0s 592us/step - loss: 0.5507 - mse: 0.5507 - val_loss: 0.4755 - val_mse: 0.4755\n",
      "Epoch 5/10\n",
      "47/47 [==============================] - 0s 583us/step - loss: 0.3992 - mse: 0.3992 - val_loss: 0.3681 - val_mse: 0.3681\n",
      "Epoch 6/10\n",
      "47/47 [==============================] - 0s 595us/step - loss: 0.3209 - mse: 0.3209 - val_loss: 0.3108 - val_mse: 0.3108\n",
      "Epoch 7/10\n",
      "47/47 [==============================] - 0s 597us/step - loss: 0.2801 - mse: 0.2801 - val_loss: 0.2802 - val_mse: 0.2802\n",
      "Epoch 8/10\n",
      "47/47 [==============================] - 0s 607us/step - loss: 0.2591 - mse: 0.2591 - val_loss: 0.2640 - val_mse: 0.2640\n",
      "Epoch 9/10\n",
      "47/47 [==============================] - 0s 588us/step - loss: 0.2482 - mse: 0.2482 - val_loss: 0.2544 - val_mse: 0.2544\n",
      "Epoch 10/10\n",
      "47/47 [==============================] - 0s 2ms/step - loss: 0.2424 - mse: 0.2424 - val_loss: 0.2495 - val_mse: 0.2495\n",
      "Epoch 1/10\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.8197 - mse: 0.8197 - val_loss: 0.4326 - val_mse: 0.4326\n",
      "Epoch 2/10\n",
      "63/63 [==============================] - 0s 537us/step - loss: 0.3572 - mse: 0.3572 - val_loss: 0.3059 - val_mse: 0.3059\n",
      "Epoch 3/10\n",
      "63/63 [==============================] - 0s 526us/step - loss: 0.2786 - mse: 0.2786 - val_loss: 0.2689 - val_mse: 0.2689\n",
      "Epoch 4/10\n",
      "63/63 [==============================] - 0s 510us/step - loss: 0.2554 - mse: 0.2554 - val_loss: 0.2532 - val_mse: 0.2532\n",
      "Epoch 5/10\n",
      "63/63 [==============================] - 0s 501us/step - loss: 0.2465 - mse: 0.2465 - val_loss: 0.2457 - val_mse: 0.2457\n",
      "Epoch 6/10\n",
      "63/63 [==============================] - 0s 515us/step - loss: 0.2427 - mse: 0.2427 - val_loss: 0.2420 - val_mse: 0.2420\n",
      "Epoch 7/10\n",
      "63/63 [==============================] - 0s 516us/step - loss: 0.2412 - mse: 0.2412 - val_loss: 0.2400 - val_mse: 0.2400\n",
      "Epoch 8/10\n",
      "63/63 [==============================] - 0s 517us/step - loss: 0.2405 - mse: 0.2405 - val_loss: 0.2390 - val_mse: 0.2390\n",
      "Epoch 9/10\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.2401 - mse: 0.2401 - val_loss: 0.2385 - val_mse: 0.2385\n",
      "Epoch 10/10\n",
      "63/63 [==============================] - 0s 523us/step - loss: 0.2401 - mse: 0.2401 - val_loss: 0.2382 - val_mse: 0.2382\n",
      "Epoch 1/15\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1.8122 - mse: 1.8122 - val_loss: 1.6551 - val_mse: 1.6551\n",
      "Epoch 2/15\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.3243 - mse: 1.3243 - val_loss: 1.2413 - val_mse: 1.2413\n",
      "Epoch 3/15\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0166 - mse: 1.0166 - val_loss: 0.9767 - val_mse: 0.9767\n",
      "Epoch 4/15\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.8170 - mse: 0.8170 - val_loss: 0.7936 - val_mse: 0.7936\n",
      "Epoch 5/15\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.6778 - mse: 0.6778 - val_loss: 0.6643 - val_mse: 0.6643\n",
      "Epoch 6/15\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.5784 - mse: 0.5784 - val_loss: 0.5699 - val_mse: 0.5699\n",
      "Epoch 7/15\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.5056 - mse: 0.5056 - val_loss: 0.5034 - val_mse: 0.5034\n",
      "Epoch 8/15\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.4507 - mse: 0.4507 - val_loss: 0.4511 - val_mse: 0.4511\n",
      "Epoch 9/15\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.4081 - mse: 0.4081 - val_loss: 0.4125 - val_mse: 0.4125\n",
      "Epoch 10/15\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3752 - mse: 0.3752 - val_loss: 0.3827 - val_mse: 0.3827\n",
      "Epoch 11/15\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3491 - mse: 0.3491 - val_loss: 0.3582 - val_mse: 0.3582\n",
      "Epoch 12/15\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3281 - mse: 0.3281 - val_loss: 0.3393 - val_mse: 0.3393\n",
      "Epoch 13/15\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3117 - mse: 0.3117 - val_loss: 0.3246 - val_mse: 0.3246\n",
      "Epoch 14/15\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.2985 - mse: 0.2985 - val_loss: 0.3136 - val_mse: 0.3136\n",
      "Epoch 15/15\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.2882 - mse: 0.2882 - val_loss: 0.3058 - val_mse: 0.3058\n",
      "Epoch 1/15\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3.3925 - mse: 3.3925 - val_loss: 2.7570 - val_mse: 2.7570\n",
      "Epoch 2/15\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2.0729 - mse: 2.0729 - val_loss: 1.8314 - val_mse: 1.8314\n",
      "Epoch 3/15\n",
      "32/32 [==============================] - 0s 766us/step - loss: 1.3675 - mse: 1.3675 - val_loss: 1.2787 - val_mse: 1.2787\n",
      "Epoch 4/15\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.9498 - mse: 0.9498 - val_loss: 0.9271 - val_mse: 0.9271\n",
      "Epoch 5/15\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.6935 - mse: 0.6935 - val_loss: 0.6958 - val_mse: 0.6958\n",
      "Epoch 6/15\n",
      "32/32 [==============================] - 0s 750us/step - loss: 0.5301 - mse: 0.5301 - val_loss: 0.5459 - val_mse: 0.5459\n",
      "Epoch 7/15\n",
      "32/32 [==============================] - 0s 750us/step - loss: 0.4271 - mse: 0.4271 - val_loss: 0.4436 - val_mse: 0.4436\n",
      "Epoch 8/15\n",
      "32/32 [==============================] - 0s 737us/step - loss: 0.3591 - mse: 0.3591 - val_loss: 0.3750 - val_mse: 0.3750\n",
      "Epoch 9/15\n",
      "32/32 [==============================] - 0s 770us/step - loss: 0.3164 - mse: 0.3164 - val_loss: 0.3269 - val_mse: 0.3269\n",
      "Epoch 10/15\n",
      "32/32 [==============================] - 0s 749us/step - loss: 0.2880 - mse: 0.2880 - val_loss: 0.2927 - val_mse: 0.2927\n",
      "Epoch 11/15\n",
      "32/32 [==============================] - 0s 754us/step - loss: 0.2698 - mse: 0.2698 - val_loss: 0.2698 - val_mse: 0.2698\n",
      "Epoch 12/15\n",
      "32/32 [==============================] - 0s 748us/step - loss: 0.2581 - mse: 0.2581 - val_loss: 0.2534 - val_mse: 0.2534\n",
      "Epoch 13/15\n",
      "32/32 [==============================] - 0s 781us/step - loss: 0.2502 - mse: 0.2502 - val_loss: 0.2422 - val_mse: 0.2422\n",
      "Epoch 14/15\n",
      "32/32 [==============================] - 0s 737us/step - loss: 0.2453 - mse: 0.2453 - val_loss: 0.2335 - val_mse: 0.2335\n",
      "Epoch 15/15\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2419 - mse: 0.2419 - val_loss: 0.2267 - val_mse: 0.2267\n",
      "Epoch 1/15\n",
      "47/47 [==============================] - 0s 2ms/step - loss: 3.2337 - mse: 3.2337 - val_loss: 2.3561 - val_mse: 2.3561\n",
      "Epoch 2/15\n",
      "47/47 [==============================] - 0s 629us/step - loss: 1.6254 - mse: 1.6254 - val_loss: 1.2372 - val_mse: 1.2372\n",
      "Epoch 3/15\n",
      "47/47 [==============================] - 0s 613us/step - loss: 0.9335 - mse: 0.9335 - val_loss: 0.7467 - val_mse: 0.7467\n",
      "Epoch 4/15\n",
      "47/47 [==============================] - 0s 663us/step - loss: 0.5951 - mse: 0.5951 - val_loss: 0.5064 - val_mse: 0.5064\n",
      "Epoch 5/15\n",
      "47/47 [==============================] - 0s 617us/step - loss: 0.4224 - mse: 0.4224 - val_loss: 0.3845 - val_mse: 0.3845\n",
      "Epoch 6/15\n",
      "47/47 [==============================] - 0s 594us/step - loss: 0.3331 - mse: 0.3331 - val_loss: 0.3198 - val_mse: 0.3198\n",
      "Epoch 7/15\n",
      "47/47 [==============================] - 0s 611us/step - loss: 0.2865 - mse: 0.2865 - val_loss: 0.2851 - val_mse: 0.2851\n",
      "Epoch 8/15\n",
      "47/47 [==============================] - 0s 624us/step - loss: 0.2623 - mse: 0.2623 - val_loss: 0.2665 - val_mse: 0.2665\n",
      "Epoch 9/15\n",
      "47/47 [==============================] - 0s 620us/step - loss: 0.2498 - mse: 0.2498 - val_loss: 0.2562 - val_mse: 0.2562\n",
      "Epoch 10/15\n",
      "47/47 [==============================] - 0s 2ms/step - loss: 0.2433 - mse: 0.2433 - val_loss: 0.2506 - val_mse: 0.2506\n",
      "Epoch 11/15\n",
      "47/47 [==============================] - 0s 620us/step - loss: 0.2400 - mse: 0.2400 - val_loss: 0.2469 - val_mse: 0.2469\n",
      "Epoch 12/15\n",
      "47/47 [==============================] - 0s 602us/step - loss: 0.2382 - mse: 0.2382 - val_loss: 0.2450 - val_mse: 0.2450\n",
      "Epoch 13/15\n",
      "47/47 [==============================] - 0s 615us/step - loss: 0.2374 - mse: 0.2374 - val_loss: 0.2440 - val_mse: 0.2440\n",
      "Epoch 14/15\n",
      "47/47 [==============================] - 0s 604us/step - loss: 0.2368 - mse: 0.2368 - val_loss: 0.2432 - val_mse: 0.2432\n",
      "Epoch 15/15\n",
      "47/47 [==============================] - 0s 594us/step - loss: 0.2366 - mse: 0.2366 - val_loss: 0.2428 - val_mse: 0.2428\n",
      "Epoch 1/15\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.7686 - mse: 0.7686 - val_loss: 0.4071 - val_mse: 0.4071\n",
      "Epoch 2/15\n",
      "63/63 [==============================] - 0s 554us/step - loss: 0.3369 - mse: 0.3369 - val_loss: 0.2914 - val_mse: 0.2914\n",
      "Epoch 3/15\n",
      "63/63 [==============================] - 0s 524us/step - loss: 0.2698 - mse: 0.2698 - val_loss: 0.2621 - val_mse: 0.2621\n",
      "Epoch 4/15\n",
      "63/63 [==============================] - 0s 524us/step - loss: 0.2516 - mse: 0.2516 - val_loss: 0.2502 - val_mse: 0.2502\n",
      "Epoch 5/15\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.2450 - mse: 0.2450 - val_loss: 0.2444 - val_mse: 0.2444\n",
      "Epoch 6/15\n",
      "63/63 [==============================] - 0s 528us/step - loss: 0.2422 - mse: 0.2422 - val_loss: 0.2413 - val_mse: 0.2413\n",
      "Epoch 7/15\n",
      "63/63 [==============================] - 0s 532us/step - loss: 0.2410 - mse: 0.2410 - val_loss: 0.2397 - val_mse: 0.2397\n",
      "Epoch 8/15\n",
      "63/63 [==============================] - 0s 526us/step - loss: 0.2404 - mse: 0.2404 - val_loss: 0.2387 - val_mse: 0.2387\n",
      "Epoch 9/15\n",
      "63/63 [==============================] - 0s 547us/step - loss: 0.2402 - mse: 0.2402 - val_loss: 0.2383 - val_mse: 0.2383\n",
      "Epoch 10/15\n",
      "63/63 [==============================] - 0s 532us/step - loss: 0.2401 - mse: 0.2401 - val_loss: 0.2380 - val_mse: 0.2380\n",
      "Epoch 11/15\n",
      "63/63 [==============================] - 0s 529us/step - loss: 0.2401 - mse: 0.2401 - val_loss: 0.2378 - val_mse: 0.2378\n",
      "Epoch 12/15\n",
      "63/63 [==============================] - 0s 531us/step - loss: 0.2400 - mse: 0.2400 - val_loss: 0.2378 - val_mse: 0.2378\n",
      "Epoch 13/15\n",
      "63/63 [==============================] - 0s 526us/step - loss: 0.2400 - mse: 0.2400 - val_loss: 0.2377 - val_mse: 0.2377\n",
      "Epoch 14/15\n",
      "63/63 [==============================] - 0s 523us/step - loss: 0.2400 - mse: 0.2400 - val_loss: 0.2378 - val_mse: 0.2378\n",
      "Epoch 15/15\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.2400 - mse: 0.2400 - val_loss: 0.2378 - val_mse: 0.2378\n",
      "Epoch 1/30\n",
      "16/16 [==============================] - 1s 4ms/step - loss: 1.4065 - mse: 1.4065 - val_loss: 1.3563 - val_mse: 1.3563\n",
      "Epoch 2/30\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.9950 - mse: 0.9950 - val_loss: 1.0016 - val_mse: 1.0016\n",
      "Epoch 3/30\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.7533 - mse: 0.7533 - val_loss: 0.7766 - val_mse: 0.7766\n",
      "Epoch 4/30\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.6022 - mse: 0.6022 - val_loss: 0.6323 - val_mse: 0.6323\n",
      "Epoch 5/30\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.5059 - mse: 0.5059 - val_loss: 0.5370 - val_mse: 0.5370\n",
      "Epoch 6/30\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.4413 - mse: 0.4413 - val_loss: 0.4692 - val_mse: 0.4692\n",
      "Epoch 7/30\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3955 - mse: 0.3955 - val_loss: 0.4194 - val_mse: 0.4194\n",
      "Epoch 8/30\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3614 - mse: 0.3614 - val_loss: 0.3853 - val_mse: 0.3853\n",
      "Epoch 9/30\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3366 - mse: 0.3366 - val_loss: 0.3602 - val_mse: 0.3602\n",
      "Epoch 10/30\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3176 - mse: 0.3176 - val_loss: 0.3406 - val_mse: 0.3406\n",
      "Epoch 11/30\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3029 - mse: 0.3029 - val_loss: 0.3259 - val_mse: 0.3259\n",
      "Epoch 12/30\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.2909 - mse: 0.2909 - val_loss: 0.3147 - val_mse: 0.3147\n",
      "Epoch 13/30\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.2817 - mse: 0.2817 - val_loss: 0.3059 - val_mse: 0.3059\n",
      "Epoch 14/30\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.2745 - mse: 0.2745 - val_loss: 0.2988 - val_mse: 0.2988\n",
      "Epoch 15/30\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.2686 - mse: 0.2686 - val_loss: 0.2936 - val_mse: 0.2936\n",
      "Epoch 16/30\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.2638 - mse: 0.2638 - val_loss: 0.2903 - val_mse: 0.2903\n",
      "Epoch 17/30\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.2599 - mse: 0.2599 - val_loss: 0.2871 - val_mse: 0.2871\n",
      "Epoch 18/30\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.2569 - mse: 0.2569 - val_loss: 0.2847 - val_mse: 0.2847\n",
      "Epoch 19/30\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.2544 - mse: 0.2544 - val_loss: 0.2826 - val_mse: 0.2826\n",
      "Epoch 20/30\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.2524 - mse: 0.2524 - val_loss: 0.2807 - val_mse: 0.2807\n",
      "Epoch 21/30\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.2509 - mse: 0.2509 - val_loss: 0.2797 - val_mse: 0.2797\n",
      "Epoch 22/30\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.2496 - mse: 0.2496 - val_loss: 0.2795 - val_mse: 0.2795\n",
      "Epoch 23/30\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.2486 - mse: 0.2486 - val_loss: 0.2790 - val_mse: 0.2790\n",
      "Epoch 24/30\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.2479 - mse: 0.2479 - val_loss: 0.2781 - val_mse: 0.2781\n",
      "Epoch 25/30\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.2472 - mse: 0.2472 - val_loss: 0.2780 - val_mse: 0.2780\n",
      "Epoch 26/30\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.2467 - mse: 0.2467 - val_loss: 0.2785 - val_mse: 0.2785\n",
      "Epoch 27/30\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.2464 - mse: 0.2464 - val_loss: 0.2783 - val_mse: 0.2783\n",
      "Epoch 28/30\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.2461 - mse: 0.2461 - val_loss: 0.2783 - val_mse: 0.2783\n",
      "Epoch 29/30\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.2457 - mse: 0.2457 - val_loss: 0.2777 - val_mse: 0.2777\n",
      "Epoch 30/30\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.2455 - mse: 0.2455 - val_loss: 0.2784 - val_mse: 0.2784\n",
      "Epoch 1/30\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3.7821 - mse: 3.7821 - val_loss: 3.0986 - val_mse: 3.0986\n",
      "Epoch 2/30\n",
      "32/32 [==============================] - 0s 832us/step - loss: 2.3252 - mse: 2.3252 - val_loss: 2.0585 - val_mse: 2.0585\n",
      "Epoch 3/30\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.5321 - mse: 1.5321 - val_loss: 1.4369 - val_mse: 1.4369\n",
      "Epoch 4/30\n",
      "32/32 [==============================] - 0s 806us/step - loss: 1.0629 - mse: 1.0629 - val_loss: 1.0381 - val_mse: 1.0381\n",
      "Epoch 5/30\n",
      "32/32 [==============================] - 0s 804us/step - loss: 0.7688 - mse: 0.7688 - val_loss: 0.7740 - val_mse: 0.7740\n",
      "Epoch 6/30\n",
      "32/32 [==============================] - 0s 833us/step - loss: 0.5813 - mse: 0.5813 - val_loss: 0.5976 - val_mse: 0.5976\n",
      "Epoch 7/30\n",
      "32/32 [==============================] - 0s 798us/step - loss: 0.4607 - mse: 0.4607 - val_loss: 0.4776 - val_mse: 0.4776\n",
      "Epoch 8/30\n",
      "32/32 [==============================] - 0s 824us/step - loss: 0.3813 - mse: 0.3813 - val_loss: 0.3946 - val_mse: 0.3946\n",
      "Epoch 9/30\n",
      "32/32 [==============================] - 0s 811us/step - loss: 0.3289 - mse: 0.3289 - val_loss: 0.3399 - val_mse: 0.3399\n",
      "Epoch 10/30\n",
      "32/32 [==============================] - 0s 809us/step - loss: 0.2966 - mse: 0.2966 - val_loss: 0.3028 - val_mse: 0.3028\n",
      "Epoch 11/30\n",
      "32/32 [==============================] - 0s 831us/step - loss: 0.2755 - mse: 0.2755 - val_loss: 0.2767 - val_mse: 0.2767\n",
      "Epoch 12/30\n",
      "32/32 [==============================] - 0s 799us/step - loss: 0.2617 - mse: 0.2617 - val_loss: 0.2585 - val_mse: 0.2585\n",
      "Epoch 13/30\n",
      "32/32 [==============================] - 0s 805us/step - loss: 0.2529 - mse: 0.2529 - val_loss: 0.2460 - val_mse: 0.2460\n",
      "Epoch 14/30\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2470 - mse: 0.2470 - val_loss: 0.2371 - val_mse: 0.2371\n",
      "Epoch 15/30\n",
      "32/32 [==============================] - 0s 788us/step - loss: 0.2436 - mse: 0.2436 - val_loss: 0.2297 - val_mse: 0.2297\n",
      "Epoch 16/30\n",
      "32/32 [==============================] - 0s 824us/step - loss: 0.2408 - mse: 0.2408 - val_loss: 0.2253 - val_mse: 0.2253\n",
      "Epoch 17/30\n",
      "32/32 [==============================] - 0s 822us/step - loss: 0.2391 - mse: 0.2391 - val_loss: 0.2225 - val_mse: 0.2225\n",
      "Epoch 18/30\n",
      "32/32 [==============================] - 0s 795us/step - loss: 0.2382 - mse: 0.2382 - val_loss: 0.2193 - val_mse: 0.2193\n",
      "Epoch 19/30\n",
      "32/32 [==============================] - 0s 808us/step - loss: 0.2375 - mse: 0.2375 - val_loss: 0.2166 - val_mse: 0.2166\n",
      "Epoch 20/30\n",
      "32/32 [==============================] - 0s 807us/step - loss: 0.2370 - mse: 0.2370 - val_loss: 0.2138 - val_mse: 0.2138\n",
      "Epoch 21/30\n",
      "32/32 [==============================] - 0s 822us/step - loss: 0.2369 - mse: 0.2369 - val_loss: 0.2135 - val_mse: 0.2135\n",
      "Epoch 22/30\n",
      "32/32 [==============================] - 0s 797us/step - loss: 0.2367 - mse: 0.2367 - val_loss: 0.2139 - val_mse: 0.2139\n",
      "Epoch 23/30\n",
      "32/32 [==============================] - 0s 802us/step - loss: 0.2365 - mse: 0.2365 - val_loss: 0.2142 - val_mse: 0.2142\n",
      "Epoch 24/30\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2364 - mse: 0.2364 - val_loss: 0.2132 - val_mse: 0.2132\n",
      "Epoch 25/30\n",
      "32/32 [==============================] - 0s 907us/step - loss: 0.2365 - mse: 0.2365 - val_loss: 0.2120 - val_mse: 0.2120\n",
      "Epoch 26/30\n",
      "32/32 [==============================] - 0s 789us/step - loss: 0.2364 - mse: 0.2364 - val_loss: 0.2115 - val_mse: 0.2115\n",
      "Epoch 27/30\n",
      "32/32 [==============================] - 0s 817us/step - loss: 0.2363 - mse: 0.2363 - val_loss: 0.2105 - val_mse: 0.2105\n",
      "Epoch 28/30\n",
      "32/32 [==============================] - 0s 815us/step - loss: 0.2363 - mse: 0.2363 - val_loss: 0.2099 - val_mse: 0.2099\n",
      "Epoch 29/30\n",
      "32/32 [==============================] - 0s 803us/step - loss: 0.2363 - mse: 0.2363 - val_loss: 0.2106 - val_mse: 0.2106\n",
      "Epoch 30/30\n",
      "32/32 [==============================] - 0s 953us/step - loss: 0.2363 - mse: 0.2363 - val_loss: 0.2115 - val_mse: 0.2115\n",
      "Epoch 1/30\n",
      "47/47 [==============================] - 0s 2ms/step - loss: 1.0205 - mse: 1.0205 - val_loss: 0.6865 - val_mse: 0.6865\n",
      "Epoch 2/30\n",
      "47/47 [==============================] - 0s 688us/step - loss: 0.4767 - mse: 0.4767 - val_loss: 0.4056 - val_mse: 0.4056\n",
      "Epoch 3/30\n",
      "47/47 [==============================] - 0s 659us/step - loss: 0.3366 - mse: 0.3366 - val_loss: 0.3190 - val_mse: 0.3190\n",
      "Epoch 4/30\n",
      "47/47 [==============================] - 0s 2ms/step - loss: 0.2847 - mse: 0.2847 - val_loss: 0.2826 - val_mse: 0.2826\n",
      "Epoch 5/30\n",
      "47/47 [==============================] - 0s 639us/step - loss: 0.2610 - mse: 0.2610 - val_loss: 0.2649 - val_mse: 0.2649\n",
      "Epoch 6/30\n",
      "47/47 [==============================] - 0s 639us/step - loss: 0.2491 - mse: 0.2491 - val_loss: 0.2552 - val_mse: 0.2552\n",
      "Epoch 7/30\n",
      "47/47 [==============================] - 0s 649us/step - loss: 0.2430 - mse: 0.2430 - val_loss: 0.2495 - val_mse: 0.2495\n",
      "Epoch 8/30\n",
      "47/47 [==============================] - 0s 643us/step - loss: 0.2397 - mse: 0.2397 - val_loss: 0.2467 - val_mse: 0.2467\n",
      "Epoch 9/30\n",
      "47/47 [==============================] - 0s 669us/step - loss: 0.2381 - mse: 0.2381 - val_loss: 0.2447 - val_mse: 0.2447\n",
      "Epoch 10/30\n",
      "47/47 [==============================] - 0s 656us/step - loss: 0.2373 - mse: 0.2373 - val_loss: 0.2435 - val_mse: 0.2435\n",
      "Epoch 11/30\n",
      "47/47 [==============================] - 0s 656us/step - loss: 0.2368 - mse: 0.2368 - val_loss: 0.2433 - val_mse: 0.2433\n",
      "Epoch 12/30\n",
      "47/47 [==============================] - 0s 637us/step - loss: 0.2366 - mse: 0.2366 - val_loss: 0.2427 - val_mse: 0.2427\n",
      "Epoch 13/30\n",
      "47/47 [==============================] - 0s 650us/step - loss: 0.2364 - mse: 0.2364 - val_loss: 0.2427 - val_mse: 0.2427\n",
      "Epoch 14/30\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.2363 - mse: 0.2363 - val_loss: 0.2425 - val_mse: 0.2425\n",
      "Epoch 15/30\n",
      "47/47 [==============================] - 0s 698us/step - loss: 0.2363 - mse: 0.2363 - val_loss: 0.2417 - val_mse: 0.2417\n",
      "Epoch 16/30\n",
      "47/47 [==============================] - 0s 652us/step - loss: 0.2363 - mse: 0.2363 - val_loss: 0.2415 - val_mse: 0.2415\n",
      "Epoch 17/30\n",
      "47/47 [==============================] - 0s 639us/step - loss: 0.2363 - mse: 0.2363 - val_loss: 0.2412 - val_mse: 0.2412\n",
      "Epoch 18/30\n",
      "47/47 [==============================] - 0s 635us/step - loss: 0.2363 - mse: 0.2363 - val_loss: 0.2413 - val_mse: 0.2413\n",
      "Epoch 19/30\n",
      "47/47 [==============================] - 0s 667us/step - loss: 0.2363 - mse: 0.2363 - val_loss: 0.2416 - val_mse: 0.2416\n",
      "Epoch 20/30\n",
      "47/47 [==============================] - 0s 631us/step - loss: 0.2363 - mse: 0.2363 - val_loss: 0.2416 - val_mse: 0.2416\n",
      "Epoch 21/30\n",
      "47/47 [==============================] - 0s 647us/step - loss: 0.2363 - mse: 0.2363 - val_loss: 0.2414 - val_mse: 0.2414\n",
      "Epoch 22/30\n",
      "47/47 [==============================] - 0s 670us/step - loss: 0.2363 - mse: 0.2363 - val_loss: 0.2420 - val_mse: 0.2420\n",
      "Epoch 23/30\n",
      "47/47 [==============================] - 0s 663us/step - loss: 0.2364 - mse: 0.2364 - val_loss: 0.2417 - val_mse: 0.2417\n",
      "Epoch 24/30\n",
      "47/47 [==============================] - 0s 639us/step - loss: 0.2364 - mse: 0.2364 - val_loss: 0.2414 - val_mse: 0.2414\n",
      "Epoch 25/30\n",
      "47/47 [==============================] - 0s 2ms/step - loss: 0.2363 - mse: 0.2363 - val_loss: 0.2416 - val_mse: 0.2416\n",
      "Epoch 26/30\n",
      "47/47 [==============================] - 0s 637us/step - loss: 0.2363 - mse: 0.2363 - val_loss: 0.2412 - val_mse: 0.2412\n",
      "Epoch 27/30\n",
      "47/47 [==============================] - 0s 643us/step - loss: 0.2363 - mse: 0.2363 - val_loss: 0.2416 - val_mse: 0.2416\n",
      "Epoch 28/30\n",
      "47/47 [==============================] - 0s 642us/step - loss: 0.2363 - mse: 0.2363 - val_loss: 0.2417 - val_mse: 0.2417\n",
      "Epoch 29/30\n",
      "47/47 [==============================] - 0s 643us/step - loss: 0.2363 - mse: 0.2363 - val_loss: 0.2415 - val_mse: 0.2415\n",
      "Epoch 30/30\n",
      "47/47 [==============================] - 0s 656us/step - loss: 0.2363 - mse: 0.2363 - val_loss: 0.2419 - val_mse: 0.2419\n",
      "Epoch 1/30\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 3.8939 - mse: 3.8939 - val_loss: 2.2145 - val_mse: 2.2145\n",
      "Epoch 2/30\n",
      "63/63 [==============================] - 0s 565us/step - loss: 1.6243 - mse: 1.6243 - val_loss: 1.0863 - val_mse: 1.0863\n",
      "Epoch 3/30\n",
      "63/63 [==============================] - 0s 555us/step - loss: 0.8050 - mse: 0.8050 - val_loss: 0.6062 - val_mse: 0.6062\n",
      "Epoch 4/30\n",
      "63/63 [==============================] - 0s 552us/step - loss: 0.4734 - mse: 0.4734 - val_loss: 0.4013 - val_mse: 0.4013\n",
      "Epoch 5/30\n",
      "63/63 [==============================] - 0s 552us/step - loss: 0.3370 - mse: 0.3370 - val_loss: 0.3112 - val_mse: 0.3112\n",
      "Epoch 6/30\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.2800 - mse: 0.2800 - val_loss: 0.2714 - val_mse: 0.2714\n",
      "Epoch 7/30\n",
      "63/63 [==============================] - 0s 566us/step - loss: 0.2567 - mse: 0.2567 - val_loss: 0.2543 - val_mse: 0.2543\n",
      "Epoch 8/30\n",
      "63/63 [==============================] - 0s 549us/step - loss: 0.2470 - mse: 0.2470 - val_loss: 0.2460 - val_mse: 0.2460\n",
      "Epoch 9/30\n",
      "63/63 [==============================] - 0s 554us/step - loss: 0.2428 - mse: 0.2428 - val_loss: 0.2420 - val_mse: 0.2420\n",
      "Epoch 10/30\n",
      "63/63 [==============================] - 0s 555us/step - loss: 0.2412 - mse: 0.2412 - val_loss: 0.2400 - val_mse: 0.2400\n",
      "Epoch 11/30\n",
      "63/63 [==============================] - 0s 561us/step - loss: 0.2405 - mse: 0.2405 - val_loss: 0.2390 - val_mse: 0.2390\n",
      "Epoch 12/30\n",
      "63/63 [==============================] - 0s 551us/step - loss: 0.2403 - mse: 0.2403 - val_loss: 0.2385 - val_mse: 0.2385\n",
      "Epoch 13/30\n",
      "63/63 [==============================] - 0s 562us/step - loss: 0.2401 - mse: 0.2401 - val_loss: 0.2382 - val_mse: 0.2382\n",
      "Epoch 14/30\n",
      "63/63 [==============================] - 0s 554us/step - loss: 0.2401 - mse: 0.2401 - val_loss: 0.2381 - val_mse: 0.2381\n",
      "Epoch 15/30\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.2400 - mse: 0.2400 - val_loss: 0.2380 - val_mse: 0.2380\n",
      "Epoch 16/30\n",
      "63/63 [==============================] - 0s 564us/step - loss: 0.2400 - mse: 0.2400 - val_loss: 0.2378 - val_mse: 0.2378\n",
      "Epoch 17/30\n",
      "63/63 [==============================] - 0s 560us/step - loss: 0.2400 - mse: 0.2400 - val_loss: 0.2378 - val_mse: 0.2378\n",
      "Epoch 18/30\n",
      "63/63 [==============================] - 0s 559us/step - loss: 0.2401 - mse: 0.2401 - val_loss: 0.2377 - val_mse: 0.2377\n",
      "Epoch 19/30\n",
      "63/63 [==============================] - 0s 558us/step - loss: 0.2400 - mse: 0.2400 - val_loss: 0.2376 - val_mse: 0.2376\n",
      "Epoch 20/30\n",
      "63/63 [==============================] - 0s 563us/step - loss: 0.2400 - mse: 0.2400 - val_loss: 0.2376 - val_mse: 0.2376\n",
      "Epoch 21/30\n",
      "63/63 [==============================] - 0s 547us/step - loss: 0.2400 - mse: 0.2400 - val_loss: 0.2376 - val_mse: 0.2376\n",
      "Epoch 22/30\n",
      "63/63 [==============================] - 0s 544us/step - loss: 0.2401 - mse: 0.2401 - val_loss: 0.2377 - val_mse: 0.2377\n",
      "Epoch 23/30\n",
      "63/63 [==============================] - 0s 541us/step - loss: 0.2400 - mse: 0.2400 - val_loss: 0.2377 - val_mse: 0.2377\n",
      "Epoch 24/30\n",
      "63/63 [==============================] - 0s 541us/step - loss: 0.2400 - mse: 0.2400 - val_loss: 0.2376 - val_mse: 0.2376\n",
      "Epoch 25/30\n",
      "63/63 [==============================] - 0s 529us/step - loss: 0.2400 - mse: 0.2400 - val_loss: 0.2376 - val_mse: 0.2376\n",
      "Epoch 26/30\n",
      "63/63 [==============================] - 0s 554us/step - loss: 0.2400 - mse: 0.2400 - val_loss: 0.2376 - val_mse: 0.2376\n",
      "Epoch 27/30\n",
      "63/63 [==============================] - 0s 540us/step - loss: 0.2400 - mse: 0.2400 - val_loss: 0.2376 - val_mse: 0.2376\n",
      "Epoch 28/30\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.2400 - mse: 0.2400 - val_loss: 0.2377 - val_mse: 0.2377\n",
      "Epoch 29/30\n",
      "63/63 [==============================] - 0s 538us/step - loss: 0.2400 - mse: 0.2400 - val_loss: 0.2376 - val_mse: 0.2376\n",
      "Epoch 30/30\n",
      "63/63 [==============================] - 0s 546us/step - loss: 0.2400 - mse: 0.2400 - val_loss: 0.2377 - val_mse: 0.2377\n",
      "Epoch 1/50\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1.0280 - mse: 1.0280 - val_loss: 1.0980 - val_mse: 1.0980\n",
      "Epoch 2/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.6850 - mse: 0.6850 - val_loss: 0.7872 - val_mse: 0.7872\n",
      "Epoch 3/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.5019 - mse: 0.5019 - val_loss: 0.6049 - val_mse: 0.6049\n",
      "Epoch 4/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.4009 - mse: 0.4009 - val_loss: 0.4916 - val_mse: 0.4916\n",
      "Epoch 5/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3428 - mse: 0.3428 - val_loss: 0.4225 - val_mse: 0.4225\n",
      "Epoch 6/50\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.3097 - mse: 0.3097 - val_loss: 0.3768 - val_mse: 0.3768\n",
      "Epoch 7/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2894 - mse: 0.2894 - val_loss: 0.3459 - val_mse: 0.3459\n",
      "Epoch 8/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.2763 - mse: 0.2763 - val_loss: 0.3263 - val_mse: 0.3263\n",
      "Epoch 9/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.2682 - mse: 0.2682 - val_loss: 0.3123 - val_mse: 0.3123\n",
      "Epoch 10/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.2625 - mse: 0.2625 - val_loss: 0.3024 - val_mse: 0.3024\n",
      "Epoch 11/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.2584 - mse: 0.2584 - val_loss: 0.2949 - val_mse: 0.2949\n",
      "Epoch 12/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.2554 - mse: 0.2554 - val_loss: 0.2892 - val_mse: 0.2892\n",
      "Epoch 13/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2531 - mse: 0.2531 - val_loss: 0.2852 - val_mse: 0.2852\n",
      "Epoch 14/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.2513 - mse: 0.2513 - val_loss: 0.2827 - val_mse: 0.2827\n",
      "Epoch 15/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.2501 - mse: 0.2501 - val_loss: 0.2810 - val_mse: 0.2810\n",
      "Epoch 16/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.2488 - mse: 0.2488 - val_loss: 0.2805 - val_mse: 0.2805\n",
      "Epoch 17/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.2481 - mse: 0.2481 - val_loss: 0.2800 - val_mse: 0.2800\n",
      "Epoch 18/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2473 - mse: 0.2473 - val_loss: 0.2793 - val_mse: 0.2793\n",
      "Epoch 19/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.2470 - mse: 0.2470 - val_loss: 0.2793 - val_mse: 0.2793\n",
      "Epoch 20/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.2464 - mse: 0.2464 - val_loss: 0.2783 - val_mse: 0.2783\n",
      "Epoch 21/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.2461 - mse: 0.2461 - val_loss: 0.2789 - val_mse: 0.2789\n",
      "Epoch 22/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.2458 - mse: 0.2458 - val_loss: 0.2801 - val_mse: 0.2801\n",
      "Epoch 23/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.2455 - mse: 0.2455 - val_loss: 0.2804 - val_mse: 0.2804\n",
      "Epoch 24/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.2455 - mse: 0.2455 - val_loss: 0.2804 - val_mse: 0.2804\n",
      "Epoch 25/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.2452 - mse: 0.2452 - val_loss: 0.2793 - val_mse: 0.2793\n",
      "Epoch 26/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.2452 - mse: 0.2452 - val_loss: 0.2791 - val_mse: 0.2791\n",
      "Epoch 27/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.2450 - mse: 0.2450 - val_loss: 0.2790 - val_mse: 0.2790\n",
      "Epoch 28/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.2450 - mse: 0.2450 - val_loss: 0.2786 - val_mse: 0.2786\n",
      "Epoch 29/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.2449 - mse: 0.2449 - val_loss: 0.2779 - val_mse: 0.2779\n",
      "Epoch 30/50\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.2448 - mse: 0.2448 - val_loss: 0.2783 - val_mse: 0.2783\n",
      "Epoch 31/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2448 - mse: 0.2448 - val_loss: 0.2775 - val_mse: 0.2775\n",
      "Epoch 32/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.2448 - mse: 0.2448 - val_loss: 0.2773 - val_mse: 0.2773\n",
      "Epoch 33/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.2448 - mse: 0.2448 - val_loss: 0.2769 - val_mse: 0.2769\n",
      "Epoch 34/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.2447 - mse: 0.2447 - val_loss: 0.2777 - val_mse: 0.2777\n",
      "Epoch 35/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.2447 - mse: 0.2447 - val_loss: 0.2786 - val_mse: 0.2786\n",
      "Epoch 36/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.2446 - mse: 0.2446 - val_loss: 0.2780 - val_mse: 0.2780\n",
      "Epoch 37/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.2447 - mse: 0.2447 - val_loss: 0.2785 - val_mse: 0.2785\n",
      "Epoch 38/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.2448 - mse: 0.2448 - val_loss: 0.2782 - val_mse: 0.2782\n",
      "Epoch 39/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.2447 - mse: 0.2447 - val_loss: 0.2779 - val_mse: 0.2779\n",
      "Epoch 40/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.2447 - mse: 0.2447 - val_loss: 0.2780 - val_mse: 0.2780\n",
      "Epoch 41/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.2446 - mse: 0.2446 - val_loss: 0.2781 - val_mse: 0.2781\n",
      "Epoch 42/50\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.2447 - mse: 0.2447 - val_loss: 0.2785 - val_mse: 0.2785\n",
      "Epoch 43/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.2448 - mse: 0.2448 - val_loss: 0.2789 - val_mse: 0.2789\n",
      "Epoch 44/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.2447 - mse: 0.2447 - val_loss: 0.2795 - val_mse: 0.2795\n",
      "Epoch 45/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.2447 - mse: 0.2447 - val_loss: 0.2799 - val_mse: 0.2799\n",
      "Epoch 46/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.2446 - mse: 0.2446 - val_loss: 0.2794 - val_mse: 0.2794\n",
      "Epoch 47/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.2446 - mse: 0.2446 - val_loss: 0.2798 - val_mse: 0.2798\n",
      "Epoch 48/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.2446 - mse: 0.2446 - val_loss: 0.2799 - val_mse: 0.2799\n",
      "Epoch 49/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.2446 - mse: 0.2446 - val_loss: 0.2788 - val_mse: 0.2788\n",
      "Epoch 50/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.2446 - mse: 0.2446 - val_loss: 0.2784 - val_mse: 0.2784\n",
      "Epoch 1/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2.8631 - mse: 2.8631 - val_loss: 2.3087 - val_mse: 2.3087\n",
      "Epoch 2/50\n",
      "32/32 [==============================] - 0s 834us/step - loss: 1.7416 - mse: 1.7416 - val_loss: 1.5255 - val_mse: 1.5255\n",
      "Epoch 3/50\n",
      "32/32 [==============================] - 0s 829us/step - loss: 1.1474 - mse: 1.1474 - val_loss: 1.0752 - val_mse: 1.0752\n",
      "Epoch 4/50\n",
      "32/32 [==============================] - 0s 804us/step - loss: 0.8081 - mse: 0.8081 - val_loss: 0.7984 - val_mse: 0.7984\n",
      "Epoch 5/50\n",
      "32/32 [==============================] - 0s 755us/step - loss: 0.6051 - mse: 0.6051 - val_loss: 0.6097 - val_mse: 0.6097\n",
      "Epoch 6/50\n",
      "32/32 [==============================] - 0s 833us/step - loss: 0.4724 - mse: 0.4724 - val_loss: 0.4859 - val_mse: 0.4859\n",
      "Epoch 7/50\n",
      "32/32 [==============================] - 0s 797us/step - loss: 0.3893 - mse: 0.3893 - val_loss: 0.4026 - val_mse: 0.4026\n",
      "Epoch 8/50\n",
      "32/32 [==============================] - 0s 798us/step - loss: 0.3360 - mse: 0.3360 - val_loss: 0.3462 - val_mse: 0.3462\n",
      "Epoch 9/50\n",
      "32/32 [==============================] - 0s 791us/step - loss: 0.3008 - mse: 0.3008 - val_loss: 0.3068 - val_mse: 0.3068\n",
      "Epoch 10/50\n",
      "32/32 [==============================] - 0s 787us/step - loss: 0.2782 - mse: 0.2782 - val_loss: 0.2799 - val_mse: 0.2799\n",
      "Epoch 11/50\n",
      "32/32 [==============================] - 0s 813us/step - loss: 0.2635 - mse: 0.2635 - val_loss: 0.2615 - val_mse: 0.2615\n",
      "Epoch 12/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2542 - mse: 0.2542 - val_loss: 0.2487 - val_mse: 0.2487\n",
      "Epoch 13/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2481 - mse: 0.2481 - val_loss: 0.2392 - val_mse: 0.2392\n",
      "Epoch 14/50\n",
      "32/32 [==============================] - 0s 788us/step - loss: 0.2439 - mse: 0.2439 - val_loss: 0.2319 - val_mse: 0.2319\n",
      "Epoch 15/50\n",
      "32/32 [==============================] - 0s 804us/step - loss: 0.2413 - mse: 0.2413 - val_loss: 0.2279 - val_mse: 0.2279\n",
      "Epoch 16/50\n",
      "32/32 [==============================] - 0s 792us/step - loss: 0.2396 - mse: 0.2396 - val_loss: 0.2244 - val_mse: 0.2244\n",
      "Epoch 17/50\n",
      "32/32 [==============================] - 0s 831us/step - loss: 0.2384 - mse: 0.2384 - val_loss: 0.2216 - val_mse: 0.2216\n",
      "Epoch 18/50\n",
      "32/32 [==============================] - 0s 865us/step - loss: 0.2376 - mse: 0.2376 - val_loss: 0.2191 - val_mse: 0.2191\n",
      "Epoch 19/50\n",
      "32/32 [==============================] - 0s 780us/step - loss: 0.2372 - mse: 0.2372 - val_loss: 0.2182 - val_mse: 0.2182\n",
      "Epoch 20/50\n",
      "32/32 [==============================] - 0s 819us/step - loss: 0.2370 - mse: 0.2370 - val_loss: 0.2161 - val_mse: 0.2161\n",
      "Epoch 21/50\n",
      "32/32 [==============================] - 0s 796us/step - loss: 0.2367 - mse: 0.2367 - val_loss: 0.2150 - val_mse: 0.2150\n",
      "Epoch 22/50\n",
      "32/32 [==============================] - 0s 774us/step - loss: 0.2366 - mse: 0.2366 - val_loss: 0.2142 - val_mse: 0.2142\n",
      "Epoch 23/50\n",
      "32/32 [==============================] - 0s 801us/step - loss: 0.2365 - mse: 0.2365 - val_loss: 0.2119 - val_mse: 0.2119\n",
      "Epoch 24/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2364 - mse: 0.2364 - val_loss: 0.2122 - val_mse: 0.2122\n",
      "Epoch 25/50\n",
      "32/32 [==============================] - 0s 835us/step - loss: 0.2364 - mse: 0.2364 - val_loss: 0.2108 - val_mse: 0.2108\n",
      "Epoch 26/50\n",
      "32/32 [==============================] - 0s 806us/step - loss: 0.2364 - mse: 0.2364 - val_loss: 0.2102 - val_mse: 0.2102\n",
      "Epoch 27/50\n",
      "32/32 [==============================] - 0s 884us/step - loss: 0.2364 - mse: 0.2364 - val_loss: 0.2114 - val_mse: 0.2114\n",
      "Epoch 28/50\n",
      "32/32 [==============================] - 0s 768us/step - loss: 0.2364 - mse: 0.2364 - val_loss: 0.2111 - val_mse: 0.2111\n",
      "Epoch 29/50\n",
      "32/32 [==============================] - 0s 799us/step - loss: 0.2363 - mse: 0.2363 - val_loss: 0.2108 - val_mse: 0.2108\n",
      "Epoch 30/50\n",
      "32/32 [==============================] - 0s 783us/step - loss: 0.2363 - mse: 0.2363 - val_loss: 0.2110 - val_mse: 0.2110\n",
      "Epoch 31/50\n",
      "32/32 [==============================] - 0s 806us/step - loss: 0.2363 - mse: 0.2363 - val_loss: 0.2107 - val_mse: 0.2107\n",
      "Epoch 32/50\n",
      "32/32 [==============================] - 0s 770us/step - loss: 0.2363 - mse: 0.2363 - val_loss: 0.2102 - val_mse: 0.2102\n",
      "Epoch 33/50\n",
      "32/32 [==============================] - 0s 805us/step - loss: 0.2363 - mse: 0.2363 - val_loss: 0.2103 - val_mse: 0.2103\n",
      "Epoch 34/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2363 - mse: 0.2363 - val_loss: 0.2104 - val_mse: 0.2104\n",
      "Epoch 35/50\n",
      "32/32 [==============================] - 0s 790us/step - loss: 0.2363 - mse: 0.2363 - val_loss: 0.2107 - val_mse: 0.2107\n",
      "Epoch 36/50\n",
      "32/32 [==============================] - 0s 828us/step - loss: 0.2363 - mse: 0.2363 - val_loss: 0.2103 - val_mse: 0.2103\n",
      "Epoch 37/50\n",
      "32/32 [==============================] - 0s 775us/step - loss: 0.2364 - mse: 0.2364 - val_loss: 0.2103 - val_mse: 0.2103\n",
      "Epoch 38/50\n",
      "32/32 [==============================] - 0s 803us/step - loss: 0.2363 - mse: 0.2363 - val_loss: 0.2107 - val_mse: 0.2107\n",
      "Epoch 39/50\n",
      "32/32 [==============================] - 0s 783us/step - loss: 0.2363 - mse: 0.2363 - val_loss: 0.2098 - val_mse: 0.2098\n",
      "Epoch 40/50\n",
      "32/32 [==============================] - 0s 811us/step - loss: 0.2364 - mse: 0.2364 - val_loss: 0.2098 - val_mse: 0.2098\n",
      "Epoch 41/50\n",
      "32/32 [==============================] - 0s 810us/step - loss: 0.2363 - mse: 0.2363 - val_loss: 0.2101 - val_mse: 0.2101\n",
      "Epoch 42/50\n",
      "32/32 [==============================] - 0s 785us/step - loss: 0.2363 - mse: 0.2363 - val_loss: 0.2107 - val_mse: 0.2107\n",
      "Epoch 43/50\n",
      "32/32 [==============================] - 0s 790us/step - loss: 0.2363 - mse: 0.2363 - val_loss: 0.2107 - val_mse: 0.2107\n",
      "Epoch 44/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2364 - mse: 0.2364 - val_loss: 0.2099 - val_mse: 0.2099\n",
      "Epoch 45/50\n",
      "32/32 [==============================] - 0s 803us/step - loss: 0.2364 - mse: 0.2364 - val_loss: 0.2097 - val_mse: 0.2097\n",
      "Epoch 46/50\n",
      "32/32 [==============================] - 0s 778us/step - loss: 0.2363 - mse: 0.2363 - val_loss: 0.2104 - val_mse: 0.2104\n",
      "Epoch 47/50\n",
      "32/32 [==============================] - 0s 772us/step - loss: 0.2364 - mse: 0.2364 - val_loss: 0.2112 - val_mse: 0.2112\n",
      "Epoch 48/50\n",
      "32/32 [==============================] - 0s 783us/step - loss: 0.2364 - mse: 0.2364 - val_loss: 0.2111 - val_mse: 0.2111\n",
      "Epoch 49/50\n",
      "32/32 [==============================] - 0s 792us/step - loss: 0.2363 - mse: 0.2363 - val_loss: 0.2105 - val_mse: 0.2105\n",
      "Epoch 50/50\n",
      "32/32 [==============================] - 0s 794us/step - loss: 0.2363 - mse: 0.2363 - val_loss: 0.2097 - val_mse: 0.2097\n",
      "Epoch 1/50\n",
      "47/47 [==============================] - 0s 2ms/step - loss: 0.8372 - mse: 0.8372 - val_loss: 0.5247 - val_mse: 0.5247\n",
      "Epoch 2/50\n",
      "47/47 [==============================] - 0s 653us/step - loss: 0.3813 - mse: 0.3813 - val_loss: 0.3268 - val_mse: 0.3268\n",
      "Epoch 3/50\n",
      "47/47 [==============================] - 0s 639us/step - loss: 0.2869 - mse: 0.2869 - val_loss: 0.2788 - val_mse: 0.2788\n",
      "Epoch 4/50\n",
      "47/47 [==============================] - 0s 679us/step - loss: 0.2590 - mse: 0.2590 - val_loss: 0.2614 - val_mse: 0.2614\n",
      "Epoch 5/50\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.2476 - mse: 0.2476 - val_loss: 0.2530 - val_mse: 0.2530\n",
      "Epoch 6/50\n",
      "47/47 [==============================] - 0s 639us/step - loss: 0.2419 - mse: 0.2419 - val_loss: 0.2492 - val_mse: 0.2492\n",
      "Epoch 7/50\n",
      "47/47 [==============================] - 0s 608us/step - loss: 0.2393 - mse: 0.2393 - val_loss: 0.2460 - val_mse: 0.2460\n",
      "Epoch 8/50\n",
      "47/47 [==============================] - 0s 625us/step - loss: 0.2378 - mse: 0.2378 - val_loss: 0.2445 - val_mse: 0.2445\n",
      "Epoch 9/50\n",
      "47/47 [==============================] - 0s 650us/step - loss: 0.2371 - mse: 0.2371 - val_loss: 0.2436 - val_mse: 0.2436\n",
      "Epoch 10/50\n",
      "47/47 [==============================] - 0s 635us/step - loss: 0.2367 - mse: 0.2367 - val_loss: 0.2429 - val_mse: 0.2429\n",
      "Epoch 11/50\n",
      "47/47 [==============================] - 0s 643us/step - loss: 0.2366 - mse: 0.2366 - val_loss: 0.2426 - val_mse: 0.2426\n",
      "Epoch 12/50\n",
      "47/47 [==============================] - 0s 654us/step - loss: 0.2364 - mse: 0.2364 - val_loss: 0.2424 - val_mse: 0.2424\n",
      "Epoch 13/50\n",
      "47/47 [==============================] - 0s 636us/step - loss: 0.2364 - mse: 0.2364 - val_loss: 0.2419 - val_mse: 0.2419\n",
      "Epoch 14/50\n",
      "47/47 [==============================] - 0s 640us/step - loss: 0.2364 - mse: 0.2364 - val_loss: 0.2414 - val_mse: 0.2414\n",
      "Epoch 15/50\n",
      "47/47 [==============================] - 0s 701us/step - loss: 0.2363 - mse: 0.2363 - val_loss: 0.2414 - val_mse: 0.2414\n",
      "Epoch 16/50\n",
      "47/47 [==============================] - 0s 2ms/step - loss: 0.2364 - mse: 0.2364 - val_loss: 0.2412 - val_mse: 0.2412\n",
      "Epoch 17/50\n",
      "47/47 [==============================] - 0s 655us/step - loss: 0.2364 - mse: 0.2364 - val_loss: 0.2414 - val_mse: 0.2414\n",
      "Epoch 18/50\n",
      "47/47 [==============================] - 0s 616us/step - loss: 0.2363 - mse: 0.2363 - val_loss: 0.2415 - val_mse: 0.2415\n",
      "Epoch 19/50\n",
      "47/47 [==============================] - 0s 650us/step - loss: 0.2363 - mse: 0.2363 - val_loss: 0.2422 - val_mse: 0.2422\n",
      "Epoch 20/50\n",
      "47/47 [==============================] - 0s 636us/step - loss: 0.2363 - mse: 0.2363 - val_loss: 0.2416 - val_mse: 0.2416\n",
      "Epoch 21/50\n",
      "47/47 [==============================] - 0s 640us/step - loss: 0.2363 - mse: 0.2363 - val_loss: 0.2418 - val_mse: 0.2418\n",
      "Epoch 22/50\n",
      "47/47 [==============================] - 0s 647us/step - loss: 0.2363 - mse: 0.2363 - val_loss: 0.2410 - val_mse: 0.2410\n",
      "Epoch 23/50\n",
      "47/47 [==============================] - 0s 641us/step - loss: 0.2363 - mse: 0.2363 - val_loss: 0.2418 - val_mse: 0.2418\n",
      "Epoch 24/50\n",
      "47/47 [==============================] - 0s 622us/step - loss: 0.2363 - mse: 0.2363 - val_loss: 0.2418 - val_mse: 0.2418\n",
      "Epoch 25/50\n",
      "47/47 [==============================] - 0s 634us/step - loss: 0.2363 - mse: 0.2363 - val_loss: 0.2416 - val_mse: 0.2416\n",
      "Epoch 26/50\n",
      "47/47 [==============================] - 0s 639us/step - loss: 0.2363 - mse: 0.2363 - val_loss: 0.2415 - val_mse: 0.2415\n",
      "Epoch 27/50\n",
      "47/47 [==============================] - 0s 2ms/step - loss: 0.2363 - mse: 0.2363 - val_loss: 0.2417 - val_mse: 0.2417\n",
      "Epoch 28/50\n",
      "47/47 [==============================] - 0s 669us/step - loss: 0.2363 - mse: 0.2363 - val_loss: 0.2417 - val_mse: 0.2417\n",
      "Epoch 29/50\n",
      "47/47 [==============================] - 0s 646us/step - loss: 0.2363 - mse: 0.2363 - val_loss: 0.2414 - val_mse: 0.2414\n",
      "Epoch 30/50\n",
      "47/47 [==============================] - 0s 658us/step - loss: 0.2363 - mse: 0.2363 - val_loss: 0.2419 - val_mse: 0.2419\n",
      "Epoch 31/50\n",
      "47/47 [==============================] - 0s 657us/step - loss: 0.2363 - mse: 0.2363 - val_loss: 0.2417 - val_mse: 0.2417\n",
      "Epoch 32/50\n",
      "47/47 [==============================] - 0s 632us/step - loss: 0.2364 - mse: 0.2364 - val_loss: 0.2414 - val_mse: 0.2414\n",
      "Epoch 33/50\n",
      "47/47 [==============================] - 0s 622us/step - loss: 0.2363 - mse: 0.2363 - val_loss: 0.2411 - val_mse: 0.2411\n",
      "Epoch 34/50\n",
      "47/47 [==============================] - 0s 623us/step - loss: 0.2363 - mse: 0.2363 - val_loss: 0.2412 - val_mse: 0.2412\n",
      "Epoch 35/50\n",
      "47/47 [==============================] - 0s 645us/step - loss: 0.2362 - mse: 0.2362 - val_loss: 0.2407 - val_mse: 0.2407\n",
      "Epoch 36/50\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.2363 - mse: 0.2363 - val_loss: 0.2410 - val_mse: 0.2410\n",
      "Epoch 37/50\n",
      "47/47 [==============================] - 0s 660us/step - loss: 0.2363 - mse: 0.2363 - val_loss: 0.2412 - val_mse: 0.2412\n",
      "Epoch 38/50\n",
      "47/47 [==============================] - 0s 618us/step - loss: 0.2363 - mse: 0.2363 - val_loss: 0.2415 - val_mse: 0.2415\n",
      "Epoch 39/50\n",
      "47/47 [==============================] - 0s 637us/step - loss: 0.2363 - mse: 0.2363 - val_loss: 0.2417 - val_mse: 0.2417\n",
      "Epoch 40/50\n",
      "47/47 [==============================] - 0s 644us/step - loss: 0.2363 - mse: 0.2363 - val_loss: 0.2422 - val_mse: 0.2422\n",
      "Epoch 41/50\n",
      "47/47 [==============================] - 0s 708us/step - loss: 0.2363 - mse: 0.2363 - val_loss: 0.2417 - val_mse: 0.2417\n",
      "Epoch 42/50\n",
      "47/47 [==============================] - 0s 645us/step - loss: 0.2363 - mse: 0.2363 - val_loss: 0.2413 - val_mse: 0.2413\n",
      "Epoch 43/50\n",
      "47/47 [==============================] - 0s 639us/step - loss: 0.2363 - mse: 0.2363 - val_loss: 0.2415 - val_mse: 0.2415\n",
      "Epoch 44/50\n",
      "47/47 [==============================] - 0s 2ms/step - loss: 0.2364 - mse: 0.2364 - val_loss: 0.2413 - val_mse: 0.2413\n",
      "Epoch 45/50\n",
      "47/47 [==============================] - 0s 652us/step - loss: 0.2363 - mse: 0.2363 - val_loss: 0.2414 - val_mse: 0.2414\n",
      "Epoch 46/50\n",
      "47/47 [==============================] - 0s 635us/step - loss: 0.2363 - mse: 0.2363 - val_loss: 0.2409 - val_mse: 0.2409\n",
      "Epoch 47/50\n",
      "47/47 [==============================] - 0s 652us/step - loss: 0.2363 - mse: 0.2363 - val_loss: 0.2409 - val_mse: 0.2409\n",
      "Epoch 48/50\n",
      "47/47 [==============================] - 0s 648us/step - loss: 0.2363 - mse: 0.2363 - val_loss: 0.2409 - val_mse: 0.2409\n",
      "Epoch 49/50\n",
      "47/47 [==============================] - 0s 634us/step - loss: 0.2363 - mse: 0.2363 - val_loss: 0.2412 - val_mse: 0.2412\n",
      "Epoch 50/50\n",
      "47/47 [==============================] - 0s 637us/step - loss: 0.2363 - mse: 0.2363 - val_loss: 0.2410 - val_mse: 0.2410\n",
      "Epoch 1/50\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.0978 - mse: 1.0978 - val_loss: 0.5900 - val_mse: 0.5900\n",
      "Epoch 2/50\n",
      "63/63 [==============================] - 0s 563us/step - loss: 0.4707 - mse: 0.4707 - val_loss: 0.3778 - val_mse: 0.3778\n",
      "Epoch 3/50\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.3263 - mse: 0.3263 - val_loss: 0.3014 - val_mse: 0.3014\n",
      "Epoch 4/50\n",
      "63/63 [==============================] - 0s 548us/step - loss: 0.2750 - mse: 0.2750 - val_loss: 0.2682 - val_mse: 0.2682\n",
      "Epoch 5/50\n",
      "63/63 [==============================] - 0s 539us/step - loss: 0.2546 - mse: 0.2546 - val_loss: 0.2530 - val_mse: 0.2530\n",
      "Epoch 6/50\n",
      "63/63 [==============================] - 0s 596us/step - loss: 0.2462 - mse: 0.2462 - val_loss: 0.2456 - val_mse: 0.2456\n",
      "Epoch 7/50\n",
      "63/63 [==============================] - 0s 540us/step - loss: 0.2426 - mse: 0.2426 - val_loss: 0.2420 - val_mse: 0.2420\n",
      "Epoch 8/50\n",
      "63/63 [==============================] - 0s 531us/step - loss: 0.2412 - mse: 0.2412 - val_loss: 0.2401 - val_mse: 0.2401\n",
      "Epoch 9/50\n",
      "63/63 [==============================] - 0s 541us/step - loss: 0.2405 - mse: 0.2405 - val_loss: 0.2391 - val_mse: 0.2391\n",
      "Epoch 10/50\n",
      "63/63 [==============================] - 0s 550us/step - loss: 0.2402 - mse: 0.2402 - val_loss: 0.2385 - val_mse: 0.2385\n",
      "Epoch 11/50\n",
      "63/63 [==============================] - 0s 548us/step - loss: 0.2400 - mse: 0.2400 - val_loss: 0.2382 - val_mse: 0.2382\n",
      "Epoch 12/50\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.2400 - mse: 0.2400 - val_loss: 0.2380 - val_mse: 0.2380\n",
      "Epoch 13/50\n",
      "63/63 [==============================] - 0s 546us/step - loss: 0.2400 - mse: 0.2400 - val_loss: 0.2379 - val_mse: 0.2379\n",
      "Epoch 14/50\n",
      "63/63 [==============================] - 0s 541us/step - loss: 0.2400 - mse: 0.2400 - val_loss: 0.2378 - val_mse: 0.2378\n",
      "Epoch 15/50\n",
      "63/63 [==============================] - 0s 545us/step - loss: 0.2400 - mse: 0.2400 - val_loss: 0.2377 - val_mse: 0.2377\n",
      "Epoch 16/50\n",
      "63/63 [==============================] - 0s 558us/step - loss: 0.2400 - mse: 0.2400 - val_loss: 0.2378 - val_mse: 0.2378\n",
      "Epoch 17/50\n",
      "63/63 [==============================] - 0s 550us/step - loss: 0.2399 - mse: 0.2399 - val_loss: 0.2378 - val_mse: 0.2378\n",
      "Epoch 18/50\n",
      "63/63 [==============================] - 0s 542us/step - loss: 0.2400 - mse: 0.2400 - val_loss: 0.2378 - val_mse: 0.2378\n",
      "Epoch 19/50\n",
      "63/63 [==============================] - 0s 559us/step - loss: 0.2400 - mse: 0.2400 - val_loss: 0.2378 - val_mse: 0.2378\n",
      "Epoch 20/50\n",
      "63/63 [==============================] - 0s 552us/step - loss: 0.2400 - mse: 0.2400 - val_loss: 0.2377 - val_mse: 0.2377\n",
      "Epoch 21/50\n",
      "63/63 [==============================] - 0s 581us/step - loss: 0.2401 - mse: 0.2401 - val_loss: 0.2377 - val_mse: 0.2377\n",
      "Epoch 22/50\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.2400 - mse: 0.2400 - val_loss: 0.2376 - val_mse: 0.2376\n",
      "Epoch 23/50\n",
      "63/63 [==============================] - 0s 558us/step - loss: 0.2400 - mse: 0.2400 - val_loss: 0.2376 - val_mse: 0.2376\n",
      "Epoch 24/50\n",
      "63/63 [==============================] - 0s 546us/step - loss: 0.2400 - mse: 0.2400 - val_loss: 0.2377 - val_mse: 0.2377\n",
      "Epoch 25/50\n",
      "63/63 [==============================] - 0s 532us/step - loss: 0.2401 - mse: 0.2401 - val_loss: 0.2377 - val_mse: 0.2377\n",
      "Epoch 26/50\n",
      "63/63 [==============================] - 0s 552us/step - loss: 0.2400 - mse: 0.2400 - val_loss: 0.2376 - val_mse: 0.2376\n",
      "Epoch 27/50\n",
      "63/63 [==============================] - 0s 544us/step - loss: 0.2400 - mse: 0.2400 - val_loss: 0.2377 - val_mse: 0.2377\n",
      "Epoch 28/50\n",
      "63/63 [==============================] - 0s 538us/step - loss: 0.2400 - mse: 0.2400 - val_loss: 0.2376 - val_mse: 0.2376\n",
      "Epoch 29/50\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.2400 - mse: 0.2400 - val_loss: 0.2376 - val_mse: 0.2376\n",
      "Epoch 30/50\n",
      "63/63 [==============================] - 0s 551us/step - loss: 0.2400 - mse: 0.2400 - val_loss: 0.2377 - val_mse: 0.2377\n",
      "Epoch 31/50\n",
      "63/63 [==============================] - 0s 557us/step - loss: 0.2400 - mse: 0.2400 - val_loss: 0.2376 - val_mse: 0.2376\n",
      "Epoch 32/50\n",
      "63/63 [==============================] - 0s 552us/step - loss: 0.2400 - mse: 0.2400 - val_loss: 0.2377 - val_mse: 0.2377\n",
      "Epoch 33/50\n",
      "63/63 [==============================] - 0s 561us/step - loss: 0.2400 - mse: 0.2400 - val_loss: 0.2377 - val_mse: 0.2377\n",
      "Epoch 34/50\n",
      "63/63 [==============================] - 0s 544us/step - loss: 0.2400 - mse: 0.2400 - val_loss: 0.2376 - val_mse: 0.2376\n",
      "Epoch 35/50\n",
      "63/63 [==============================] - 0s 560us/step - loss: 0.2400 - mse: 0.2400 - val_loss: 0.2377 - val_mse: 0.2377\n",
      "Epoch 36/50\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.2400 - mse: 0.2400 - val_loss: 0.2376 - val_mse: 0.2376\n",
      "Epoch 37/50\n",
      "63/63 [==============================] - 0s 539us/step - loss: 0.2400 - mse: 0.2400 - val_loss: 0.2376 - val_mse: 0.2376\n",
      "Epoch 38/50\n",
      "63/63 [==============================] - 0s 532us/step - loss: 0.2401 - mse: 0.2401 - val_loss: 0.2377 - val_mse: 0.2377\n",
      "Epoch 39/50\n",
      "63/63 [==============================] - 0s 540us/step - loss: 0.2400 - mse: 0.2400 - val_loss: 0.2378 - val_mse: 0.2378\n",
      "Epoch 40/50\n",
      "63/63 [==============================] - 0s 542us/step - loss: 0.2400 - mse: 0.2400 - val_loss: 0.2377 - val_mse: 0.2377\n",
      "Epoch 41/50\n",
      "63/63 [==============================] - 0s 564us/step - loss: 0.2400 - mse: 0.2400 - val_loss: 0.2377 - val_mse: 0.2377\n",
      "Epoch 42/50\n",
      "63/63 [==============================] - 0s 535us/step - loss: 0.2400 - mse: 0.2400 - val_loss: 0.2376 - val_mse: 0.2376\n",
      "Epoch 43/50\n",
      "63/63 [==============================] - 0s 549us/step - loss: 0.2400 - mse: 0.2400 - val_loss: 0.2378 - val_mse: 0.2378\n",
      "Epoch 44/50\n",
      "63/63 [==============================] - 0s 548us/step - loss: 0.2400 - mse: 0.2400 - val_loss: 0.2378 - val_mse: 0.2378\n",
      "Epoch 45/50\n",
      "63/63 [==============================] - 0s 556us/step - loss: 0.2400 - mse: 0.2400 - val_loss: 0.2376 - val_mse: 0.2376\n",
      "Epoch 46/50\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.2400 - mse: 0.2400 - val_loss: 0.2378 - val_mse: 0.2378\n",
      "Epoch 47/50\n",
      "63/63 [==============================] - 0s 539us/step - loss: 0.2400 - mse: 0.2400 - val_loss: 0.2378 - val_mse: 0.2378\n",
      "Epoch 48/50\n",
      "63/63 [==============================] - 0s 567us/step - loss: 0.2400 - mse: 0.2400 - val_loss: 0.2377 - val_mse: 0.2377\n",
      "Epoch 49/50\n",
      "63/63 [==============================] - 0s 541us/step - loss: 0.2400 - mse: 0.2400 - val_loss: 0.2377 - val_mse: 0.2377\n",
      "Epoch 50/50\n",
      "63/63 [==============================] - 0s 535us/step - loss: 0.2400 - mse: 0.2400 - val_loss: 0.2377 - val_mse: 0.2377\n",
      "Epoch 1/10\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2.1110 - mse: 2.1110 - val_loss: 2.4100 - val_mse: 2.4100\n",
      "Epoch 2/10\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.7245 - mse: 1.7245 - val_loss: 1.9948 - val_mse: 1.9948\n",
      "Epoch 3/10\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.4909 - mse: 1.4909 - val_loss: 1.7275 - val_mse: 1.7275\n",
      "Epoch 4/10\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.3474 - mse: 1.3474 - val_loss: 1.5526 - val_mse: 1.5526\n",
      "Epoch 5/10\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.2533 - mse: 1.2533 - val_loss: 1.4288 - val_mse: 1.4288\n",
      "Epoch 6/10\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1.1873 - mse: 1.1873 - val_loss: 1.3431 - val_mse: 1.3431\n",
      "Epoch 7/10\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1403 - mse: 1.1403 - val_loss: 1.2855 - val_mse: 1.2855\n",
      "Epoch 8/10\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1058 - mse: 1.1058 - val_loss: 1.2392 - val_mse: 1.2392\n",
      "Epoch 9/10\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0791 - mse: 1.0791 - val_loss: 1.2049 - val_mse: 1.2049\n",
      "Epoch 10/10\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0585 - mse: 1.0585 - val_loss: 1.1782 - val_mse: 1.1782\n",
      "Epoch 1/10\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 4.8204 - mse: 4.8204 - val_loss: 4.2385 - val_mse: 4.2385\n",
      "Epoch 2/10\n",
      "32/32 [==============================] - 0s 793us/step - loss: 3.2554 - mse: 3.2554 - val_loss: 3.0732 - val_mse: 3.0732\n",
      "Epoch 3/10\n",
      "32/32 [==============================] - 0s 806us/step - loss: 2.3800 - mse: 2.3800 - val_loss: 2.3594 - val_mse: 2.3594\n",
      "Epoch 4/10\n",
      "32/32 [==============================] - 0s 768us/step - loss: 1.8587 - mse: 1.8587 - val_loss: 1.8959 - val_mse: 1.8959\n",
      "Epoch 5/10\n",
      "32/32 [==============================] - 0s 773us/step - loss: 1.5363 - mse: 1.5363 - val_loss: 1.5858 - val_mse: 1.5858\n",
      "Epoch 6/10\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.3313 - mse: 1.3313 - val_loss: 1.3637 - val_mse: 1.3637\n",
      "Epoch 7/10\n",
      "32/32 [==============================] - 0s 767us/step - loss: 1.1928 - mse: 1.1928 - val_loss: 1.2134 - val_mse: 1.2134\n",
      "Epoch 8/10\n",
      "32/32 [==============================] - 0s 780us/step - loss: 1.1059 - mse: 1.1059 - val_loss: 1.1151 - val_mse: 1.1151\n",
      "Epoch 9/10\n",
      "32/32 [==============================] - 0s 765us/step - loss: 1.0507 - mse: 1.0507 - val_loss: 1.0431 - val_mse: 1.0431\n",
      "Epoch 10/10\n",
      "32/32 [==============================] - 0s 795us/step - loss: 1.0117 - mse: 1.0117 - val_loss: 0.9821 - val_mse: 0.9821\n",
      "Epoch 1/10\n",
      "47/47 [==============================] - 0s 2ms/step - loss: 1.5884 - mse: 1.5884 - val_loss: 1.2638 - val_mse: 1.2638\n",
      "Epoch 2/10\n",
      "47/47 [==============================] - 0s 649us/step - loss: 1.1134 - mse: 1.1134 - val_loss: 1.0691 - val_mse: 1.0691\n",
      "Epoch 3/10\n",
      "47/47 [==============================] - 0s 626us/step - loss: 1.0087 - mse: 1.0087 - val_loss: 1.0184 - val_mse: 1.0184\n",
      "Epoch 4/10\n",
      "47/47 [==============================] - 0s 619us/step - loss: 0.9748 - mse: 0.9748 - val_loss: 0.9984 - val_mse: 0.9984\n",
      "Epoch 5/10\n",
      "47/47 [==============================] - 0s 608us/step - loss: 0.9602 - mse: 0.9602 - val_loss: 0.9861 - val_mse: 0.9861\n",
      "Epoch 6/10\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.9530 - mse: 0.9530 - val_loss: 0.9773 - val_mse: 0.9773\n",
      "Epoch 7/10\n",
      "47/47 [==============================] - 0s 640us/step - loss: 0.9494 - mse: 0.9494 - val_loss: 0.9737 - val_mse: 0.9737\n",
      "Epoch 8/10\n",
      "47/47 [==============================] - 0s 609us/step - loss: 0.9472 - mse: 0.9472 - val_loss: 0.9703 - val_mse: 0.9703\n",
      "Epoch 9/10\n",
      "47/47 [==============================] - 0s 636us/step - loss: 0.9462 - mse: 0.9462 - val_loss: 0.9678 - val_mse: 0.9678\n",
      "Epoch 10/10\n",
      "47/47 [==============================] - 0s 615us/step - loss: 0.9459 - mse: 0.9459 - val_loss: 0.9684 - val_mse: 0.9684\n",
      "Epoch 1/10\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 2.9892 - mse: 2.9892 - val_loss: 2.0536 - val_mse: 2.0536\n",
      "Epoch 2/10\n",
      "63/63 [==============================] - 0s 556us/step - loss: 1.6751 - mse: 1.6751 - val_loss: 1.4308 - val_mse: 1.4308\n",
      "Epoch 3/10\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.2447 - mse: 1.2447 - val_loss: 1.1732 - val_mse: 1.1732\n",
      "Epoch 4/10\n",
      "63/63 [==============================] - 0s 543us/step - loss: 1.0780 - mse: 1.0780 - val_loss: 1.0552 - val_mse: 1.0552\n",
      "Epoch 5/10\n",
      "63/63 [==============================] - 0s 562us/step - loss: 1.0091 - mse: 1.0091 - val_loss: 1.0032 - val_mse: 1.0032\n",
      "Epoch 6/10\n",
      "63/63 [==============================] - 0s 543us/step - loss: 0.9810 - mse: 0.9810 - val_loss: 0.9777 - val_mse: 0.9777\n",
      "Epoch 7/10\n",
      "63/63 [==============================] - 0s 544us/step - loss: 0.9686 - mse: 0.9686 - val_loss: 0.9650 - val_mse: 0.9650\n",
      "Epoch 8/10\n",
      "63/63 [==============================] - 0s 535us/step - loss: 0.9630 - mse: 0.9630 - val_loss: 0.9585 - val_mse: 0.9585\n",
      "Epoch 9/10\n",
      "63/63 [==============================] - 0s 533us/step - loss: 0.9612 - mse: 0.9612 - val_loss: 0.9550 - val_mse: 0.9550\n",
      "Epoch 10/10\n",
      "63/63 [==============================] - 0s 537us/step - loss: 0.9607 - mse: 0.9607 - val_loss: 0.9532 - val_mse: 0.9532\n",
      "Epoch 1/15\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 2.0864 - mse: 2.0864 - val_loss: 2.4010 - val_mse: 2.4010\n",
      "Epoch 2/15\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.7080 - mse: 1.7080 - val_loss: 1.9910 - val_mse: 1.9910\n",
      "Epoch 3/15\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.4780 - mse: 1.4780 - val_loss: 1.7261 - val_mse: 1.7261\n",
      "Epoch 4/15\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.3357 - mse: 1.3357 - val_loss: 1.5504 - val_mse: 1.5504\n",
      "Epoch 5/15\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.2427 - mse: 1.2427 - val_loss: 1.4292 - val_mse: 1.4292\n",
      "Epoch 6/15\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1785 - mse: 1.1785 - val_loss: 1.3422 - val_mse: 1.3422\n",
      "Epoch 7/15\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1331 - mse: 1.1331 - val_loss: 1.2815 - val_mse: 1.2815\n",
      "Epoch 8/15\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1002 - mse: 1.1002 - val_loss: 1.2354 - val_mse: 1.2354\n",
      "Epoch 9/15\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0735 - mse: 1.0735 - val_loss: 1.2021 - val_mse: 1.2021\n",
      "Epoch 10/15\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0538 - mse: 1.0538 - val_loss: 1.1758 - val_mse: 1.1758\n",
      "Epoch 11/15\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1.0390 - mse: 1.0390 - val_loss: 1.1594 - val_mse: 1.1594\n",
      "Epoch 12/15\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0260 - mse: 1.0260 - val_loss: 1.1465 - val_mse: 1.1465\n",
      "Epoch 13/15\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0163 - mse: 1.0163 - val_loss: 1.1340 - val_mse: 1.1340\n",
      "Epoch 14/15\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0092 - mse: 1.0092 - val_loss: 1.1278 - val_mse: 1.1278\n",
      "Epoch 15/15\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0033 - mse: 1.0033 - val_loss: 1.1247 - val_mse: 1.1247\n",
      "Epoch 1/15\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 4.7293 - mse: 4.7293 - val_loss: 4.1510 - val_mse: 4.1510\n",
      "Epoch 2/15\n",
      "32/32 [==============================] - 0s 825us/step - loss: 3.1949 - mse: 3.1949 - val_loss: 3.0328 - val_mse: 3.0328\n",
      "Epoch 3/15\n",
      "32/32 [==============================] - 0s 790us/step - loss: 2.3476 - mse: 2.3476 - val_loss: 2.3220 - val_mse: 2.3220\n",
      "Epoch 4/15\n",
      "32/32 [==============================] - 0s 775us/step - loss: 1.8302 - mse: 1.8302 - val_loss: 1.8618 - val_mse: 1.8618\n",
      "Epoch 5/15\n",
      "32/32 [==============================] - 0s 791us/step - loss: 1.5106 - mse: 1.5106 - val_loss: 1.5611 - val_mse: 1.5611\n",
      "Epoch 6/15\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.3147 - mse: 1.3147 - val_loss: 1.3513 - val_mse: 1.3513\n",
      "Epoch 7/15\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.1860 - mse: 1.1860 - val_loss: 1.2066 - val_mse: 1.2066\n",
      "Epoch 8/15\n",
      "32/32 [==============================] - 0s 761us/step - loss: 1.1019 - mse: 1.1019 - val_loss: 1.1054 - val_mse: 1.1054\n",
      "Epoch 9/15\n",
      "32/32 [==============================] - 0s 774us/step - loss: 1.0475 - mse: 1.0475 - val_loss: 1.0354 - val_mse: 1.0354\n",
      "Epoch 10/15\n",
      "32/32 [==============================] - 0s 766us/step - loss: 1.0123 - mse: 1.0123 - val_loss: 0.9881 - val_mse: 0.9881\n",
      "Epoch 11/15\n",
      "32/32 [==============================] - 0s 767us/step - loss: 0.9898 - mse: 0.9898 - val_loss: 0.9525 - val_mse: 0.9525\n",
      "Epoch 12/15\n",
      "32/32 [==============================] - 0s 796us/step - loss: 0.9741 - mse: 0.9741 - val_loss: 0.9234 - val_mse: 0.9234\n",
      "Epoch 13/15\n",
      "32/32 [==============================] - 0s 776us/step - loss: 0.9645 - mse: 0.9645 - val_loss: 0.9056 - val_mse: 0.9056\n",
      "Epoch 14/15\n",
      "32/32 [==============================] - 0s 770us/step - loss: 0.9573 - mse: 0.9573 - val_loss: 0.8931 - val_mse: 0.8931\n",
      "Epoch 15/15\n",
      "32/32 [==============================] - 0s 919us/step - loss: 0.9532 - mse: 0.9532 - val_loss: 0.8788 - val_mse: 0.8788\n",
      "Epoch 1/15\n",
      "47/47 [==============================] - 0s 2ms/step - loss: 1.4539 - mse: 1.4539 - val_loss: 1.1243 - val_mse: 1.1243\n",
      "Epoch 2/15\n",
      "47/47 [==============================] - 0s 661us/step - loss: 1.0470 - mse: 1.0470 - val_loss: 1.0027 - val_mse: 1.0027\n",
      "Epoch 3/15\n",
      "47/47 [==============================] - 0s 640us/step - loss: 0.9730 - mse: 0.9730 - val_loss: 0.9831 - val_mse: 0.9831\n",
      "Epoch 4/15\n",
      "47/47 [==============================] - 0s 635us/step - loss: 0.9558 - mse: 0.9558 - val_loss: 0.9777 - val_mse: 0.9777\n",
      "Epoch 5/15\n",
      "47/47 [==============================] - 0s 632us/step - loss: 0.9502 - mse: 0.9502 - val_loss: 0.9749 - val_mse: 0.9749\n",
      "Epoch 6/15\n",
      "47/47 [==============================] - 0s 627us/step - loss: 0.9478 - mse: 0.9478 - val_loss: 0.9720 - val_mse: 0.9720\n",
      "Epoch 7/15\n",
      "47/47 [==============================] - 0s 640us/step - loss: 0.9465 - mse: 0.9465 - val_loss: 0.9712 - val_mse: 0.9712\n",
      "Epoch 8/15\n",
      "47/47 [==============================] - 0s 635us/step - loss: 0.9458 - mse: 0.9458 - val_loss: 0.9678 - val_mse: 0.9678\n",
      "Epoch 9/15\n",
      "47/47 [==============================] - 0s 638us/step - loss: 0.9455 - mse: 0.9455 - val_loss: 0.9683 - val_mse: 0.9683\n",
      "Epoch 10/15\n",
      "47/47 [==============================] - 0s 689us/step - loss: 0.9457 - mse: 0.9457 - val_loss: 0.9683 - val_mse: 0.9683\n",
      "Epoch 11/15\n",
      "47/47 [==============================] - 0s 2ms/step - loss: 0.9454 - mse: 0.9454 - val_loss: 0.9699 - val_mse: 0.9699\n",
      "Epoch 12/15\n",
      "47/47 [==============================] - 0s 634us/step - loss: 0.9453 - mse: 0.9453 - val_loss: 0.9668 - val_mse: 0.9668\n",
      "Epoch 13/15\n",
      "47/47 [==============================] - 0s 638us/step - loss: 0.9454 - mse: 0.9454 - val_loss: 0.9672 - val_mse: 0.9672\n",
      "Epoch 14/15\n",
      "47/47 [==============================] - 0s 626us/step - loss: 0.9451 - mse: 0.9451 - val_loss: 0.9653 - val_mse: 0.9653\n",
      "Epoch 15/15\n",
      "47/47 [==============================] - 0s 646us/step - loss: 0.9454 - mse: 0.9454 - val_loss: 0.9661 - val_mse: 0.9661\n",
      "Epoch 1/15\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 2.8035 - mse: 2.8035 - val_loss: 1.9513 - val_mse: 1.9513\n",
      "Epoch 2/15\n",
      "63/63 [==============================] - 0s 560us/step - loss: 1.6079 - mse: 1.6079 - val_loss: 1.3940 - val_mse: 1.3940\n",
      "Epoch 3/15\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.2205 - mse: 1.2205 - val_loss: 1.1536 - val_mse: 1.1536\n",
      "Epoch 4/15\n",
      "63/63 [==============================] - 0s 546us/step - loss: 1.0666 - mse: 1.0666 - val_loss: 1.0492 - val_mse: 1.0492\n",
      "Epoch 5/15\n",
      "63/63 [==============================] - 0s 531us/step - loss: 1.0048 - mse: 1.0048 - val_loss: 0.9987 - val_mse: 0.9987\n",
      "Epoch 6/15\n",
      "63/63 [==============================] - 0s 546us/step - loss: 0.9784 - mse: 0.9784 - val_loss: 0.9760 - val_mse: 0.9760\n",
      "Epoch 7/15\n",
      "63/63 [==============================] - 0s 561us/step - loss: 0.9678 - mse: 0.9678 - val_loss: 0.9646 - val_mse: 0.9646\n",
      "Epoch 8/15\n",
      "63/63 [==============================] - 0s 539us/step - loss: 0.9633 - mse: 0.9633 - val_loss: 0.9586 - val_mse: 0.9586\n",
      "Epoch 9/15\n",
      "63/63 [==============================] - 0s 536us/step - loss: 0.9613 - mse: 0.9613 - val_loss: 0.9551 - val_mse: 0.9551\n",
      "Epoch 10/15\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.9608 - mse: 0.9608 - val_loss: 0.9534 - val_mse: 0.9534\n",
      "Epoch 11/15\n",
      "63/63 [==============================] - 0s 548us/step - loss: 0.9601 - mse: 0.9601 - val_loss: 0.9521 - val_mse: 0.9521\n",
      "Epoch 12/15\n",
      "63/63 [==============================] - 0s 551us/step - loss: 0.9599 - mse: 0.9599 - val_loss: 0.9518 - val_mse: 0.9518\n",
      "Epoch 13/15\n",
      "63/63 [==============================] - 0s 527us/step - loss: 0.9601 - mse: 0.9601 - val_loss: 0.9512 - val_mse: 0.9512\n",
      "Epoch 14/15\n",
      "63/63 [==============================] - 0s 532us/step - loss: 0.9602 - mse: 0.9602 - val_loss: 0.9511 - val_mse: 0.9511\n",
      "Epoch 15/15\n",
      "63/63 [==============================] - 0s 547us/step - loss: 0.9600 - mse: 0.9600 - val_loss: 0.9505 - val_mse: 0.9505\n",
      "Epoch 1/30\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1.8296 - mse: 1.8296 - val_loss: 2.2302 - val_mse: 2.2302\n",
      "Epoch 2/30\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.4987 - mse: 1.4987 - val_loss: 1.8555 - val_mse: 1.8555\n",
      "Epoch 3/30\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.3110 - mse: 1.3110 - val_loss: 1.6135 - val_mse: 1.6135\n",
      "Epoch 4/30\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.2003 - mse: 1.2003 - val_loss: 1.4574 - val_mse: 1.4574\n",
      "Epoch 5/30\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 1.1333 - mse: 1.1333 - val_loss: 1.3602 - val_mse: 1.3602\n",
      "Epoch 6/30\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0915 - mse: 1.0915 - val_loss: 1.2907 - val_mse: 1.2907\n",
      "Epoch 7/30\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0630 - mse: 1.0630 - val_loss: 1.2362 - val_mse: 1.2362\n",
      "Epoch 8/30\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0422 - mse: 1.0422 - val_loss: 1.1993 - val_mse: 1.1993\n",
      "Epoch 9/30\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0280 - mse: 1.0280 - val_loss: 1.1738 - val_mse: 1.1738\n",
      "Epoch 10/30\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0171 - mse: 1.0171 - val_loss: 1.1545 - val_mse: 1.1545\n",
      "Epoch 11/30\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0091 - mse: 1.0091 - val_loss: 1.1437 - val_mse: 1.1437\n",
      "Epoch 12/30\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1.0022 - mse: 1.0022 - val_loss: 1.1373 - val_mse: 1.1373\n",
      "Epoch 13/30\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.9980 - mse: 0.9980 - val_loss: 1.1334 - val_mse: 1.1334\n",
      "Epoch 14/30\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.9940 - mse: 0.9940 - val_loss: 1.1292 - val_mse: 1.1292\n",
      "Epoch 15/30\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.9912 - mse: 0.9912 - val_loss: 1.1220 - val_mse: 1.1220\n",
      "Epoch 16/30\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.9885 - mse: 0.9885 - val_loss: 1.1206 - val_mse: 1.1206\n",
      "Epoch 17/30\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.9865 - mse: 0.9865 - val_loss: 1.1174 - val_mse: 1.1174\n",
      "Epoch 18/30\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.9852 - mse: 0.9852 - val_loss: 1.1165 - val_mse: 1.1165\n",
      "Epoch 19/30\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.9836 - mse: 0.9836 - val_loss: 1.1167 - val_mse: 1.1167\n",
      "Epoch 20/30\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.9829 - mse: 0.9829 - val_loss: 1.1158 - val_mse: 1.1158\n",
      "Epoch 21/30\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.9821 - mse: 0.9821 - val_loss: 1.1093 - val_mse: 1.1093\n",
      "Epoch 22/30\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.9808 - mse: 0.9808 - val_loss: 1.1100 - val_mse: 1.1100\n",
      "Epoch 23/30\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.9804 - mse: 0.9804 - val_loss: 1.1087 - val_mse: 1.1087\n",
      "Epoch 24/30\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.9800 - mse: 0.9800 - val_loss: 1.1099 - val_mse: 1.1099\n",
      "Epoch 25/30\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.9797 - mse: 0.9797 - val_loss: 1.1129 - val_mse: 1.1129\n",
      "Epoch 26/30\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.9795 - mse: 0.9795 - val_loss: 1.1145 - val_mse: 1.1145\n",
      "Epoch 27/30\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.9793 - mse: 0.9793 - val_loss: 1.1153 - val_mse: 1.1153\n",
      "Epoch 28/30\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.9795 - mse: 0.9795 - val_loss: 1.1154 - val_mse: 1.1154\n",
      "Epoch 29/30\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.9794 - mse: 0.9794 - val_loss: 1.1159 - val_mse: 1.1159\n",
      "Epoch 30/30\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.9792 - mse: 0.9792 - val_loss: 1.1140 - val_mse: 1.1140\n",
      "Epoch 1/30\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3.1410 - mse: 3.1410 - val_loss: 2.6395 - val_mse: 2.6395\n",
      "Epoch 2/30\n",
      "32/32 [==============================] - 0s 800us/step - loss: 2.1442 - mse: 2.1442 - val_loss: 1.9954 - val_mse: 1.9954\n",
      "Epoch 3/30\n",
      "32/32 [==============================] - 0s 782us/step - loss: 1.6562 - mse: 1.6562 - val_loss: 1.6343 - val_mse: 1.6343\n",
      "Epoch 4/30\n",
      "32/32 [==============================] - 0s 805us/step - loss: 1.3893 - mse: 1.3893 - val_loss: 1.4056 - val_mse: 1.4056\n",
      "Epoch 5/30\n",
      "32/32 [==============================] - 0s 768us/step - loss: 1.2307 - mse: 1.2307 - val_loss: 1.2440 - val_mse: 1.2440\n",
      "Epoch 6/30\n",
      "32/32 [==============================] - 0s 781us/step - loss: 1.1297 - mse: 1.1297 - val_loss: 1.1367 - val_mse: 1.1367\n",
      "Epoch 7/30\n",
      "32/32 [==============================] - 0s 754us/step - loss: 1.0641 - mse: 1.0641 - val_loss: 1.0528 - val_mse: 1.0528\n",
      "Epoch 8/30\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.0216 - mse: 1.0216 - val_loss: 0.9942 - val_mse: 0.9942\n",
      "Epoch 9/30\n",
      "32/32 [==============================] - 0s 752us/step - loss: 0.9947 - mse: 0.9947 - val_loss: 0.9559 - val_mse: 0.9559\n",
      "Epoch 10/30\n",
      "32/32 [==============================] - 0s 838us/step - loss: 0.9781 - mse: 0.9781 - val_loss: 0.9273 - val_mse: 0.9273\n",
      "Epoch 11/30\n",
      "32/32 [==============================] - 0s 773us/step - loss: 0.9661 - mse: 0.9661 - val_loss: 0.9057 - val_mse: 0.9057\n",
      "Epoch 12/30\n",
      "32/32 [==============================] - 0s 777us/step - loss: 0.9585 - mse: 0.9585 - val_loss: 0.8972 - val_mse: 0.8972\n",
      "Epoch 13/30\n",
      "32/32 [==============================] - 0s 830us/step - loss: 0.9537 - mse: 0.9537 - val_loss: 0.8787 - val_mse: 0.8787\n",
      "Epoch 14/30\n",
      "32/32 [==============================] - 0s 782us/step - loss: 0.9503 - mse: 0.9503 - val_loss: 0.8677 - val_mse: 0.8677\n",
      "Epoch 15/30\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.9484 - mse: 0.9484 - val_loss: 0.8635 - val_mse: 0.8635\n",
      "Epoch 16/30\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.9475 - mse: 0.9475 - val_loss: 0.8575 - val_mse: 0.8575\n",
      "Epoch 17/30\n",
      "32/32 [==============================] - 0s 768us/step - loss: 0.9466 - mse: 0.9466 - val_loss: 0.8551 - val_mse: 0.8551\n",
      "Epoch 18/30\n",
      "32/32 [==============================] - 0s 832us/step - loss: 0.9459 - mse: 0.9459 - val_loss: 0.8574 - val_mse: 0.8574\n",
      "Epoch 19/30\n",
      "32/32 [==============================] - 0s 780us/step - loss: 0.9458 - mse: 0.9458 - val_loss: 0.8498 - val_mse: 0.8498\n",
      "Epoch 20/30\n",
      "32/32 [==============================] - 0s 756us/step - loss: 0.9459 - mse: 0.9459 - val_loss: 0.8449 - val_mse: 0.8449\n",
      "Epoch 21/30\n",
      "32/32 [==============================] - 0s 797us/step - loss: 0.9454 - mse: 0.9454 - val_loss: 0.8473 - val_mse: 0.8473\n",
      "Epoch 22/30\n",
      "32/32 [==============================] - 0s 759us/step - loss: 0.9454 - mse: 0.9454 - val_loss: 0.8432 - val_mse: 0.8432\n",
      "Epoch 23/30\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.9454 - mse: 0.9454 - val_loss: 0.8418 - val_mse: 0.8418\n",
      "Epoch 24/30\n",
      "32/32 [==============================] - 0s 751us/step - loss: 0.9450 - mse: 0.9450 - val_loss: 0.8454 - val_mse: 0.8454\n",
      "Epoch 25/30\n",
      "32/32 [==============================] - 0s 777us/step - loss: 0.9451 - mse: 0.9451 - val_loss: 0.8424 - val_mse: 0.8424\n",
      "Epoch 26/30\n",
      "32/32 [==============================] - 0s 805us/step - loss: 0.9451 - mse: 0.9451 - val_loss: 0.8410 - val_mse: 0.8410\n",
      "Epoch 27/30\n",
      "32/32 [==============================] - 0s 792us/step - loss: 0.9453 - mse: 0.9453 - val_loss: 0.8440 - val_mse: 0.8440\n",
      "Epoch 28/30\n",
      "32/32 [==============================] - 0s 778us/step - loss: 0.9452 - mse: 0.9452 - val_loss: 0.8461 - val_mse: 0.8461\n",
      "Epoch 29/30\n",
      "32/32 [==============================] - 0s 763us/step - loss: 0.9454 - mse: 0.9454 - val_loss: 0.8433 - val_mse: 0.8433\n",
      "Epoch 30/30\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.9452 - mse: 0.9452 - val_loss: 0.8454 - val_mse: 0.8454\n",
      "Epoch 1/30\n",
      "47/47 [==============================] - 0s 2ms/step - loss: 1.4428 - mse: 1.4428 - val_loss: 1.1091 - val_mse: 1.1091\n",
      "Epoch 2/30\n",
      "47/47 [==============================] - 0s 666us/step - loss: 1.0420 - mse: 1.0420 - val_loss: 0.9930 - val_mse: 0.9930\n",
      "Epoch 3/30\n",
      "47/47 [==============================] - 0s 660us/step - loss: 0.9695 - mse: 0.9695 - val_loss: 0.9774 - val_mse: 0.9774\n",
      "Epoch 4/30\n",
      "47/47 [==============================] - 0s 635us/step - loss: 0.9538 - mse: 0.9538 - val_loss: 0.9752 - val_mse: 0.9752\n",
      "Epoch 5/30\n",
      "47/47 [==============================] - 0s 707us/step - loss: 0.9493 - mse: 0.9493 - val_loss: 0.9713 - val_mse: 0.9713\n",
      "Epoch 6/30\n",
      "47/47 [==============================] - 0s 640us/step - loss: 0.9475 - mse: 0.9475 - val_loss: 0.9683 - val_mse: 0.9683\n",
      "Epoch 7/30\n",
      "47/47 [==============================] - 0s 623us/step - loss: 0.9464 - mse: 0.9464 - val_loss: 0.9683 - val_mse: 0.9683\n",
      "Epoch 8/30\n",
      "47/47 [==============================] - 0s 623us/step - loss: 0.9460 - mse: 0.9460 - val_loss: 0.9671 - val_mse: 0.9671\n",
      "Epoch 9/30\n",
      "47/47 [==============================] - 0s 2ms/step - loss: 0.9455 - mse: 0.9455 - val_loss: 0.9691 - val_mse: 0.9691\n",
      "Epoch 10/30\n",
      "47/47 [==============================] - 0s 630us/step - loss: 0.9453 - mse: 0.9453 - val_loss: 0.9675 - val_mse: 0.9675\n",
      "Epoch 11/30\n",
      "47/47 [==============================] - 0s 657us/step - loss: 0.9454 - mse: 0.9454 - val_loss: 0.9663 - val_mse: 0.9663\n",
      "Epoch 12/30\n",
      "47/47 [==============================] - 0s 628us/step - loss: 0.9451 - mse: 0.9451 - val_loss: 0.9680 - val_mse: 0.9680\n",
      "Epoch 13/30\n",
      "47/47 [==============================] - 0s 609us/step - loss: 0.9451 - mse: 0.9451 - val_loss: 0.9691 - val_mse: 0.9691\n",
      "Epoch 14/30\n",
      "47/47 [==============================] - 0s 679us/step - loss: 0.9451 - mse: 0.9451 - val_loss: 0.9685 - val_mse: 0.9685\n",
      "Epoch 15/30\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.9453 - mse: 0.9453 - val_loss: 0.9676 - val_mse: 0.9676\n",
      "Epoch 16/30\n",
      "47/47 [==============================] - 0s 686us/step - loss: 0.9452 - mse: 0.9452 - val_loss: 0.9677 - val_mse: 0.9677\n",
      "Epoch 17/30\n",
      "47/47 [==============================] - 0s 650us/step - loss: 0.9452 - mse: 0.9452 - val_loss: 0.9665 - val_mse: 0.9665\n",
      "Epoch 18/30\n",
      "47/47 [==============================] - 0s 633us/step - loss: 0.9451 - mse: 0.9451 - val_loss: 0.9663 - val_mse: 0.9663\n",
      "Epoch 19/30\n",
      "47/47 [==============================] - 0s 638us/step - loss: 0.9453 - mse: 0.9453 - val_loss: 0.9645 - val_mse: 0.9645\n",
      "Epoch 20/30\n",
      "47/47 [==============================] - 0s 632us/step - loss: 0.9452 - mse: 0.9452 - val_loss: 0.9649 - val_mse: 0.9649\n",
      "Epoch 21/30\n",
      "47/47 [==============================] - 0s 619us/step - loss: 0.9453 - mse: 0.9453 - val_loss: 0.9646 - val_mse: 0.9646\n",
      "Epoch 22/30\n",
      "47/47 [==============================] - 0s 607us/step - loss: 0.9451 - mse: 0.9451 - val_loss: 0.9649 - val_mse: 0.9649\n",
      "Epoch 23/30\n",
      "47/47 [==============================] - 0s 639us/step - loss: 0.9453 - mse: 0.9453 - val_loss: 0.9645 - val_mse: 0.9645\n",
      "Epoch 24/30\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.9452 - mse: 0.9452 - val_loss: 0.9662 - val_mse: 0.9662\n",
      "Epoch 25/30\n",
      "47/47 [==============================] - 0s 669us/step - loss: 0.9451 - mse: 0.9451 - val_loss: 0.9675 - val_mse: 0.9675\n",
      "Epoch 26/30\n",
      "47/47 [==============================] - 0s 618us/step - loss: 0.9450 - mse: 0.9450 - val_loss: 0.9655 - val_mse: 0.9655\n",
      "Epoch 27/30\n",
      "47/47 [==============================] - 0s 655us/step - loss: 0.9453 - mse: 0.9453 - val_loss: 0.9656 - val_mse: 0.9656\n",
      "Epoch 28/30\n",
      "47/47 [==============================] - 212s 5s/step - loss: 0.9450 - mse: 0.9450 - val_loss: 0.9673 - val_mse: 0.9673\n",
      "Epoch 29/30\n",
      "47/47 [==============================] - 0s 874us/step - loss: 0.9453 - mse: 0.9453 - val_loss: 0.9678 - val_mse: 0.9678\n",
      "Epoch 30/30\n",
      "47/47 [==============================] - 0s 701us/step - loss: 0.9453 - mse: 0.9453 - val_loss: 0.9666 - val_mse: 0.9666\n",
      "Epoch 1/30\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.7100 - mse: 1.7100 - val_loss: 1.2807 - val_mse: 1.2807\n",
      "Epoch 2/30\n",
      "63/63 [==============================] - 0s 633us/step - loss: 1.1503 - mse: 1.1503 - val_loss: 1.0866 - val_mse: 1.0866\n",
      "Epoch 3/30\n",
      "63/63 [==============================] - 0s 540us/step - loss: 1.0292 - mse: 1.0292 - val_loss: 1.0167 - val_mse: 1.0167\n",
      "Epoch 4/30\n",
      "63/63 [==============================] - 0s 553us/step - loss: 0.9882 - mse: 0.9882 - val_loss: 0.9849 - val_mse: 0.9849\n",
      "Epoch 5/30\n",
      "63/63 [==============================] - 0s 592us/step - loss: 0.9718 - mse: 0.9718 - val_loss: 0.9692 - val_mse: 0.9692\n",
      "Epoch 6/30\n",
      "63/63 [==============================] - 0s 562us/step - loss: 0.9650 - mse: 0.9650 - val_loss: 0.9607 - val_mse: 0.9607\n",
      "Epoch 7/30\n",
      "63/63 [==============================] - 0s 614us/step - loss: 0.9623 - mse: 0.9623 - val_loss: 0.9568 - val_mse: 0.9568\n",
      "Epoch 8/30\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.9609 - mse: 0.9609 - val_loss: 0.9538 - val_mse: 0.9538\n",
      "Epoch 9/30\n",
      "63/63 [==============================] - 0s 545us/step - loss: 0.9605 - mse: 0.9605 - val_loss: 0.9528 - val_mse: 0.9528\n",
      "Epoch 10/30\n",
      "63/63 [==============================] - 0s 561us/step - loss: 0.9601 - mse: 0.9601 - val_loss: 0.9519 - val_mse: 0.9519\n",
      "Epoch 11/30\n",
      "63/63 [==============================] - 0s 553us/step - loss: 0.9600 - mse: 0.9600 - val_loss: 0.9515 - val_mse: 0.9515\n",
      "Epoch 12/30\n",
      "63/63 [==============================] - 0s 556us/step - loss: 0.9601 - mse: 0.9601 - val_loss: 0.9508 - val_mse: 0.9508\n",
      "Epoch 13/30\n",
      "63/63 [==============================] - 0s 559us/step - loss: 0.9603 - mse: 0.9603 - val_loss: 0.9507 - val_mse: 0.9507\n",
      "Epoch 14/30\n",
      "63/63 [==============================] - 0s 542us/step - loss: 0.9599 - mse: 0.9599 - val_loss: 0.9504 - val_mse: 0.9504\n",
      "Epoch 15/30\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.9600 - mse: 0.9600 - val_loss: 0.9503 - val_mse: 0.9503\n",
      "Epoch 16/30\n",
      "63/63 [==============================] - 0s 535us/step - loss: 0.9600 - mse: 0.9600 - val_loss: 0.9505 - val_mse: 0.9505\n",
      "Epoch 17/30\n",
      "63/63 [==============================] - 0s 540us/step - loss: 0.9600 - mse: 0.9600 - val_loss: 0.9504 - val_mse: 0.9504\n",
      "Epoch 18/30\n",
      "63/63 [==============================] - 0s 541us/step - loss: 0.9597 - mse: 0.9597 - val_loss: 0.9506 - val_mse: 0.9506\n",
      "Epoch 19/30\n",
      "63/63 [==============================] - 0s 526us/step - loss: 0.9601 - mse: 0.9601 - val_loss: 0.9503 - val_mse: 0.9503\n",
      "Epoch 20/30\n",
      "63/63 [==============================] - 0s 560us/step - loss: 0.9599 - mse: 0.9599 - val_loss: 0.9507 - val_mse: 0.9507\n",
      "Epoch 21/30\n",
      "63/63 [==============================] - 0s 595us/step - loss: 0.9602 - mse: 0.9602 - val_loss: 0.9504 - val_mse: 0.9504\n",
      "Epoch 22/30\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.9600 - mse: 0.9600 - val_loss: 0.9506 - val_mse: 0.9506\n",
      "Epoch 23/30\n",
      "63/63 [==============================] - 0s 585us/step - loss: 0.9600 - mse: 0.9600 - val_loss: 0.9505 - val_mse: 0.9505\n",
      "Epoch 24/30\n",
      "63/63 [==============================] - 0s 565us/step - loss: 0.9600 - mse: 0.9600 - val_loss: 0.9506 - val_mse: 0.9506\n",
      "Epoch 25/30\n",
      "63/63 [==============================] - 0s 561us/step - loss: 0.9600 - mse: 0.9600 - val_loss: 0.9502 - val_mse: 0.9502\n",
      "Epoch 26/30\n",
      "63/63 [==============================] - 0s 559us/step - loss: 0.9601 - mse: 0.9601 - val_loss: 0.9504 - val_mse: 0.9504\n",
      "Epoch 27/30\n",
      "63/63 [==============================] - 0s 634us/step - loss: 0.9601 - mse: 0.9601 - val_loss: 0.9504 - val_mse: 0.9504\n",
      "Epoch 28/30\n",
      "63/63 [==============================] - 0s 654us/step - loss: 0.9600 - mse: 0.9600 - val_loss: 0.9503 - val_mse: 0.9503\n",
      "Epoch 29/30\n",
      "63/63 [==============================] - 0s 575us/step - loss: 0.9601 - mse: 0.9601 - val_loss: 0.9502 - val_mse: 0.9502\n",
      "Epoch 30/30\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.9599 - mse: 0.9599 - val_loss: 0.9500 - val_mse: 0.9500\n",
      "Epoch 1/50\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1.9170 - mse: 1.9170 - val_loss: 2.2848 - val_mse: 2.2848\n",
      "Epoch 2/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.5690 - mse: 1.5690 - val_loss: 1.8886 - val_mse: 1.8886\n",
      "Epoch 3/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.3650 - mse: 1.3650 - val_loss: 1.6384 - val_mse: 1.6384\n",
      "Epoch 4/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.2445 - mse: 1.2445 - val_loss: 1.4831 - val_mse: 1.4831\n",
      "Epoch 5/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1709 - mse: 1.1709 - val_loss: 1.3795 - val_mse: 1.3795\n",
      "Epoch 6/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1224 - mse: 1.1224 - val_loss: 1.2999 - val_mse: 1.2999\n",
      "Epoch 7/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0874 - mse: 1.0874 - val_loss: 1.2469 - val_mse: 1.2469\n",
      "Epoch 8/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.0642 - mse: 1.0642 - val_loss: 1.2059 - val_mse: 1.2059\n",
      "Epoch 9/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.0438 - mse: 1.0438 - val_loss: 1.1778 - val_mse: 1.1778\n",
      "Epoch 10/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.0308 - mse: 1.0308 - val_loss: 1.1592 - val_mse: 1.1592\n",
      "Epoch 11/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0195 - mse: 1.0195 - val_loss: 1.1455 - val_mse: 1.1455\n",
      "Epoch 12/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0115 - mse: 1.0115 - val_loss: 1.1373 - val_mse: 1.1373\n",
      "Epoch 13/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0047 - mse: 1.0047 - val_loss: 1.1279 - val_mse: 1.1279\n",
      "Epoch 14/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.9996 - mse: 0.9996 - val_loss: 1.1236 - val_mse: 1.1236\n",
      "Epoch 15/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.9957 - mse: 0.9957 - val_loss: 1.1201 - val_mse: 1.1201\n",
      "Epoch 16/50\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.9918 - mse: 0.9918 - val_loss: 1.1172 - val_mse: 1.1172\n",
      "Epoch 17/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.9897 - mse: 0.9897 - val_loss: 1.1135 - val_mse: 1.1135\n",
      "Epoch 18/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.9871 - mse: 0.9871 - val_loss: 1.1084 - val_mse: 1.1084\n",
      "Epoch 19/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.9858 - mse: 0.9858 - val_loss: 1.1092 - val_mse: 1.1092\n",
      "Epoch 20/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.9841 - mse: 0.9841 - val_loss: 1.1055 - val_mse: 1.1055\n",
      "Epoch 21/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.9831 - mse: 0.9831 - val_loss: 1.1060 - val_mse: 1.1060\n",
      "Epoch 22/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.9821 - mse: 0.9821 - val_loss: 1.1093 - val_mse: 1.1093\n",
      "Epoch 23/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.9818 - mse: 0.9818 - val_loss: 1.1139 - val_mse: 1.1139\n",
      "Epoch 24/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.9806 - mse: 0.9806 - val_loss: 1.1118 - val_mse: 1.1118\n",
      "Epoch 25/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.9803 - mse: 0.9803 - val_loss: 1.1096 - val_mse: 1.1096\n",
      "Epoch 26/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.9805 - mse: 0.9805 - val_loss: 1.1094 - val_mse: 1.1094\n",
      "Epoch 27/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.9799 - mse: 0.9799 - val_loss: 1.1084 - val_mse: 1.1084\n",
      "Epoch 28/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.9796 - mse: 0.9796 - val_loss: 1.1094 - val_mse: 1.1094\n",
      "Epoch 29/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.9793 - mse: 0.9793 - val_loss: 1.1090 - val_mse: 1.1090\n",
      "Epoch 30/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.9789 - mse: 0.9789 - val_loss: 1.1125 - val_mse: 1.1125\n",
      "Epoch 31/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.9788 - mse: 0.9788 - val_loss: 1.1128 - val_mse: 1.1128\n",
      "Epoch 32/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.9789 - mse: 0.9789 - val_loss: 1.1110 - val_mse: 1.1110\n",
      "Epoch 33/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.9785 - mse: 0.9785 - val_loss: 1.1134 - val_mse: 1.1134\n",
      "Epoch 34/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.9788 - mse: 0.9788 - val_loss: 1.1131 - val_mse: 1.1131\n",
      "Epoch 35/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.9787 - mse: 0.9787 - val_loss: 1.1141 - val_mse: 1.1141\n",
      "Epoch 36/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.9787 - mse: 0.9787 - val_loss: 1.1165 - val_mse: 1.1165\n",
      "Epoch 37/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.9790 - mse: 0.9790 - val_loss: 1.1186 - val_mse: 1.1186\n",
      "Epoch 38/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.9786 - mse: 0.9786 - val_loss: 1.1188 - val_mse: 1.1188\n",
      "Epoch 39/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.9786 - mse: 0.9786 - val_loss: 1.1197 - val_mse: 1.1197\n",
      "Epoch 40/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.9784 - mse: 0.9784 - val_loss: 1.1217 - val_mse: 1.1217\n",
      "Epoch 41/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.9785 - mse: 0.9785 - val_loss: 1.1195 - val_mse: 1.1195\n",
      "Epoch 42/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.9789 - mse: 0.9789 - val_loss: 1.1198 - val_mse: 1.1198\n",
      "Epoch 43/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.9784 - mse: 0.9784 - val_loss: 1.1195 - val_mse: 1.1195\n",
      "Epoch 44/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.9783 - mse: 0.9783 - val_loss: 1.1190 - val_mse: 1.1190\n",
      "Epoch 45/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.9786 - mse: 0.9786 - val_loss: 1.1200 - val_mse: 1.1200\n",
      "Epoch 46/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.9785 - mse: 0.9785 - val_loss: 1.1177 - val_mse: 1.1177\n",
      "Epoch 47/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.9785 - mse: 0.9785 - val_loss: 1.1182 - val_mse: 1.1182\n",
      "Epoch 48/50\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.9787 - mse: 0.9787 - val_loss: 1.1175 - val_mse: 1.1175\n",
      "Epoch 49/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.9786 - mse: 0.9786 - val_loss: 1.1165 - val_mse: 1.1165\n",
      "Epoch 50/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.9788 - mse: 0.9788 - val_loss: 1.1163 - val_mse: 1.1163\n",
      "Epoch 1/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.9206 - mse: 1.9206 - val_loss: 1.4191 - val_mse: 1.4191\n",
      "Epoch 2/50\n",
      "32/32 [==============================] - 0s 786us/step - loss: 1.3473 - mse: 1.3473 - val_loss: 1.1743 - val_mse: 1.1743\n",
      "Epoch 3/50\n",
      "32/32 [==============================] - 0s 776us/step - loss: 1.1427 - mse: 1.1427 - val_loss: 1.0739 - val_mse: 1.0739\n",
      "Epoch 4/50\n",
      "32/32 [==============================] - 0s 751us/step - loss: 1.0584 - mse: 1.0584 - val_loss: 1.0122 - val_mse: 1.0122\n",
      "Epoch 5/50\n",
      "32/32 [==============================] - 0s 773us/step - loss: 1.0150 - mse: 1.0150 - val_loss: 0.9737 - val_mse: 0.9737\n",
      "Epoch 6/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.9895 - mse: 0.9895 - val_loss: 0.9446 - val_mse: 0.9446\n",
      "Epoch 7/50\n",
      "32/32 [==============================] - 0s 721us/step - loss: 0.9739 - mse: 0.9739 - val_loss: 0.9191 - val_mse: 0.9191\n",
      "Epoch 8/50\n",
      "32/32 [==============================] - 0s 857us/step - loss: 0.9638 - mse: 0.9638 - val_loss: 0.8975 - val_mse: 0.8975\n",
      "Epoch 9/50\n",
      "32/32 [==============================] - 0s 747us/step - loss: 0.9573 - mse: 0.9573 - val_loss: 0.8857 - val_mse: 0.8857\n",
      "Epoch 10/50\n",
      "32/32 [==============================] - 0s 740us/step - loss: 0.9537 - mse: 0.9537 - val_loss: 0.8761 - val_mse: 0.8761\n",
      "Epoch 11/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.9500 - mse: 0.9500 - val_loss: 0.8670 - val_mse: 0.8670\n",
      "Epoch 12/50\n",
      "32/32 [==============================] - 0s 786us/step - loss: 0.9489 - mse: 0.9489 - val_loss: 0.8607 - val_mse: 0.8607\n",
      "Epoch 13/50\n",
      "32/32 [==============================] - 0s 767us/step - loss: 0.9477 - mse: 0.9477 - val_loss: 0.8592 - val_mse: 0.8592\n",
      "Epoch 14/50\n",
      "32/32 [==============================] - 0s 771us/step - loss: 0.9468 - mse: 0.9468 - val_loss: 0.8573 - val_mse: 0.8573\n",
      "Epoch 15/50\n",
      "32/32 [==============================] - 0s 741us/step - loss: 0.9463 - mse: 0.9463 - val_loss: 0.8525 - val_mse: 0.8525\n",
      "Epoch 16/50\n",
      "32/32 [==============================] - 0s 718us/step - loss: 0.9458 - mse: 0.9458 - val_loss: 0.8505 - val_mse: 0.8505\n",
      "Epoch 17/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.9455 - mse: 0.9455 - val_loss: 0.8500 - val_mse: 0.8500\n",
      "Epoch 18/50\n",
      "32/32 [==============================] - 0s 813us/step - loss: 0.9454 - mse: 0.9454 - val_loss: 0.8469 - val_mse: 0.8469\n",
      "Epoch 19/50\n",
      "32/32 [==============================] - 0s 745us/step - loss: 0.9455 - mse: 0.9455 - val_loss: 0.8465 - val_mse: 0.8465\n",
      "Epoch 20/50\n",
      "32/32 [==============================] - 0s 764us/step - loss: 0.9455 - mse: 0.9455 - val_loss: 0.8448 - val_mse: 0.8448\n",
      "Epoch 21/50\n",
      "32/32 [==============================] - 0s 726us/step - loss: 0.9454 - mse: 0.9454 - val_loss: 0.8464 - val_mse: 0.8464\n",
      "Epoch 22/50\n",
      "32/32 [==============================] - 0s 729us/step - loss: 0.9452 - mse: 0.9452 - val_loss: 0.8448 - val_mse: 0.8448\n",
      "Epoch 23/50\n",
      "32/32 [==============================] - 0s 761us/step - loss: 0.9453 - mse: 0.9453 - val_loss: 0.8427 - val_mse: 0.8427\n",
      "Epoch 24/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.9453 - mse: 0.9453 - val_loss: 0.8411 - val_mse: 0.8411\n",
      "Epoch 25/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.9451 - mse: 0.9451 - val_loss: 0.8428 - val_mse: 0.8428\n",
      "Epoch 26/50\n",
      "32/32 [==============================] - 0s 740us/step - loss: 0.9455 - mse: 0.9455 - val_loss: 0.8419 - val_mse: 0.8419\n",
      "Epoch 27/50\n",
      "32/32 [==============================] - 0s 741us/step - loss: 0.9453 - mse: 0.9453 - val_loss: 0.8447 - val_mse: 0.8447\n",
      "Epoch 28/50\n",
      "32/32 [==============================] - 0s 756us/step - loss: 0.9457 - mse: 0.9457 - val_loss: 0.8460 - val_mse: 0.8460\n",
      "Epoch 29/50\n",
      "32/32 [==============================] - 0s 735us/step - loss: 0.9453 - mse: 0.9453 - val_loss: 0.8462 - val_mse: 0.8462\n",
      "Epoch 30/50\n",
      "32/32 [==============================] - 0s 721us/step - loss: 0.9452 - mse: 0.9452 - val_loss: 0.8443 - val_mse: 0.8443\n",
      "Epoch 31/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.9452 - mse: 0.9452 - val_loss: 0.8431 - val_mse: 0.8431\n",
      "Epoch 32/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.9455 - mse: 0.9455 - val_loss: 0.8425 - val_mse: 0.8425\n",
      "Epoch 33/50\n",
      "32/32 [==============================] - 0s 740us/step - loss: 0.9455 - mse: 0.9455 - val_loss: 0.8424 - val_mse: 0.8424\n",
      "Epoch 34/50\n",
      "32/32 [==============================] - 0s 736us/step - loss: 0.9453 - mse: 0.9453 - val_loss: 0.8429 - val_mse: 0.8429\n",
      "Epoch 35/50\n",
      "32/32 [==============================] - 0s 762us/step - loss: 0.9453 - mse: 0.9453 - val_loss: 0.8397 - val_mse: 0.8397\n",
      "Epoch 36/50\n",
      "32/32 [==============================] - 0s 718us/step - loss: 0.9454 - mse: 0.9454 - val_loss: 0.8404 - val_mse: 0.8404\n",
      "Epoch 37/50\n",
      "32/32 [==============================] - 0s 795us/step - loss: 0.9451 - mse: 0.9451 - val_loss: 0.8394 - val_mse: 0.8394\n",
      "Epoch 38/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.9454 - mse: 0.9454 - val_loss: 0.8373 - val_mse: 0.8373\n",
      "Epoch 39/50\n",
      "32/32 [==============================] - 0s 800us/step - loss: 0.9452 - mse: 0.9452 - val_loss: 0.8412 - val_mse: 0.8412\n",
      "Epoch 40/50\n",
      "32/32 [==============================] - 0s 750us/step - loss: 0.9452 - mse: 0.9452 - val_loss: 0.8449 - val_mse: 0.8449\n",
      "Epoch 41/50\n",
      "32/32 [==============================] - 0s 744us/step - loss: 0.9456 - mse: 0.9456 - val_loss: 0.8420 - val_mse: 0.8420\n",
      "Epoch 42/50\n",
      "32/32 [==============================] - 0s 736us/step - loss: 0.9451 - mse: 0.9451 - val_loss: 0.8404 - val_mse: 0.8404\n",
      "Epoch 43/50\n",
      "32/32 [==============================] - 0s 785us/step - loss: 0.9454 - mse: 0.9454 - val_loss: 0.8414 - val_mse: 0.8414\n",
      "Epoch 44/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.9452 - mse: 0.9452 - val_loss: 0.8429 - val_mse: 0.8429\n",
      "Epoch 45/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.9452 - mse: 0.9452 - val_loss: 0.8446 - val_mse: 0.8446\n",
      "Epoch 46/50\n",
      "32/32 [==============================] - 0s 744us/step - loss: 0.9454 - mse: 0.9454 - val_loss: 0.8390 - val_mse: 0.8390\n",
      "Epoch 47/50\n",
      "32/32 [==============================] - 0s 747us/step - loss: 0.9456 - mse: 0.9456 - val_loss: 0.8404 - val_mse: 0.8404\n",
      "Epoch 48/50\n",
      "32/32 [==============================] - 0s 749us/step - loss: 0.9451 - mse: 0.9451 - val_loss: 0.8379 - val_mse: 0.8379\n",
      "Epoch 49/50\n",
      "32/32 [==============================] - 0s 772us/step - loss: 0.9456 - mse: 0.9456 - val_loss: 0.8371 - val_mse: 0.8371\n",
      "Epoch 50/50\n",
      "32/32 [==============================] - 0s 746us/step - loss: 0.9454 - mse: 0.9454 - val_loss: 0.8409 - val_mse: 0.8409\n",
      "Epoch 1/50\n",
      "47/47 [==============================] - 0s 2ms/step - loss: 3.2677 - mse: 3.2677 - val_loss: 2.6063 - val_mse: 2.6063\n",
      "Epoch 2/50\n",
      "47/47 [==============================] - 0s 2ms/step - loss: 1.9857 - mse: 1.9857 - val_loss: 1.7449 - val_mse: 1.7449\n",
      "Epoch 3/50\n",
      "47/47 [==============================] - 0s 626us/step - loss: 1.4600 - mse: 1.4600 - val_loss: 1.3703 - val_mse: 1.3703\n",
      "Epoch 4/50\n",
      "47/47 [==============================] - 0s 623us/step - loss: 1.2093 - mse: 1.2093 - val_loss: 1.1874 - val_mse: 1.1874\n",
      "Epoch 5/50\n",
      "47/47 [==============================] - 0s 616us/step - loss: 1.0825 - mse: 1.0825 - val_loss: 1.0912 - val_mse: 1.0912\n",
      "Epoch 6/50\n",
      "47/47 [==============================] - 0s 621us/step - loss: 1.0168 - mse: 1.0168 - val_loss: 1.0381 - val_mse: 1.0381\n",
      "Epoch 7/50\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.9823 - mse: 0.9823 - val_loss: 1.0095 - val_mse: 1.0095\n",
      "Epoch 8/50\n",
      "47/47 [==============================] - 0s 654us/step - loss: 0.9645 - mse: 0.9645 - val_loss: 0.9928 - val_mse: 0.9928\n",
      "Epoch 9/50\n",
      "47/47 [==============================] - 0s 618us/step - loss: 0.9552 - mse: 0.9552 - val_loss: 0.9833 - val_mse: 0.9833\n",
      "Epoch 10/50\n",
      "47/47 [==============================] - 0s 625us/step - loss: 0.9503 - mse: 0.9503 - val_loss: 0.9773 - val_mse: 0.9773\n",
      "Epoch 11/50\n",
      "47/47 [==============================] - 0s 682us/step - loss: 0.9478 - mse: 0.9478 - val_loss: 0.9721 - val_mse: 0.9721\n",
      "Epoch 12/50\n",
      "47/47 [==============================] - 0s 601us/step - loss: 0.9467 - mse: 0.9467 - val_loss: 0.9708 - val_mse: 0.9708\n",
      "Epoch 13/50\n",
      "47/47 [==============================] - 0s 606us/step - loss: 0.9459 - mse: 0.9459 - val_loss: 0.9684 - val_mse: 0.9684\n",
      "Epoch 14/50\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.9457 - mse: 0.9457 - val_loss: 0.9696 - val_mse: 0.9696\n",
      "Epoch 15/50\n",
      "47/47 [==============================] - 0s 735us/step - loss: 0.9454 - mse: 0.9454 - val_loss: 0.9690 - val_mse: 0.9690\n",
      "Epoch 16/50\n",
      "47/47 [==============================] - 0s 612us/step - loss: 0.9451 - mse: 0.9451 - val_loss: 0.9655 - val_mse: 0.9655\n",
      "Epoch 17/50\n",
      "47/47 [==============================] - 0s 623us/step - loss: 0.9451 - mse: 0.9451 - val_loss: 0.9638 - val_mse: 0.9638\n",
      "Epoch 18/50\n",
      "47/47 [==============================] - 0s 618us/step - loss: 0.9456 - mse: 0.9456 - val_loss: 0.9641 - val_mse: 0.9641\n",
      "Epoch 19/50\n",
      "47/47 [==============================] - 0s 614us/step - loss: 0.9453 - mse: 0.9453 - val_loss: 0.9658 - val_mse: 0.9658\n",
      "Epoch 20/50\n",
      "47/47 [==============================] - 0s 2ms/step - loss: 0.9453 - mse: 0.9453 - val_loss: 0.9660 - val_mse: 0.9660\n",
      "Epoch 21/50\n",
      "47/47 [==============================] - 0s 598us/step - loss: 0.9452 - mse: 0.9452 - val_loss: 0.9647 - val_mse: 0.9647\n",
      "Epoch 22/50\n",
      "47/47 [==============================] - 0s 607us/step - loss: 0.9452 - mse: 0.9452 - val_loss: 0.9639 - val_mse: 0.9639\n",
      "Epoch 23/50\n",
      "47/47 [==============================] - 0s 666us/step - loss: 0.9451 - mse: 0.9451 - val_loss: 0.9647 - val_mse: 0.9647\n",
      "Epoch 24/50\n",
      "47/47 [==============================] - 0s 616us/step - loss: 0.9452 - mse: 0.9452 - val_loss: 0.9641 - val_mse: 0.9641\n",
      "Epoch 25/50\n",
      "47/47 [==============================] - 0s 611us/step - loss: 0.9452 - mse: 0.9452 - val_loss: 0.9643 - val_mse: 0.9643\n",
      "Epoch 26/50\n",
      "47/47 [==============================] - 0s 670us/step - loss: 0.9451 - mse: 0.9451 - val_loss: 0.9658 - val_mse: 0.9658\n",
      "Epoch 27/50\n",
      "47/47 [==============================] - 0s 2ms/step - loss: 0.9454 - mse: 0.9454 - val_loss: 0.9657 - val_mse: 0.9657\n",
      "Epoch 28/50\n",
      "47/47 [==============================] - 0s 614us/step - loss: 0.9454 - mse: 0.9454 - val_loss: 0.9669 - val_mse: 0.9669\n",
      "Epoch 29/50\n",
      "47/47 [==============================] - 0s 669us/step - loss: 0.9453 - mse: 0.9453 - val_loss: 0.9675 - val_mse: 0.9675\n",
      "Epoch 30/50\n",
      "47/47 [==============================] - 0s 611us/step - loss: 0.9453 - mse: 0.9453 - val_loss: 0.9659 - val_mse: 0.9659\n",
      "Epoch 31/50\n",
      "47/47 [==============================] - 0s 615us/step - loss: 0.9453 - mse: 0.9453 - val_loss: 0.9648 - val_mse: 0.9648\n",
      "Epoch 32/50\n",
      "47/47 [==============================] - 0s 666us/step - loss: 0.9452 - mse: 0.9452 - val_loss: 0.9648 - val_mse: 0.9648\n",
      "Epoch 33/50\n",
      "47/47 [==============================] - 0s 635us/step - loss: 0.9452 - mse: 0.9452 - val_loss: 0.9659 - val_mse: 0.9659\n",
      "Epoch 34/50\n",
      "47/47 [==============================] - 0s 2ms/step - loss: 0.9451 - mse: 0.9451 - val_loss: 0.9637 - val_mse: 0.9637\n",
      "Epoch 35/50\n",
      "47/47 [==============================] - 0s 684us/step - loss: 0.9453 - mse: 0.9453 - val_loss: 0.9641 - val_mse: 0.9641\n",
      "Epoch 36/50\n",
      "47/47 [==============================] - 0s 613us/step - loss: 0.9453 - mse: 0.9453 - val_loss: 0.9641 - val_mse: 0.9641\n",
      "Epoch 37/50\n",
      "47/47 [==============================] - 0s 612us/step - loss: 0.9454 - mse: 0.9454 - val_loss: 0.9666 - val_mse: 0.9666\n",
      "Epoch 38/50\n",
      "47/47 [==============================] - 0s 678us/step - loss: 0.9452 - mse: 0.9452 - val_loss: 0.9657 - val_mse: 0.9657\n",
      "Epoch 39/50\n",
      "47/47 [==============================] - 0s 614us/step - loss: 0.9454 - mse: 0.9454 - val_loss: 0.9664 - val_mse: 0.9664\n",
      "Epoch 40/50\n",
      "47/47 [==============================] - 0s 608us/step - loss: 0.9453 - mse: 0.9453 - val_loss: 0.9649 - val_mse: 0.9649\n",
      "Epoch 41/50\n",
      "47/47 [==============================] - 0s 2ms/step - loss: 0.9450 - mse: 0.9450 - val_loss: 0.9672 - val_mse: 0.9672\n",
      "Epoch 42/50\n",
      "47/47 [==============================] - 0s 632us/step - loss: 0.9456 - mse: 0.9456 - val_loss: 0.9685 - val_mse: 0.9685\n",
      "Epoch 43/50\n",
      "47/47 [==============================] - 0s 604us/step - loss: 0.9454 - mse: 0.9454 - val_loss: 0.9686 - val_mse: 0.9686\n",
      "Epoch 44/50\n",
      "47/47 [==============================] - 0s 664us/step - loss: 0.9451 - mse: 0.9451 - val_loss: 0.9688 - val_mse: 0.9688\n",
      "Epoch 45/50\n",
      "47/47 [==============================] - 0s 600us/step - loss: 0.9454 - mse: 0.9454 - val_loss: 0.9675 - val_mse: 0.9675\n",
      "Epoch 46/50\n",
      "47/47 [==============================] - 0s 592us/step - loss: 0.9451 - mse: 0.9451 - val_loss: 0.9662 - val_mse: 0.9662\n",
      "Epoch 47/50\n",
      "47/47 [==============================] - 0s 653us/step - loss: 0.9452 - mse: 0.9452 - val_loss: 0.9672 - val_mse: 0.9672\n",
      "Epoch 48/50\n",
      "47/47 [==============================] - 0s 2ms/step - loss: 0.9451 - mse: 0.9451 - val_loss: 0.9679 - val_mse: 0.9679\n",
      "Epoch 49/50\n",
      "47/47 [==============================] - 0s 591us/step - loss: 0.9452 - mse: 0.9452 - val_loss: 0.9650 - val_mse: 0.9650\n",
      "Epoch 50/50\n",
      "47/47 [==============================] - 0s 656us/step - loss: 0.9453 - mse: 0.9453 - val_loss: 0.9658 - val_mse: 0.9658\n",
      "Epoch 1/50\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 2.9565 - mse: 2.9565 - val_loss: 2.0332 - val_mse: 2.0332\n",
      "Epoch 2/50\n",
      "63/63 [==============================] - 0s 545us/step - loss: 1.6612 - mse: 1.6612 - val_loss: 1.4234 - val_mse: 1.4234\n",
      "Epoch 3/50\n",
      "63/63 [==============================] - 0s 582us/step - loss: 1.2390 - mse: 1.2390 - val_loss: 1.1684 - val_mse: 1.1684\n",
      "Epoch 4/50\n",
      "63/63 [==============================] - 0s 534us/step - loss: 1.0759 - mse: 1.0759 - val_loss: 1.0539 - val_mse: 1.0539\n",
      "Epoch 5/50\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.0084 - mse: 1.0084 - val_loss: 1.0021 - val_mse: 1.0021\n",
      "Epoch 6/50\n",
      "63/63 [==============================] - 0s 589us/step - loss: 0.9800 - mse: 0.9800 - val_loss: 0.9770 - val_mse: 0.9770\n",
      "Epoch 7/50\n",
      "63/63 [==============================] - 0s 519us/step - loss: 0.9680 - mse: 0.9680 - val_loss: 0.9645 - val_mse: 0.9645\n",
      "Epoch 8/50\n",
      "63/63 [==============================] - 0s 532us/step - loss: 0.9633 - mse: 0.9633 - val_loss: 0.9588 - val_mse: 0.9588\n",
      "Epoch 9/50\n",
      "63/63 [==============================] - 0s 536us/step - loss: 0.9614 - mse: 0.9614 - val_loss: 0.9551 - val_mse: 0.9551\n",
      "Epoch 10/50\n",
      "63/63 [==============================] - 0s 574us/step - loss: 0.9604 - mse: 0.9604 - val_loss: 0.9538 - val_mse: 0.9538\n",
      "Epoch 11/50\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.9604 - mse: 0.9604 - val_loss: 0.9527 - val_mse: 0.9527\n",
      "Epoch 12/50\n",
      "63/63 [==============================] - 0s 516us/step - loss: 0.9601 - mse: 0.9601 - val_loss: 0.9523 - val_mse: 0.9523\n",
      "Epoch 13/50\n",
      "63/63 [==============================] - 0s 568us/step - loss: 0.9605 - mse: 0.9605 - val_loss: 0.9514 - val_mse: 0.9514\n",
      "Epoch 14/50\n",
      "63/63 [==============================] - 0s 529us/step - loss: 0.9598 - mse: 0.9598 - val_loss: 0.9513 - val_mse: 0.9513\n",
      "Epoch 15/50\n",
      "63/63 [==============================] - 0s 531us/step - loss: 0.9600 - mse: 0.9600 - val_loss: 0.9510 - val_mse: 0.9510\n",
      "Epoch 16/50\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.9601 - mse: 0.9601 - val_loss: 0.9510 - val_mse: 0.9510\n",
      "Epoch 17/50\n",
      "63/63 [==============================] - 0s 539us/step - loss: 0.9599 - mse: 0.9599 - val_loss: 0.9508 - val_mse: 0.9508\n",
      "Epoch 18/50\n",
      "63/63 [==============================] - 0s 532us/step - loss: 0.9599 - mse: 0.9599 - val_loss: 0.9504 - val_mse: 0.9504\n",
      "Epoch 19/50\n",
      "63/63 [==============================] - 0s 523us/step - loss: 0.9599 - mse: 0.9599 - val_loss: 0.9508 - val_mse: 0.9508\n",
      "Epoch 20/50\n",
      "63/63 [==============================] - 0s 514us/step - loss: 0.9601 - mse: 0.9601 - val_loss: 0.9503 - val_mse: 0.9503\n",
      "Epoch 21/50\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.9603 - mse: 0.9603 - val_loss: 0.9503 - val_mse: 0.9503\n",
      "Epoch 22/50\n",
      "63/63 [==============================] - 0s 519us/step - loss: 0.9600 - mse: 0.9600 - val_loss: 0.9501 - val_mse: 0.9501\n",
      "Epoch 23/50\n",
      "63/63 [==============================] - 0s 524us/step - loss: 0.9602 - mse: 0.9602 - val_loss: 0.9504 - val_mse: 0.9504\n",
      "Epoch 24/50\n",
      "63/63 [==============================] - 0s 585us/step - loss: 0.9601 - mse: 0.9601 - val_loss: 0.9503 - val_mse: 0.9503\n",
      "Epoch 25/50\n",
      "63/63 [==============================] - 0s 530us/step - loss: 0.9601 - mse: 0.9601 - val_loss: 0.9501 - val_mse: 0.9501\n",
      "Epoch 26/50\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.9601 - mse: 0.9601 - val_loss: 0.9504 - val_mse: 0.9504\n",
      "Epoch 27/50\n",
      "63/63 [==============================] - 0s 587us/step - loss: 0.9599 - mse: 0.9599 - val_loss: 0.9506 - val_mse: 0.9506\n",
      "Epoch 28/50\n",
      "63/63 [==============================] - 0s 525us/step - loss: 0.9600 - mse: 0.9600 - val_loss: 0.9506 - val_mse: 0.9506\n",
      "Epoch 29/50\n",
      "63/63 [==============================] - 0s 527us/step - loss: 0.9601 - mse: 0.9601 - val_loss: 0.9506 - val_mse: 0.9506\n",
      "Epoch 30/50\n",
      "63/63 [==============================] - 0s 574us/step - loss: 0.9603 - mse: 0.9603 - val_loss: 0.9507 - val_mse: 0.9507\n",
      "Epoch 31/50\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.9602 - mse: 0.9602 - val_loss: 0.9509 - val_mse: 0.9509\n",
      "Epoch 32/50\n",
      "63/63 [==============================] - 0s 541us/step - loss: 0.9602 - mse: 0.9602 - val_loss: 0.9507 - val_mse: 0.9507\n",
      "Epoch 33/50\n",
      "63/63 [==============================] - 0s 524us/step - loss: 0.9601 - mse: 0.9601 - val_loss: 0.9507 - val_mse: 0.9507\n",
      "Epoch 34/50\n",
      "63/63 [==============================] - 0s 531us/step - loss: 0.9600 - mse: 0.9600 - val_loss: 0.9502 - val_mse: 0.9502\n",
      "Epoch 35/50\n",
      "63/63 [==============================] - 0s 580us/step - loss: 0.9602 - mse: 0.9602 - val_loss: 0.9502 - val_mse: 0.9502\n",
      "Epoch 36/50\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.9602 - mse: 0.9602 - val_loss: 0.9503 - val_mse: 0.9503\n",
      "Epoch 37/50\n",
      "63/63 [==============================] - 0s 535us/step - loss: 0.9601 - mse: 0.9601 - val_loss: 0.9504 - val_mse: 0.9504\n",
      "Epoch 38/50\n",
      "63/63 [==============================] - 0s 576us/step - loss: 0.9601 - mse: 0.9601 - val_loss: 0.9502 - val_mse: 0.9502\n",
      "Epoch 39/50\n",
      "63/63 [==============================] - 0s 532us/step - loss: 0.9601 - mse: 0.9601 - val_loss: 0.9501 - val_mse: 0.9501\n",
      "Epoch 40/50\n",
      "63/63 [==============================] - 0s 526us/step - loss: 0.9602 - mse: 0.9602 - val_loss: 0.9502 - val_mse: 0.9502\n",
      "Epoch 41/50\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.9599 - mse: 0.9599 - val_loss: 0.9505 - val_mse: 0.9505\n",
      "Epoch 42/50\n",
      "63/63 [==============================] - 0s 530us/step - loss: 0.9600 - mse: 0.9600 - val_loss: 0.9506 - val_mse: 0.9506\n",
      "Epoch 43/50\n",
      "63/63 [==============================] - 0s 527us/step - loss: 0.9600 - mse: 0.9600 - val_loss: 0.9503 - val_mse: 0.9503\n",
      "Epoch 44/50\n",
      "63/63 [==============================] - 0s 529us/step - loss: 0.9599 - mse: 0.9599 - val_loss: 0.9501 - val_mse: 0.9501\n",
      "Epoch 45/50\n",
      "63/63 [==============================] - 0s 527us/step - loss: 0.9599 - mse: 0.9599 - val_loss: 0.9500 - val_mse: 0.9500\n",
      "Epoch 46/50\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.9602 - mse: 0.9602 - val_loss: 0.9501 - val_mse: 0.9501\n",
      "Epoch 47/50\n",
      "63/63 [==============================] - 0s 543us/step - loss: 0.9602 - mse: 0.9602 - val_loss: 0.9503 - val_mse: 0.9503\n",
      "Epoch 48/50\n",
      "63/63 [==============================] - 0s 525us/step - loss: 0.9600 - mse: 0.9600 - val_loss: 0.9503 - val_mse: 0.9503\n",
      "Epoch 49/50\n",
      "63/63 [==============================] - 0s 574us/step - loss: 0.9601 - mse: 0.9601 - val_loss: 0.9508 - val_mse: 0.9508\n",
      "Epoch 50/50\n",
      "63/63 [==============================] - 0s 521us/step - loss: 0.9600 - mse: 0.9600 - val_loss: 0.9509 - val_mse: 0.9509\n"
     ]
    }
   ],
   "source": [
    "# Iterate model over all combinations of parameters\n",
    "param_combinations = list(itertools.product(_sigma, _n_epochs, _n_train))\n",
    "\n",
    "loss = []\n",
    "\n",
    "for combination in param_combinations:\n",
    "\tloss.append(run_model (combination[0], combination[1], combination[2], N_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally now that we have the loss array, we are going to visualize the data obtained. I am using heatmaps to visualize the loss obtained from the model as a function of the number of epochs, the number of training data and the noise $\\sigma$. With fixed number of `N_train`, each heatmap gives the values of the loss with respect to the number of epochs (on the x-axis) and the noise $\\sigma$ (on the y-axis). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x400 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Function to plot heatmaps\n",
    "def plot_heatmaps(loss, param_combinations, sigma_values, n_epochs_values, n_train_values):\n",
    "    fig = plt.figure(figsize=(20, 4))\n",
    "    gs = GridSpec(1, len(n_train_values), width_ratios=[1]*len(n_train_values), wspace=0.05)\n",
    "    \n",
    "    # Calculate the min and max of the loss values\n",
    "    vmin = min(loss)\n",
    "    vmax = max(loss)\n",
    "    \n",
    "    axes = []\n",
    "    for i, n_train in enumerate(n_train_values):\n",
    "        ax = fig.add_subplot(gs[i])\n",
    "        axes.append(ax)\n",
    "        \n",
    "        # Filter the loss values for the current n_train\n",
    "        filtered_loss = [loss[j] for j in range(len(loss)) if param_combinations[j][2] == n_train]\n",
    "        \n",
    "        # Reshape the filtered loss into a matrix\n",
    "        loss_matrix = np.array(filtered_loss).reshape(len(sigma_values), len(n_epochs_values))\n",
    "        \n",
    "        # Plot heatmap\n",
    "        sns.heatmap(\n",
    "            loss_matrix, annot=True, fmt='.2f', ax=ax, cmap='viridis', vmin=vmin, vmax=vmax,\n",
    "            cbar=False,  # Disable individual colorbars\n",
    "            xticklabels=n_epochs_values, yticklabels=sigma_values\n",
    "        )\n",
    "        \n",
    "        ax.set_title(f'# of training data = {n_train}', fontsize=14)\n",
    "        ax.set_xlabel('Epochs', fontsize=12)\n",
    "        \n",
    "        if i == 0:\n",
    "            ax.set_ylabel('Sigma $\\sigma$', fontsize=12)\n",
    "        else:\n",
    "            ax.set_ylabel('')\n",
    "            ax.tick_params(labelleft=False)\n",
    "    \n",
    "    plt.subplots_adjust(left=0.05, right=0.9, top=0.9, bottom=0.1, wspace=0.2)\n",
    "    cbar_ax = fig.add_axes([0.91, 0.15, 0.02, 0.7])\n",
    "    plt.colorbar(axes[0].collections[0], cax=cbar_ax).set_label('Loss')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# Plot the heatmaps\n",
    "plot_heatmaps(loss, param_combinations, _sigma, _n_epochs, _n_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite being not too easy to read as a data visualization method, these heatmaps actually contain a lot of information about the performance of the model. Let us draw some conclusions from the heatmaps.\n",
    "\n",
    "First off, looking at the difference of the same point in the heatmaps (fixed sigma and epochs) throughout the difference plots, i.e. different number of traning data, we can see how the loss function does not decrease directly with the increase of training data: it saw an improvement when going from $500$ to $1000$, but after that the loss got worse. We can interpret this as an overfitting of the model where more training data resulted in a worse prediction of the test data.\n",
    "\n",
    "The second observation is that with fixed sigma and number of training data, the loss did not vary significantly with the number of epochs (horizontally within a heatmap) especially for higher training data. This shows how the number of epochs does not affect the loss function significantly, and that the model saturates after a certain number of epochs. This number can be seen varying with training data, but we can affirm that after $15$ epochs, the loss does not get much better.\n",
    "\n",
    "Finally, we can see that the loss function increases with the noise $\\sigma$, as expected.\n",
    "\n",
    "In conclusion, the ideal number of training data was evidently $1000$. With this training data $30$ epochs were enough to saturate the model, and the noise $\\sigma$ should be kept as low as possible to have a good fit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Here is an alternate visualization that I tried, with a 3D plot rather than a heatmap. Not particularly clear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hoverinfo": "text",
         "marker": {
          "color": [
           0.03945993259549141,
           0.014337940141558647,
           0.010259142145514488,
           0.009620142169296741,
           0.1402379274368286,
           0.009365476667881012,
           0.00985492579638958,
           0.009536894969642162,
           0.015350277535617352,
           0.008510014042258263,
           0.009658165276050568,
           0.009501088410615921,
           0.011162311770021915,
           0.008425332605838776,
           0.009675457142293453,
           0.009502509608864784,
           0.465322345495224,
           0.05151011794805527,
           0.04193757846951485,
           0.03869154304265976,
           0.048626627773046494,
           0.046511925756931305,
           0.03909594565629959,
           0.038045283406972885,
           0.0478634275496006,
           0.033691879361867905,
           0.0386146679520607,
           0.038037560880184174,
           0.044568996876478195,
           0.033557791262865067,
           0.03859744220972061,
           0.03802476078271866,
           0.3022347688674927,
           0.22134512662887573,
           0.2495303899049759,
           0.2381657212972641,
           0.3057687282562256,
           0.22674746811389923,
           0.24283134937286377,
           0.2378118336200714,
           0.27843227982521057,
           0.21150082349777222,
           0.24194152653217316,
           0.23765462636947632,
           0.27836811542510986,
           0.2097042202949524,
           0.24095802009105682,
           0.23765282332897186,
           1.1782411336898804,
           0.9821128845214844,
           0.9684074521064758,
           0.9532440304756165,
           1.1247440576553345,
           0.8788458108901978,
           0.9661140441894531,
           0.9505408406257629,
           1.1140152215957642,
           0.845366358757019,
           0.9665963649749756,
           0.9500280618667603,
           1.1162984371185303,
           0.840889573097229,
           0.9657875299453735,
           0.950888991355896
          ],
          "colorbar": {
           "title": {
            "text": "Log(Loss)"
           }
          },
          "colorscale": [
           [
            0,
            "#440154"
           ],
           [
            0.1111111111111111,
            "#482878"
           ],
           [
            0.2222222222222222,
            "#3e4989"
           ],
           [
            0.3333333333333333,
            "#31688e"
           ],
           [
            0.4444444444444444,
            "#26828e"
           ],
           [
            0.5555555555555556,
            "#1f9e89"
           ],
           [
            0.6666666666666666,
            "#35b779"
           ],
           [
            0.7777777777777778,
            "#6ece58"
           ],
           [
            0.8888888888888888,
            "#b5de2b"
           ],
           [
            1,
            "#fde725"
           ]
          ],
          "opacity": 0.9,
          "size": 4
         },
         "mode": "markers",
         "text": [
          "Sigma: 0.1<br>Train data: 10<br>Epochs: 500<br>Loss: 0.0395",
          "Sigma: 0.1<br>Train data: 10<br>Epochs: 1000<br>Loss: 0.0143",
          "Sigma: 0.1<br>Train data: 10<br>Epochs: 1500<br>Loss: 0.0103",
          "Sigma: 0.1<br>Train data: 10<br>Epochs: 2000<br>Loss: 0.0096",
          "Sigma: 0.1<br>Train data: 15<br>Epochs: 500<br>Loss: 0.1402",
          "Sigma: 0.1<br>Train data: 15<br>Epochs: 1000<br>Loss: 0.0094",
          "Sigma: 0.1<br>Train data: 15<br>Epochs: 1500<br>Loss: 0.0099",
          "Sigma: 0.1<br>Train data: 15<br>Epochs: 2000<br>Loss: 0.0095",
          "Sigma: 0.1<br>Train data: 30<br>Epochs: 500<br>Loss: 0.0154",
          "Sigma: 0.1<br>Train data: 30<br>Epochs: 1000<br>Loss: 0.0085",
          "Sigma: 0.1<br>Train data: 30<br>Epochs: 1500<br>Loss: 0.0097",
          "Sigma: 0.1<br>Train data: 30<br>Epochs: 2000<br>Loss: 0.0095",
          "Sigma: 0.1<br>Train data: 50<br>Epochs: 500<br>Loss: 0.0112",
          "Sigma: 0.1<br>Train data: 50<br>Epochs: 1000<br>Loss: 0.0084",
          "Sigma: 0.1<br>Train data: 50<br>Epochs: 1500<br>Loss: 0.0097",
          "Sigma: 0.1<br>Train data: 50<br>Epochs: 2000<br>Loss: 0.0095",
          "Sigma: 0.2<br>Train data: 10<br>Epochs: 500<br>Loss: 0.4653",
          "Sigma: 0.2<br>Train data: 10<br>Epochs: 1000<br>Loss: 0.0515",
          "Sigma: 0.2<br>Train data: 10<br>Epochs: 1500<br>Loss: 0.0419",
          "Sigma: 0.2<br>Train data: 10<br>Epochs: 2000<br>Loss: 0.0387",
          "Sigma: 0.2<br>Train data: 15<br>Epochs: 500<br>Loss: 0.0486",
          "Sigma: 0.2<br>Train data: 15<br>Epochs: 1000<br>Loss: 0.0465",
          "Sigma: 0.2<br>Train data: 15<br>Epochs: 1500<br>Loss: 0.0391",
          "Sigma: 0.2<br>Train data: 15<br>Epochs: 2000<br>Loss: 0.0380",
          "Sigma: 0.2<br>Train data: 30<br>Epochs: 500<br>Loss: 0.0479",
          "Sigma: 0.2<br>Train data: 30<br>Epochs: 1000<br>Loss: 0.0337",
          "Sigma: 0.2<br>Train data: 30<br>Epochs: 1500<br>Loss: 0.0386",
          "Sigma: 0.2<br>Train data: 30<br>Epochs: 2000<br>Loss: 0.0380",
          "Sigma: 0.2<br>Train data: 50<br>Epochs: 500<br>Loss: 0.0446",
          "Sigma: 0.2<br>Train data: 50<br>Epochs: 1000<br>Loss: 0.0336",
          "Sigma: 0.2<br>Train data: 50<br>Epochs: 1500<br>Loss: 0.0386",
          "Sigma: 0.2<br>Train data: 50<br>Epochs: 2000<br>Loss: 0.0380",
          "Sigma: 0.5<br>Train data: 10<br>Epochs: 500<br>Loss: 0.3022",
          "Sigma: 0.5<br>Train data: 10<br>Epochs: 1000<br>Loss: 0.2213",
          "Sigma: 0.5<br>Train data: 10<br>Epochs: 1500<br>Loss: 0.2495",
          "Sigma: 0.5<br>Train data: 10<br>Epochs: 2000<br>Loss: 0.2382",
          "Sigma: 0.5<br>Train data: 15<br>Epochs: 500<br>Loss: 0.3058",
          "Sigma: 0.5<br>Train data: 15<br>Epochs: 1000<br>Loss: 0.2267",
          "Sigma: 0.5<br>Train data: 15<br>Epochs: 1500<br>Loss: 0.2428",
          "Sigma: 0.5<br>Train data: 15<br>Epochs: 2000<br>Loss: 0.2378",
          "Sigma: 0.5<br>Train data: 30<br>Epochs: 500<br>Loss: 0.2784",
          "Sigma: 0.5<br>Train data: 30<br>Epochs: 1000<br>Loss: 0.2115",
          "Sigma: 0.5<br>Train data: 30<br>Epochs: 1500<br>Loss: 0.2419",
          "Sigma: 0.5<br>Train data: 30<br>Epochs: 2000<br>Loss: 0.2377",
          "Sigma: 0.5<br>Train data: 50<br>Epochs: 500<br>Loss: 0.2784",
          "Sigma: 0.5<br>Train data: 50<br>Epochs: 1000<br>Loss: 0.2097",
          "Sigma: 0.5<br>Train data: 50<br>Epochs: 1500<br>Loss: 0.2410",
          "Sigma: 0.5<br>Train data: 50<br>Epochs: 2000<br>Loss: 0.2377",
          "Sigma: 1<br>Train data: 10<br>Epochs: 500<br>Loss: 1.1782",
          "Sigma: 1<br>Train data: 10<br>Epochs: 1000<br>Loss: 0.9821",
          "Sigma: 1<br>Train data: 10<br>Epochs: 1500<br>Loss: 0.9684",
          "Sigma: 1<br>Train data: 10<br>Epochs: 2000<br>Loss: 0.9532",
          "Sigma: 1<br>Train data: 15<br>Epochs: 500<br>Loss: 1.1247",
          "Sigma: 1<br>Train data: 15<br>Epochs: 1000<br>Loss: 0.8788",
          "Sigma: 1<br>Train data: 15<br>Epochs: 1500<br>Loss: 0.9661",
          "Sigma: 1<br>Train data: 15<br>Epochs: 2000<br>Loss: 0.9505",
          "Sigma: 1<br>Train data: 30<br>Epochs: 500<br>Loss: 1.1140",
          "Sigma: 1<br>Train data: 30<br>Epochs: 1000<br>Loss: 0.8454",
          "Sigma: 1<br>Train data: 30<br>Epochs: 1500<br>Loss: 0.9666",
          "Sigma: 1<br>Train data: 30<br>Epochs: 2000<br>Loss: 0.9500",
          "Sigma: 1<br>Train data: 50<br>Epochs: 500<br>Loss: 1.1163",
          "Sigma: 1<br>Train data: 50<br>Epochs: 1000<br>Loss: 0.8409",
          "Sigma: 1<br>Train data: 50<br>Epochs: 1500<br>Loss: 0.9658",
          "Sigma: 1<br>Train data: 50<br>Epochs: 2000<br>Loss: 0.9509"
         ],
         "type": "scatter3d",
         "x": [
          0.1,
          0.1,
          0.1,
          0.1,
          0.1,
          0.1,
          0.1,
          0.1,
          0.1,
          0.1,
          0.1,
          0.1,
          0.1,
          0.1,
          0.1,
          0.1,
          0.2,
          0.2,
          0.2,
          0.2,
          0.2,
          0.2,
          0.2,
          0.2,
          0.2,
          0.2,
          0.2,
          0.2,
          0.2,
          0.2,
          0.2,
          0.2,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          0.5,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1
         ],
         "y": [
          10,
          10,
          10,
          10,
          15,
          15,
          15,
          15,
          30,
          30,
          30,
          30,
          50,
          50,
          50,
          50,
          10,
          10,
          10,
          10,
          15,
          15,
          15,
          15,
          30,
          30,
          30,
          30,
          50,
          50,
          50,
          50,
          10,
          10,
          10,
          10,
          15,
          15,
          15,
          15,
          30,
          30,
          30,
          30,
          50,
          50,
          50,
          50,
          10,
          10,
          10,
          10,
          15,
          15,
          15,
          15,
          30,
          30,
          30,
          30,
          50,
          50,
          50,
          50
         ],
         "z": [
          500,
          1000,
          1500,
          2000,
          500,
          1000,
          1500,
          2000,
          500,
          1000,
          1500,
          2000,
          500,
          1000,
          1500,
          2000,
          500,
          1000,
          1500,
          2000,
          500,
          1000,
          1500,
          2000,
          500,
          1000,
          1500,
          2000,
          500,
          1000,
          1500,
          2000,
          500,
          1000,
          1500,
          2000,
          500,
          1000,
          1500,
          2000,
          500,
          1000,
          1500,
          2000,
          500,
          1000,
          1500,
          2000,
          500,
          1000,
          1500,
          2000,
          500,
          1000,
          1500,
          2000,
          500,
          1000,
          1500,
          2000,
          500,
          1000,
          1500,
          2000
         ]
        }
       ],
       "layout": {
        "height": 700,
        "scene": {
         "xaxis": {
          "title": {
           "font": {
            "size": 12
           },
           "text": "sigma"
          }
         },
         "yaxis": {
          "title": {
           "font": {
            "size": 12
           },
           "text": "train data"
          }
         },
         "zaxis": {
          "title": {
           "font": {
            "size": 12
           },
           "text": "epochs"
          }
         }
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "3D Scatter Plot of Loss with Different Parameters"
        },
        "width": 900
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Separate the parameters and losses into individual arrays\n",
    "sigma, n_train, n_epochs = zip(*param_combinations)\n",
    "loss = np.array(loss)\n",
    "\n",
    "# Create a trace for the 3D plot\n",
    "trace = go.Scatter3d(\n",
    "    x=sigma,\n",
    "    y=n_train,\n",
    "    z=n_epochs,\n",
    "    mode='markers',\n",
    "    marker=dict(\n",
    "        size=4,\n",
    "        color=loss,  # Use the log-transformed loss for coloring\n",
    "        colorscale='Viridis',\n",
    "        colorbar=dict(title='Log(Loss)'),\n",
    "        opacity=0.9\n",
    "    ),\n",
    "    text=[f'Sigma: {s}<br>Train data: {t}<br>Epochs: {e}<br>Loss: {l:.4f}'\n",
    "          for s, t, e, l in zip(sigma, n_train, n_epochs, loss)],\n",
    "    hoverinfo='text'\n",
    ")\n",
    "\n",
    "# Create the layout\n",
    "layout = go.Layout(\n",
    "    title='3D Scatter Plot of Loss with Different Parameters',\n",
    "    scene=dict(\n",
    "        xaxis=dict(title=f'sigma', titlefont=dict(size=12)),\n",
    "        yaxis=dict(title='train data', titlefont=dict(size=12)),\n",
    "        zaxis=dict(title='epochs', titlefont=dict(size=12))\n",
    "    ),\n",
    "    width=900,  # Increase the width of the plot\n",
    "    height=700,  # Increase the height of the plot\n",
    ")\n",
    "\n",
    "# Create the figure\n",
    "fig = go.Figure(data=[trace], layout=layout)\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
